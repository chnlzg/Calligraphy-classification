{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "batch_size = 50\n",
    "learning_rate = 0.01\n",
    "num_epoch = 100\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_font_train = datasets.ImageFolder('Data_extend2/811/train/', transform=transform)\n",
    "dataset_font_val = datasets.ImageFolder('Data_extend2/811/val/', transform=transform)\n",
    "dataset_font_test = datasets.ImageFolder('Data_extend2/811/test/', transform=transform)\n",
    "\n",
    "font_train = torch.utils.data.DataLoader(dataset_font_train, batch_size=64, shuffle=True)\n",
    "font_val = torch.utils.data.DataLoader(dataset_font_val, batch_size=32)\n",
    "font_test = torch.utils.data.DataLoader(dataset_font_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(NetModel, times, alpha):\n",
    "    acc_all = []\n",
    "    for i in range(times):\n",
    "        net = NetModel().to(device)\n",
    "        summary(net, (1, 64, 64))\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5, last_epoch=-1)\n",
    "        file_pth = 'Model_pth/Loss/'+ str(int(time.time()*100)) + '.pth'\n",
    "\n",
    "        train_loss_hist = []\n",
    "        val_loss_hist = []\n",
    "        min_loss = 10000\n",
    "\n",
    "        for epoch in range(60):\n",
    "\n",
    "            net.train()\n",
    "            running_loss = 0.0\n",
    "            now_i = 0\n",
    "            for i, (images, labels) in enumerate(font_train):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                output1,output2  = net(images)\n",
    "                loss = criterion(output1, labels) + alpha * criterion(output2, labels)\n",
    "\n",
    "                optimizer.zero_grad() \n",
    "                loss.backward() \n",
    "                optimizer.step() \n",
    "                theta = 0.0\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                # print(i)\n",
    "                now_i = i\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "            correct = []\n",
    "            total = 0.0\n",
    "\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_data in font_val:\n",
    "                    val_image, val_labels = val_data\n",
    "\n",
    "                    val_image = val_image.to(device)\n",
    "                    val_labels = val_labels.to(device)\n",
    "\n",
    "                    output1,output2  = net(images)\n",
    "                    # loss = criterion(output, labels)\n",
    "\n",
    "                    val_loss = criterion(output1, labels) + alpha * criterion(output2, labels)\n",
    "                    #val_loss = criterion(output, val_labels)\n",
    "\n",
    "                    correct.append(val_loss.item())\n",
    "\n",
    "            val_loss_tes = sum(correct) / len(correct)\n",
    "\n",
    "            if val_loss_tes < min_loss:\n",
    "                min_loss = val_loss_tes\n",
    "                print(\"save model\")\n",
    "                torch.save(net.state_dict(), file_pth)\n",
    "\n",
    "            print('epoch: ',epoch ,'||| train loss : ', round(running_loss / now_i,7), ' <-> test loss : ',  round(val_loss_tes,7), '|||')\n",
    "\n",
    "        count_test = []\n",
    "\n",
    "        print('last test:', file_pth)\n",
    "        net.load_state_dict(torch.load(file_pth))\n",
    "        net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # acc = 0\n",
    "            for test_data in font_test:\n",
    "                test_images, test_labels = test_data\n",
    "                \n",
    "                test_images = test_images.to(device)\n",
    "                test_labels = test_labels.to(device)\n",
    "\n",
    "                output1,output2  = net(test_images)\n",
    "                \n",
    "                test_outputs = output2.detach().cpu().numpy()\n",
    "                result = test_outputs.argmax(1)\n",
    "                \n",
    "                test_labels = test_labels.detach().cpu().numpy()\n",
    "                \n",
    "                count = ((result - test_labels) == 0).sum() / len(result)\n",
    "\n",
    "                count_test.append(count)\n",
    "                print(count)\n",
    "                \n",
    "            print(sum(count_test) / len(count_test))\n",
    "            acc_all.append(sum(count_test) / len(count_test))\n",
    "    print(acc_all)\n",
    "    print(sum(acc_all) / len(acc_all))\n",
    "    print(round(sum(acc_all) / len(acc_all), 4))\n",
    "    return acc_all, sum(acc_all) / len(acc_all), round(sum(acc_all) / len(acc_all), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_table = [[0.2, 1], [0.5, 1], [0.8, 1],\n",
    "              [1, 1],\n",
    "              [1, 0.2], [1, 0.5], [1, 0.8]]\n",
    "\n",
    "grid_table2 = [[0.2, 1], [0.5, 1], [0.8, 1],\n",
    "               [1, 1],\n",
    "               [1, 0.8], [1, 0.5], [1, 0.2]]\n",
    "\n",
    "grid_table_b = [5, 2, 1.25, 1, 0.8, 0.5, 0.2]\n",
    "\n",
    "times = 5\n",
    "\n",
    "def Train_grid_table(Net, times, grid_table):\n",
    "    result_grid = []\n",
    "    for para_alpha,para_beta in grid_table:\n",
    "        a1, a2, a3 = train2(Net, times, para_alpha, para_beta)\n",
    "        result_grid.append([para_alpha, para_beta, a1, a2, a3])\n",
    "        print(\"----------loss1 weight==\" +str(para_alpha)+\", loss2 weight==\" +str(para_beta)+\"----------\")\n",
    "        \n",
    "    for content in result_grid:\n",
    "        print(\"----------loss1 weight==\" +str(content[0])+\", loss2 weight==\" +str(content[1])+\"----------\")\n",
    "        print(\"all loss：\" + str(content[2]))\n",
    "        print(\"mean loss：     \" + str(content[4]) + \"    (\"+str(content[3])+\")   \\n \")\n",
    "        \n",
    "def Train_grid_table_single(Net, times, grid_table):\n",
    "    result_grid = []\n",
    "    for para_alpha in grid_table:\n",
    "        a1, a2, a3 = train2(Net, times, para_alpha)\n",
    "        result_grid.append([para_alpha, a1, a2, a3])\n",
    "        print(\"----------loss weight==\" +str(para_alpha)+\"----------\")\n",
    "        \n",
    "    for content in result_grid:\n",
    "        print(\"----------loss weight==\" +str(content[0])+\"----------\")\n",
    "        print(\"all loss：\" + str(content[1]))\n",
    "        print(\"mean loss：     \" + str(content[3]) + \"    (\"+str(content[2])+\")   \\n \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class backbone_2loss_cross_self_mask_mask2_Projection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(backbone_2loss_cross_self_mask_mask2_Projection, self).__init__()\n",
    "        channel_t = 1\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(2, 32, 5, stride=1, padding=2),\n",
    "                                   nn.MaxPool2d(3, 1),\n",
    "                                   nn.BatchNorm2d(32), nn.ReLU(inplace=True))\n",
    "\n",
    "        self.groupConv1 = nn.Sequential(nn.Conv2d(32, 32, 3, stride=1),\n",
    "                                        nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "                                        nn.Conv2d(32, 32, 3, stride=1),\n",
    "                                        nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "                                        nn.Conv2d(32, 32, 3, stride=1),\n",
    "                                        nn.BatchNorm2d(32), nn.ReLU(inplace=True)\n",
    "                                        )\n",
    "        self.mask1_a = nn.Sequential(nn.Conv2d(32, channel_t, 1, stride=1, padding=0), \n",
    "                                     nn.BatchNorm2d(channel_t), nn.ReLU(inplace=True))\n",
    "        self.mask1_b = nn.Sequential(nn.Conv2d(32, channel_t, 3, stride=1, padding=1), \n",
    "                                     nn.BatchNorm2d(channel_t), nn.ReLU(inplace=True))\n",
    "        self.mask1_c = nn.Sequential(nn.Conv2d(32, channel_t, 3, stride=1, padding=1), \n",
    "                                     nn.BatchNorm2d(channel_t), nn.ReLU(inplace=True),\n",
    "                                     nn.Conv2d(channel_t, channel_t, 3, stride=1, padding=1), \n",
    "                                     nn.BatchNorm2d(channel_t), nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.mask1_conv = nn.Sequential(nn.Conv2d(channel_t, 32, 1, stride=1), nn.ReLU(inplace=True))\n",
    "        self.mask1_fc = nn.Linear(56*56, 32)\n",
    "\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(32, 32, 5, stride=1, padding=2),\n",
    "                                   nn.MaxPool2d(3, 1),\n",
    "                                   nn.BatchNorm2d(32), nn.ReLU(inplace=True))\n",
    "\n",
    "        self.groupConv2 = nn.Sequential(nn.Conv2d(32, 32, 3, stride=1),\n",
    "                                        nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "                                        nn.Conv2d(32, 32, 3, stride=1),\n",
    "                                        nn.BatchNorm2d(32), nn.ReLU(inplace=True)\n",
    "                                        )\n",
    "\n",
    "        self.mask2_a = nn.Sequential(nn.Conv2d(32, channel_t, 1, stride=1, padding=0), \n",
    "                                     nn.BatchNorm2d(channel_t), nn.ReLU(inplace=True))\n",
    "        self.mask2_b = nn.Sequential(nn.Conv2d(32, channel_t, 3, stride=1, padding=1), \n",
    "                                     nn.BatchNorm2d(channel_t), nn.ReLU(inplace=True))\n",
    "        self.mask2_c = nn.Sequential(nn.Conv2d(32, channel_t, 3, stride=1, padding=1), \n",
    "                                     nn.BatchNorm2d(channel_t), nn.ReLU(inplace=True),\n",
    "                                     nn.Conv2d(channel_t, channel_t, 3, stride=1, padding=1), \n",
    "                                     nn.BatchNorm2d(channel_t), nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.mask2_conv = nn.Sequential(nn.Conv2d(channel_t, 32, 1, stride=1), nn.ReLU(inplace=True))\n",
    "        self.mask2_fc = nn.Linear(56*56, 32)\n",
    "\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(32, 64, 5, stride=1, padding=2),\n",
    "                                   nn.MaxPool2d(3, 1),\n",
    "                                   nn.BatchNorm2d(64), nn.ReLU(inplace=True))\n",
    "\n",
    "        self.groupConv3 = nn.Sequential(nn.Conv2d(64, 32, 3, stride=1),\n",
    "                                        nn.BatchNorm2d(32), nn.ReLU(inplace=True)\n",
    "                                        )\n",
    "\n",
    "        self.mask3_a = nn.Sequential(nn.Conv2d(32, channel_t, 1, stride=1, padding=0), \n",
    "                                     nn.BatchNorm2d(channel_t), nn.ReLU(inplace=True))\n",
    "        self.mask3_b = nn.Sequential(nn.Conv2d(32, channel_t, 3, stride=1, padding=1), \n",
    "                                     nn.BatchNorm2d(channel_t), nn.ReLU(inplace=True))\n",
    "        self.mask3_c = nn.Sequential(nn.Conv2d(32, channel_t, 3, stride=1, padding=1), \n",
    "                                     nn.BatchNorm2d(channel_t), nn.ReLU(inplace=True),\n",
    "                                     nn.Conv2d(channel_t, channel_t, 3, stride=1, padding=1), \n",
    "                                     nn.BatchNorm2d(channel_t), nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.mask3_conv = nn.Sequential(nn.Conv2d(channel_t, 32, 1, stride=1), nn.ReLU(inplace=True))\n",
    "        self.mask3_fc = nn.Linear(56*56, 32)\n",
    "\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(64, 32, 5, stride=1, padding=2),\n",
    "                                   nn.AvgPool2d(3, 1),\n",
    "                                   nn.BatchNorm2d(32), nn.ReLU(inplace=True))\n",
    "\n",
    "        self.mask4_a = nn.Sequential(nn.Conv2d(32, channel_t, 1, stride=1, padding=0), \n",
    "                                     nn.BatchNorm2d(channel_t), nn.ReLU(inplace=True))\n",
    "        self.mask4_b = nn.Sequential(nn.Conv2d(32, channel_t, 3, stride=1, padding=1), \n",
    "                                     nn.BatchNorm2d(channel_t), nn.ReLU(inplace=True))\n",
    "        self.mask4_c = nn.Sequential(nn.Conv2d(32, channel_t, 3, stride=1, padding=1), \n",
    "                                     nn.BatchNorm2d(channel_t), nn.ReLU(inplace=True),\n",
    "                                     nn.Conv2d(channel_t, channel_t, 3, stride=1, padding=1), \n",
    "                                     nn.BatchNorm2d(channel_t), nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.mask4_conv = nn.Sequential(nn.Conv2d(channel_t, 32, 1, stride=1), nn.ReLU(inplace=True))\n",
    "        self.mask4_fc = nn.Linear(56*56, 32)\n",
    "\n",
    "        self.AdaptiveAvgPool2d = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc = nn.Linear(32 * 10, 4)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32 * 1, 4)\n",
    "\n",
    "        self.Transpose1 = nn.ConvTranspose2d(128, 64, 5, stride=1, padding=2)\n",
    "        self.Transpose2 = nn.ConvTranspose2d(64, 32, 5, stride=1, padding=2)\n",
    "        self.Transpose3 = nn.ConvTranspose2d(32, 32, 5, stride=1, padding=2)\n",
    "\n",
    "        self.singleConv1 = nn.Sequential(nn.Conv2d(32, 32, (1, 3), stride=1, padding=(0, 1)), nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.proj0 = nn.Conv2d(2048, 8192, kernel_size=1, stride=1)\n",
    "        self.proj1 = nn.Conv2d(2048, 8192, kernel_size=1, stride=1)\n",
    "        self.proj2 = nn.Conv2d(2048, 8192, kernel_size=1, stride=1)\n",
    "\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        Projection_x = torch.mean(input, dim=2, keepdim=True).squeeze(1)\n",
    "        Projection_y = torch.mean(input, dim=3, keepdim=True).squeeze(1)\n",
    "        #print(Projection_x.shape, Projection_y.shape)\n",
    "        \n",
    "        Projection_matrix = torch.bmm(Projection_y, Projection_x).unsqueeze(1)\n",
    "        #print(Projection_matrix.shape)\n",
    "        #\n",
    "        # Projection_x = Projection_x.view(Projection_x.size()[0], 1, -1)\n",
    "        # Projection_y = Projection_y.view(Projection_y.size()[0], 1, -1)\n",
    "        \n",
    "        inputs_ = torch.cat([input, Projection_matrix], dim=1)\n",
    "        # print(inputs_.shape)\n",
    "        \n",
    "        x1 = self.conv1(inputs_)\n",
    "        # x_f1 = self.AdaptiveAvgPool2d(self.se1(x1))\n",
    "\n",
    "        x2 = self.conv2(x1)\n",
    "        # x_f2 = self.AdaptiveAvgPool2d(self.se2(x2))\n",
    "\n",
    "        x3 = self.conv3(x2)\n",
    "        # x_f3 = self.AdaptiveAvgPool2d(self.se3(x3))\n",
    "\n",
    "        x4 = self.conv4(x3)\n",
    "        # x_f4 = self.AdaptiveAvgPool2d(self.se4(x4))\n",
    "        x32 = x4\n",
    "\n",
    "        x1 = self.groupConv1(x1)\n",
    "        x2 = self.groupConv2(x2)\n",
    "        x3 = self.groupConv3(x3)\n",
    "        #print(2, x1.shape, x2.shape, x3.shape, x4.shape)\n",
    "\n",
    "        # sx1 = self.singleConv1(x1)\n",
    "        # print(sx1.shape)\n",
    "        # ---------------------1------------------------\n",
    "        mask1_a = self.mask1_a(x1)\n",
    "        mask1_b = self.mask1_b(x1)\n",
    "        mask1_c = self.mask1_c(x1)\n",
    "        #mask1 = mask1_a * mask1_b * mask1_c\n",
    "        mask1 = mask1_a + mask1_b + mask1_c\n",
    "        \n",
    "        mask1_bc1 = self.mask1_conv(mask1)\n",
    "        # mask1_bc2 = self.AdaptiveAvgPool2d(mask1)\n",
    "        mask1_bc2 = mask1.view(mask1.size(0), -1)\n",
    "        #print(mask1_bc2.shape)\n",
    "        mask1_bc2 = self.mask1_fc(mask1_bc2)\n",
    "        mask1_bc2 = mask1_bc2.unsqueeze(2)\n",
    "        mask1_bc2 = mask1_bc2.unsqueeze(2)\n",
    "        \n",
    "        mask1 = mask1_bc1 * mask1_bc2\n",
    "        # print(mask1.shape, x1.shape)\n",
    "        \n",
    "        # ---------------------2------------------------\n",
    "        mask2_a = self.mask2_a(x2)\n",
    "        mask2_b = self.mask2_b(x2)\n",
    "        mask2_c = self.mask2_c(x2)\n",
    "        #mask2 = mask2_a * mask2_b * mask2_c\n",
    "        mask2 = mask2_a + mask2_b + mask2_c\n",
    "        \n",
    "        mask2_bc1 = self.mask2_conv(mask2)\n",
    "        #mask2_bc2 = self.AdaptiveAvgPool2d(mask2)\n",
    "        mask2_bc2 = mask2.view(mask2.size(0), -1)\n",
    "        \n",
    "        mask2_bc2 = self.mask2_fc(mask2_bc2)\n",
    "        mask2_bc2 = mask2_bc2.unsqueeze(2)\n",
    "        mask2_bc2 = mask2_bc2.unsqueeze(2)\n",
    "        \n",
    "        mask2 = mask2_bc1 * mask2_bc2\n",
    "        \n",
    "        # ---------------------3------------------------\n",
    "        mask3_a = self.mask3_a(x3)\n",
    "        mask3_b = self.mask3_b(x3)\n",
    "        mask3_c = self.mask3_c(x3)\n",
    "        #mask3 = mask3_a * mask3_b * mask3_c\n",
    "        mask3 = mask3_a + mask3_b + mask3_c\n",
    "        \n",
    "        mask3_bc1 = self.mask3_conv(mask3)\n",
    "        # mask3_bc2 = self.AdaptiveAvgPool2d(mask3)\n",
    "        mask3_bc2 = mask3.view(mask3.size(0), -1)\n",
    "        mask3_bc2 = self.mask3_fc(mask3_bc2)\n",
    "        mask3_bc2 = mask3_bc2.unsqueeze(2)\n",
    "        mask3_bc2 = mask3_bc2.unsqueeze(2)\n",
    "        \n",
    "        mask3 = mask3_bc1 * mask3_bc2\n",
    "        \n",
    "        # ---------------------4------------------------\n",
    "        mask4_a = self.mask4_a(x4)\n",
    "        mask4_b = self.mask4_b(x4)\n",
    "        mask4_c = self.mask4_c(x4)\n",
    "        #mask4 = mask4_a * mask4_b * mask4_c\n",
    "        mask4 = mask4_a + mask4_b + mask4_c\n",
    "        \n",
    "        mask4_bc1 = self.mask4_conv(mask4)\n",
    "        #mask4_bc2 = self.AdaptiveAvgPool2d(mask4)\n",
    "        mask4_bc2 = mask4.view(mask4.size(0), -1)\n",
    "        mask4_bc2 = self.mask4_fc(mask4_bc2)\n",
    "        mask4_bc2 = mask4_bc2.unsqueeze(2)\n",
    "        mask4_bc2 = mask4_bc2.unsqueeze(2)\n",
    "        \n",
    "        mask4 = mask4_bc1 * mask4_bc2\n",
    "        \n",
    "\n",
    "        x1 = x1 * nn.Sigmoid()(mask1)\n",
    "        x2 = x2 * nn.Sigmoid()(mask2)\n",
    "        x3 = x3 * nn.Sigmoid()(mask3)\n",
    "        x4 = x4 * nn.Sigmoid()(mask4)\n",
    "        \n",
    "        x = torch.cat(\n",
    "            (self.AdaptiveAvgPool2d(x1 * x2), self.AdaptiveAvgPool2d(x1 * x3), self.AdaptiveAvgPool2d(x1 * x4),\n",
    "             self.AdaptiveAvgPool2d(x2 * x3), self.AdaptiveAvgPool2d(x2 * x4),\n",
    "             self.AdaptiveAvgPool2d(x3 * x4),\n",
    "             \n",
    "             self.AdaptiveAvgPool2d(x1 * x1),self.AdaptiveAvgPool2d(x2 * x2),\n",
    "            self.AdaptiveAvgPool2d(x3 * x3),self.AdaptiveAvgPool2d(x4 * x4)), dim=1)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        yloss = self.fc(x)\n",
    "        \n",
    "        xloss = self.AdaptiveAvgPool2d(x32)\n",
    "        xloss = xloss.view(xloss.size(0), -1)\n",
    "        xloss = self.fc1(xloss)\n",
    "\n",
    "        return xloss,yloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n",
      "save model\n",
      "epoch:  0 ||| train loss :  6.2043011  <-> test loss :  18.9178238 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  4.257689  <-> test loss :  6.4951406 |||\n",
      "epoch:  2 ||| train loss :  3.3693939  <-> test loss :  8.4767599 |||\n",
      "epoch:  3 ||| train loss :  2.6373395  <-> test loss :  29.9124584 |||\n",
      "epoch:  4 ||| train loss :  2.2044567  <-> test loss :  7.8688202 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  1.8270963  <-> test loss :  3.9469028 |||\n",
      "save model\n",
      "epoch:  6 ||| train loss :  1.467486  <-> test loss :  1.3721877 |||\n",
      "epoch:  7 ||| train loss :  1.383287  <-> test loss :  3.8183122 |||\n",
      "epoch:  8 ||| train loss :  1.1724498  <-> test loss :  2.3638585 |||\n",
      "epoch:  9 ||| train loss :  0.9673904  <-> test loss :  3.4204509 |||\n",
      "epoch:  10 ||| train loss :  0.9411185  <-> test loss :  2.4677267 |||\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  11 ||| train loss :  0.6520414  <-> test loss :  7.5680356 |||\n",
      "epoch:  12 ||| train loss :  0.4452159  <-> test loss :  3.1351676 |||\n",
      "epoch:  13 ||| train loss :  0.5106871  <-> test loss :  21.492239 |||\n",
      "epoch:  14 ||| train loss :  0.5624193  <-> test loss :  2.7411571 |||\n",
      "epoch:  15 ||| train loss :  0.6203329  <-> test loss :  2.7513337 |||\n",
      "save model\n",
      "epoch:  16 ||| train loss :  0.3550853  <-> test loss :  0.9715949 |||\n",
      "save model\n",
      "epoch:  17 ||| train loss :  0.2257933  <-> test loss :  0.1969019 |||\n",
      "epoch:  18 ||| train loss :  0.1792322  <-> test loss :  2.2405472 |||\n",
      "save model\n",
      "epoch:  19 ||| train loss :  0.2385068  <-> test loss :  0.1238082 |||\n",
      "epoch:  20 ||| train loss :  0.3466722  <-> test loss :  6.4223261 |||\n",
      "epoch:  21 ||| train loss :  0.5998084  <-> test loss :  15.236064 |||\n",
      "epoch:  22 ||| train loss :  0.3822463  <-> test loss :  0.5073348 |||\n",
      "epoch:  23 ||| train loss :  0.2338068  <-> test loss :  8.3419867 |||\n",
      "epoch:  24 ||| train loss :  0.1379391  <-> test loss :  0.2969762 |||\n",
      "save model\n",
      "epoch:  25 ||| train loss :  0.0972991  <-> test loss :  0.1185944 |||\n",
      "epoch:  26 ||| train loss :  0.080487  <-> test loss :  0.6786121 |||\n",
      "save model\n",
      "epoch:  27 ||| train loss :  0.0796672  <-> test loss :  0.0337086 |||\n",
      "epoch:  28 ||| train loss :  0.0766715  <-> test loss :  0.0965809 |||\n",
      "epoch:  29 ||| train loss :  0.0566784  <-> test loss :  1.1010156 |||\n",
      "epoch:  30 ||| train loss :  0.115071  <-> test loss :  0.2098268 |||\n",
      "epoch:  31 ||| train loss :  0.1110543  <-> test loss :  0.0958191 |||\n",
      "epoch:  32 ||| train loss :  0.0657451  <-> test loss :  2.4551661 |||\n",
      "epoch:  33 ||| train loss :  0.0451885  <-> test loss :  0.0511548 |||\n",
      "epoch:  34 ||| train loss :  0.1497248  <-> test loss :  0.2740214 |||\n",
      "epoch:  35 ||| train loss :  0.3295928  <-> test loss :  13.8129101 |||\n",
      "epoch:  36 ||| train loss :  0.3608996  <-> test loss :  0.4135134 |||\n",
      "epoch:  37 ||| train loss :  0.1949164  <-> test loss :  0.0778076 |||\n",
      "epoch:  38 ||| train loss :  0.1020127  <-> test loss :  0.1476733 |||\n",
      "epoch:  39 ||| train loss :  0.0584175  <-> test loss :  0.0462351 |||\n",
      "save model\n",
      "epoch:  40 ||| train loss :  0.0459517  <-> test loss :  0.0171356 |||\n",
      "save model\n",
      "epoch:  41 ||| train loss :  0.0345737  <-> test loss :  0.0167837 |||\n",
      "save model\n",
      "epoch:  42 ||| train loss :  0.0308988  <-> test loss :  0.0163188 |||\n",
      "save model\n",
      "epoch:  43 ||| train loss :  0.0274275  <-> test loss :  0.0153107 |||\n",
      "epoch:  44 ||| train loss :  0.0227274  <-> test loss :  0.0242732 |||\n",
      "epoch:  45 ||| train loss :  0.0210217  <-> test loss :  0.0169019 |||\n",
      "epoch:  46 ||| train loss :  0.0198398  <-> test loss :  0.0190556 |||\n",
      "save model\n",
      "epoch:  47 ||| train loss :  0.017739  <-> test loss :  0.0092919 |||\n",
      "epoch:  48 ||| train loss :  0.0354716  <-> test loss :  0.0393672 |||\n",
      "save model\n",
      "epoch:  49 ||| train loss :  0.023922  <-> test loss :  0.0037266 |||\n",
      "epoch:  50 ||| train loss :  0.0165506  <-> test loss :  0.006259 |||\n",
      "epoch:  51 ||| train loss :  0.0141923  <-> test loss :  0.0076518 |||\n",
      "epoch:  52 ||| train loss :  0.0132764  <-> test loss :  0.0150903 |||\n",
      "epoch:  53 ||| train loss :  0.0170796  <-> test loss :  0.007239 |||\n",
      "epoch:  54 ||| train loss :  0.014275  <-> test loss :  0.0127583 |||\n",
      "epoch:  55 ||| train loss :  0.0122776  <-> test loss :  0.0053797 |||\n",
      "epoch:  56 ||| train loss :  0.0115606  <-> test loss :  0.0044551 |||\n",
      "epoch:  57 ||| train loss :  0.0130283  <-> test loss :  0.0072882 |||\n",
      "save model\n",
      "epoch:  58 ||| train loss :  0.0119856  <-> test loss :  0.0023256 |||\n",
      "epoch:  59 ||| train loss :  0.0143166  <-> test loss :  0.0046392 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165634658118.pth\n",
      "0.90625\n",
      "0.9375\n",
      "0.90625\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "1.0\n",
      "0.90625\n",
      "1.0\n",
      "1.0\n",
      "0.9375\n",
      "0.90625\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "1.0\n",
      "0.90625\n",
      "0.9375\n",
      "1.0\n",
      "0.90625\n",
      "1.0\n",
      "0.9456521739130435\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  6.5103869  <-> test loss :  9.6464739 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  4.5780796  <-> test loss :  5.2205906 |||\n",
      "epoch:  2 ||| train loss :  3.3795834  <-> test loss :  16.4072876 |||\n",
      "epoch:  3 ||| train loss :  2.7296276  <-> test loss :  17.7369995 |||\n",
      "epoch:  4 ||| train loss :  2.0990843  <-> test loss :  18.5065422 |||\n",
      "epoch:  5 ||| train loss :  1.7517311  <-> test loss :  8.5421839 |||\n",
      "save model\n",
      "epoch:  6 ||| train loss :  1.5366748  <-> test loss :  3.0918229 |||\n",
      "epoch:  7 ||| train loss :  1.4266353  <-> test loss :  23.7371502 |||\n",
      "epoch:  8 ||| train loss :  1.1108221  <-> test loss :  14.486228 |||\n",
      "epoch:  9 ||| train loss :  0.8944818  <-> test loss :  7.9451666 |||\n",
      "save model\n",
      "epoch:  10 ||| train loss :  0.8396324  <-> test loss :  2.7680976 |||\n",
      "epoch:  11 ||| train loss :  0.5796782  <-> test loss :  3.160449 |||\n",
      "save model\n",
      "epoch:  12 ||| train loss :  0.6347147  <-> test loss :  0.6380545 |||\n",
      "save model\n",
      "epoch:  13 ||| train loss :  0.4814091  <-> test loss :  0.4852065 |||\n",
      "epoch:  14 ||| train loss :  0.374445  <-> test loss :  7.8368268 |||\n",
      "epoch:  15 ||| train loss :  0.390442  <-> test loss :  2.3539031 |||\n",
      "save model\n",
      "epoch:  16 ||| train loss :  0.3295212  <-> test loss :  0.4081458 |||\n",
      "epoch:  17 ||| train loss :  0.3942809  <-> test loss :  1.0890262 |||\n",
      "epoch:  18 ||| train loss :  0.3510013  <-> test loss :  0.9293513 |||\n",
      "epoch:  19 ||| train loss :  0.3222115  <-> test loss :  0.5999908 |||\n",
      "save model\n",
      "epoch:  20 ||| train loss :  0.2442361  <-> test loss :  0.2059314 |||\n",
      "save model\n",
      "epoch:  21 ||| train loss :  0.1806111  <-> test loss :  0.1812277 |||\n",
      "save model\n",
      "epoch:  22 ||| train loss :  0.1298595  <-> test loss :  0.1225628 |||\n",
      "epoch:  23 ||| train loss :  0.1200106  <-> test loss :  1.2675936 |||\n",
      "epoch:  24 ||| train loss :  0.1617451  <-> test loss :  0.2220828 |||\n",
      "epoch:  25 ||| train loss :  0.1277213  <-> test loss :  0.2026706 |||\n",
      "epoch:  26 ||| train loss :  0.1136067  <-> test loss :  0.2197017 |||\n",
      "epoch:  27 ||| train loss :  0.1163493  <-> test loss :  0.284775 |||\n",
      "epoch:  28 ||| train loss :  0.1413349  <-> test loss :  0.9335269 |||\n",
      "epoch:  29 ||| train loss :  0.1372483  <-> test loss :  0.3201393 |||\n",
      "save model\n",
      "epoch:  30 ||| train loss :  0.0783627  <-> test loss :  0.0698238 |||\n",
      "epoch:  31 ||| train loss :  0.06208  <-> test loss :  0.3991338 |||\n",
      "epoch:  32 ||| train loss :  0.077207  <-> test loss :  0.1418952 |||\n",
      "save model\n",
      "epoch:  33 ||| train loss :  0.0620434  <-> test loss :  0.0571529 |||\n",
      "save model\n",
      "epoch:  34 ||| train loss :  0.0429513  <-> test loss :  0.0259044 |||\n",
      "epoch:  35 ||| train loss :  0.0661414  <-> test loss :  9.4254074 |||\n",
      "epoch:  36 ||| train loss :  0.3445312  <-> test loss :  0.4940743 |||\n",
      "epoch:  37 ||| train loss :  0.323062  <-> test loss :  4.3034248 |||\n",
      "epoch:  38 ||| train loss :  0.3881906  <-> test loss :  2.7026246 |||\n",
      "epoch:  39 ||| train loss :  0.3064733  <-> test loss :  2.0238273 |||\n",
      "epoch:  40 ||| train loss :  0.2620327  <-> test loss :  0.1809537 |||\n",
      "epoch:  41 ||| train loss :  0.1547165  <-> test loss :  0.034178 |||\n",
      "epoch:  42 ||| train loss :  0.0880519  <-> test loss :  0.0289765 |||\n",
      "epoch:  43 ||| train loss :  0.0953166  <-> test loss :  0.0848393 |||\n",
      "save model\n",
      "epoch:  44 ||| train loss :  0.0652243  <-> test loss :  0.0234126 |||\n",
      "epoch:  45 ||| train loss :  0.0402274  <-> test loss :  0.0472512 |||\n",
      "epoch:  46 ||| train loss :  0.0402457  <-> test loss :  0.0437174 |||\n",
      "save model\n",
      "epoch:  47 ||| train loss :  0.0307372  <-> test loss :  0.0184473 |||\n",
      "epoch:  48 ||| train loss :  0.0254697  <-> test loss :  0.0223526 |||\n",
      "save model\n",
      "epoch:  49 ||| train loss :  0.0221249  <-> test loss :  0.0134397 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.019918  <-> test loss :  0.0045256 |||\n",
      "epoch:  51 ||| train loss :  0.0191391  <-> test loss :  0.0087715 |||\n",
      "epoch:  52 ||| train loss :  0.0200987  <-> test loss :  0.0048051 |||\n",
      "epoch:  53 ||| train loss :  0.0178844  <-> test loss :  0.0121605 |||\n",
      "epoch:  54 ||| train loss :  0.0176501  <-> test loss :  0.0094059 |||\n",
      "epoch:  55 ||| train loss :  0.0174571  <-> test loss :  0.0117235 |||\n",
      "epoch:  56 ||| train loss :  0.0157815  <-> test loss :  0.0058779 |||\n",
      "epoch:  57 ||| train loss :  0.0144437  <-> test loss :  0.0153675 |||\n",
      "epoch:  58 ||| train loss :  0.0191875  <-> test loss :  0.0071373 |||\n",
      "epoch:  59 ||| train loss :  0.0171703  <-> test loss :  0.0067663 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165634705812.pth\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "1.0\n",
      "0.9375\n",
      "0.9375\n",
      "0.90625\n",
      "0.9375\n",
      "0.90625\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.90625\n",
      "0.9375\n",
      "0.90625\n",
      "0.9375\n",
      "1.0\n",
      "0.96875\n",
      "1.0\n",
      "1.0\n",
      "0.9510869565217391\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  6.2635428  <-> test loss :  28.2094231 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  4.1035791  <-> test loss :  6.3004413 |||\n",
      "save model\n",
      "epoch:  2 ||| train loss :  3.1506232  <-> test loss :  3.5255625 |||\n",
      "save model\n",
      "epoch:  3 ||| train loss :  2.6064501  <-> test loss :  1.5925883 |||\n",
      "epoch:  4 ||| train loss :  1.925222  <-> test loss :  9.8951397 |||\n",
      "epoch:  5 ||| train loss :  1.5734376  <-> test loss :  3.1094246 |||\n",
      "epoch:  6 ||| train loss :  1.3645514  <-> test loss :  1.9571702 |||\n",
      "save model\n",
      "epoch:  7 ||| train loss :  1.2181901  <-> test loss :  1.5211744 |||\n",
      "epoch:  8 ||| train loss :  0.9752839  <-> test loss :  2.2498209 |||\n",
      "epoch:  9 ||| train loss :  0.7370866  <-> test loss :  2.0582333 |||\n",
      "epoch:  10 ||| train loss :  0.7278031  <-> test loss :  5.3295937 |||\n",
      "save model\n",
      "epoch:  11 ||| train loss :  0.7129903  <-> test loss :  0.6555864 |||\n",
      "epoch:  12 ||| train loss :  0.4674283  <-> test loss :  1.3788288 |||\n",
      "save model\n",
      "epoch:  13 ||| train loss :  0.4360972  <-> test loss :  0.3465947 |||\n",
      "epoch:  14 ||| train loss :  0.7242887  <-> test loss :  23.6350117 |||\n",
      "epoch:  15 ||| train loss :  0.603801  <-> test loss :  0.723545 |||\n",
      "save model\n",
      "epoch:  16 ||| train loss :  0.3748304  <-> test loss :  0.3440055 |||\n",
      "epoch:  17 ||| train loss :  0.4552798  <-> test loss :  1.2705564 |||\n",
      "epoch:  18 ||| train loss :  0.3070895  <-> test loss :  0.350047 |||\n",
      "save model\n",
      "epoch:  19 ||| train loss :  0.1603991  <-> test loss :  0.0937163 |||\n",
      "epoch:  20 ||| train loss :  0.1660533  <-> test loss :  0.3788815 |||\n",
      "epoch:  21 ||| train loss :  0.1614891  <-> test loss :  0.1512303 |||\n",
      "epoch:  22 ||| train loss :  0.1231905  <-> test loss :  0.1298254 |||\n",
      "epoch:  23 ||| train loss :  0.1007813  <-> test loss :  0.2067286 |||\n",
      "epoch:  24 ||| train loss :  0.0854944  <-> test loss :  0.9378765 |||\n",
      "save model\n",
      "epoch:  25 ||| train loss :  0.073784  <-> test loss :  0.0596014 |||\n",
      "save model\n",
      "epoch:  26 ||| train loss :  0.056717  <-> test loss :  0.0312616 |||\n",
      "epoch:  27 ||| train loss :  0.0545203  <-> test loss :  0.109674 |||\n",
      "epoch:  28 ||| train loss :  0.0639885  <-> test loss :  0.0636296 |||\n",
      "epoch:  29 ||| train loss :  0.0452613  <-> test loss :  0.034478 |||\n",
      "save model\n",
      "epoch:  30 ||| train loss :  0.0415247  <-> test loss :  0.0108007 |||\n",
      "epoch:  31 ||| train loss :  0.0347291  <-> test loss :  0.0464387 |||\n",
      "epoch:  32 ||| train loss :  0.031098  <-> test loss :  0.0230228 |||\n",
      "epoch:  33 ||| train loss :  0.0301779  <-> test loss :  0.0155946 |||\n",
      "epoch:  34 ||| train loss :  0.0280453  <-> test loss :  0.0370698 |||\n",
      "epoch:  35 ||| train loss :  0.0278771  <-> test loss :  0.0268193 |||\n",
      "epoch:  36 ||| train loss :  0.0291575  <-> test loss :  0.013116 |||\n",
      "epoch:  37 ||| train loss :  0.1913717  <-> test loss :  3.2775059 |||\n",
      "epoch:  38 ||| train loss :  0.3317264  <-> test loss :  4.6800385 |||\n",
      "epoch:  39 ||| train loss :  0.3644921  <-> test loss :  20.9165668 |||\n",
      "epoch:  40 ||| train loss :  0.5301614  <-> test loss :  2.4535024 |||\n",
      "epoch:  41 ||| train loss :  0.1738713  <-> test loss :  0.0698755 |||\n",
      "epoch:  42 ||| train loss :  0.0901056  <-> test loss :  0.1160568 |||\n",
      "epoch:  43 ||| train loss :  0.0820763  <-> test loss :  0.0612226 |||\n",
      "epoch:  44 ||| train loss :  0.0470262  <-> test loss :  0.0301742 |||\n",
      "epoch:  45 ||| train loss :  0.0319299  <-> test loss :  0.0324761 |||\n",
      "epoch:  46 ||| train loss :  0.0313649  <-> test loss :  0.0182224 |||\n",
      "save model\n",
      "epoch:  47 ||| train loss :  0.025628  <-> test loss :  0.0080648 |||\n",
      "epoch:  48 ||| train loss :  0.0264491  <-> test loss :  0.0332749 |||\n",
      "epoch:  49 ||| train loss :  0.0258497  <-> test loss :  0.0104009 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.0196031  <-> test loss :  0.0043893 |||\n",
      "epoch:  51 ||| train loss :  0.016807  <-> test loss :  0.0062978 |||\n",
      "epoch:  52 ||| train loss :  0.0197128  <-> test loss :  0.0133744 |||\n",
      "epoch:  53 ||| train loss :  0.016787  <-> test loss :  0.0160867 |||\n",
      "epoch:  54 ||| train loss :  0.0134338  <-> test loss :  0.0070902 |||\n",
      "epoch:  55 ||| train loss :  0.0138752  <-> test loss :  0.0090053 |||\n",
      "epoch:  56 ||| train loss :  0.0195616  <-> test loss :  0.0271199 |||\n",
      "epoch:  57 ||| train loss :  0.0181918  <-> test loss :  0.0128529 |||\n",
      "epoch:  58 ||| train loss :  0.0138655  <-> test loss :  0.0089551 |||\n",
      "epoch:  59 ||| train loss :  0.0146746  <-> test loss :  0.0045644 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165634753572.pth\n",
      "0.875\n",
      "0.9375\n",
      "0.90625\n",
      "0.96875\n",
      "0.9375\n",
      "0.90625\n",
      "0.9375\n",
      "0.96875\n",
      "0.84375\n",
      "1.0\n",
      "1.0\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.90625\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "0.96875\n",
      "0.9375\n",
      "0.9429347826086957\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  6.2720292  <-> test loss :  7.2959013 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  4.2902414  <-> test loss :  4.1160216 |||\n",
      "epoch:  2 ||| train loss :  3.3379725  <-> test loss :  6.1259971 |||\n",
      "epoch:  3 ||| train loss :  2.7448906  <-> test loss :  10.9796925 |||\n",
      "epoch:  4 ||| train loss :  2.3105903  <-> test loss :  7.9658036 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  1.9076424  <-> test loss :  3.2517309 |||\n",
      "save model\n",
      "epoch:  6 ||| train loss :  1.5946938  <-> test loss :  2.7288566 |||\n",
      "epoch:  7 ||| train loss :  1.3465609  <-> test loss :  14.8500471 |||\n",
      "epoch:  8 ||| train loss :  1.4393446  <-> test loss :  14.0916939 |||\n",
      "save model\n",
      "epoch:  9 ||| train loss :  0.9323229  <-> test loss :  1.4889748 |||\n",
      "save model\n",
      "epoch:  10 ||| train loss :  0.9110309  <-> test loss :  1.4394221 |||\n",
      "epoch:  11 ||| train loss :  0.6575924  <-> test loss :  11.4428396 |||\n",
      "epoch:  12 ||| train loss :  0.6330862  <-> test loss :  4.5689893 |||\n",
      "epoch:  13 ||| train loss :  0.5833561  <-> test loss :  1.9484444 |||\n",
      "epoch:  14 ||| train loss :  0.4759637  <-> test loss :  2.4136052 |||\n",
      "save model\n",
      "epoch:  15 ||| train loss :  0.4550883  <-> test loss :  0.2049064 |||\n",
      "epoch:  16 ||| train loss :  0.3150451  <-> test loss :  1.5574586 |||\n",
      "save model\n",
      "epoch:  17 ||| train loss :  0.217219  <-> test loss :  0.1860034 |||\n",
      "epoch:  18 ||| train loss :  0.1423564  <-> test loss :  0.4933744 |||\n",
      "save model\n",
      "epoch:  19 ||| train loss :  0.1546698  <-> test loss :  0.0694306 |||\n",
      "epoch:  20 ||| train loss :  0.2331526  <-> test loss :  0.1767752 |||\n",
      "epoch:  21 ||| train loss :  0.2536234  <-> test loss :  6.977416 |||\n",
      "epoch:  22 ||| train loss :  0.3072111  <-> test loss :  1.1203783 |||\n",
      "epoch:  23 ||| train loss :  0.2976177  <-> test loss :  0.5857874 |||\n",
      "epoch:  24 ||| train loss :  0.3456448  <-> test loss :  1.1654533 |||\n",
      "epoch:  25 ||| train loss :  0.2555109  <-> test loss :  1.473565 |||\n",
      "epoch:  26 ||| train loss :  0.1589579  <-> test loss :  0.143184 |||\n",
      "epoch:  27 ||| train loss :  0.1068575  <-> test loss :  0.081853 |||\n",
      "save model\n",
      "epoch:  28 ||| train loss :  0.0867499  <-> test loss :  0.0436795 |||\n",
      "save model\n",
      "epoch:  29 ||| train loss :  0.0878869  <-> test loss :  0.043016 |||\n",
      "save model\n",
      "epoch:  30 ||| train loss :  0.0554123  <-> test loss :  0.0395633 |||\n",
      "epoch:  31 ||| train loss :  0.0463504  <-> test loss :  0.0485104 |||\n",
      "save model\n",
      "epoch:  32 ||| train loss :  0.0378746  <-> test loss :  0.0242648 |||\n",
      "save model\n",
      "epoch:  33 ||| train loss :  0.0345686  <-> test loss :  0.0179987 |||\n",
      "epoch:  34 ||| train loss :  0.0324874  <-> test loss :  0.0218519 |||\n",
      "save model\n",
      "epoch:  35 ||| train loss :  0.0346745  <-> test loss :  0.016721 |||\n",
      "epoch:  36 ||| train loss :  0.0326639  <-> test loss :  0.0663706 |||\n",
      "epoch:  37 ||| train loss :  0.0261486  <-> test loss :  0.0214503 |||\n",
      "save model\n",
      "epoch:  38 ||| train loss :  0.0216332  <-> test loss :  0.0105178 |||\n",
      "epoch:  39 ||| train loss :  0.0776605  <-> test loss :  2.0114813 |||\n",
      "epoch:  40 ||| train loss :  0.2499311  <-> test loss :  3.9928293 |||\n",
      "epoch:  41 ||| train loss :  0.0974478  <-> test loss :  0.0833545 |||\n",
      "epoch:  42 ||| train loss :  0.0578772  <-> test loss :  0.1619053 |||\n",
      "epoch:  43 ||| train loss :  0.0679441  <-> test loss :  0.0242009 |||\n",
      "epoch:  44 ||| train loss :  0.0342156  <-> test loss :  0.0246544 |||\n",
      "epoch:  45 ||| train loss :  0.0281988  <-> test loss :  0.0150724 |||\n",
      "epoch:  46 ||| train loss :  0.0528887  <-> test loss :  0.0387419 |||\n",
      "epoch:  47 ||| train loss :  0.0283304  <-> test loss :  0.0160232 |||\n",
      "save model\n",
      "epoch:  48 ||| train loss :  0.0178249  <-> test loss :  0.007179 |||\n",
      "save model\n",
      "epoch:  49 ||| train loss :  0.015255  <-> test loss :  0.0071737 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.01315  <-> test loss :  0.0060897 |||\n",
      "epoch:  51 ||| train loss :  0.0124608  <-> test loss :  0.0149481 |||\n",
      "epoch:  52 ||| train loss :  0.0110872  <-> test loss :  0.0120196 |||\n",
      "epoch:  53 ||| train loss :  0.0127528  <-> test loss :  0.0075893 |||\n",
      "save model\n",
      "epoch:  54 ||| train loss :  0.0105316  <-> test loss :  0.0058659 |||\n",
      "save model\n",
      "epoch:  55 ||| train loss :  0.0111826  <-> test loss :  0.0058281 |||\n",
      "save model\n",
      "epoch:  56 ||| train loss :  0.0105621  <-> test loss :  0.0047189 |||\n",
      "save model\n",
      "epoch:  57 ||| train loss :  0.0104566  <-> test loss :  0.0037186 |||\n",
      "epoch:  58 ||| train loss :  0.0099094  <-> test loss :  0.0043374 |||\n",
      "epoch:  59 ||| train loss :  0.0089461  <-> test loss :  0.0052849 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165634801335.pth\n",
      "0.875\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "0.9375\n",
      "0.9375\n",
      "0.84375\n",
      "1.0\n",
      "0.875\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "1.0\n",
      "0.9375\n",
      "0.9375\n",
      "0.875\n",
      "0.875\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "0.9415760869565217\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  6.288918  <-> test loss :  15.3045349 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  4.1951815  <-> test loss :  9.3514442 |||\n",
      "save model\n",
      "epoch:  2 ||| train loss :  3.2345536  <-> test loss :  4.3313093 |||\n",
      "epoch:  3 ||| train loss :  2.6417079  <-> test loss :  9.4683542 |||\n",
      "save model\n",
      "epoch:  4 ||| train loss :  2.1907787  <-> test loss :  3.4410837 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  1.8553332  <-> test loss :  1.4879315 |||\n",
      "epoch:  6 ||| train loss :  1.4637345  <-> test loss :  6.4424257 |||\n",
      "epoch:  7 ||| train loss :  1.3340107  <-> test loss :  22.8268623 |||\n",
      "epoch:  8 ||| train loss :  1.0338606  <-> test loss :  14.9160604 |||\n",
      "epoch:  9 ||| train loss :  0.9902593  <-> test loss :  6.4039316 |||\n",
      "epoch:  10 ||| train loss :  0.7576666  <-> test loss :  8.003212 |||\n",
      "epoch:  11 ||| train loss :  0.7376035  <-> test loss :  2.5508146 |||\n",
      "save model\n",
      "epoch:  12 ||| train loss :  0.7444688  <-> test loss :  0.5960565 |||\n",
      "epoch:  13 ||| train loss :  0.4515739  <-> test loss :  1.2336663 |||\n",
      "epoch:  14 ||| train loss :  0.4006921  <-> test loss :  6.0636387 |||\n",
      "epoch:  15 ||| train loss :  0.3887017  <-> test loss :  3.0705595 |||\n",
      "epoch:  16 ||| train loss :  0.4159393  <-> test loss :  2.475688 |||\n",
      "epoch:  17 ||| train loss :  0.3192264  <-> test loss :  2.8438263 |||\n",
      "epoch:  18 ||| train loss :  0.255821  <-> test loss :  4.9530377 |||\n",
      "epoch:  19 ||| train loss :  0.4923099  <-> test loss :  0.9976785 |||\n",
      "epoch:  20 ||| train loss :  0.4554966  <-> test loss :  15.8700457 |||\n",
      "save model\n",
      "epoch:  21 ||| train loss :  0.3143373  <-> test loss :  0.4018965 |||\n",
      "epoch:  22 ||| train loss :  0.2775369  <-> test loss :  1.166629 |||\n",
      "epoch:  23 ||| train loss :  0.3100213  <-> test loss :  0.5334703 |||\n",
      "epoch:  24 ||| train loss :  0.1907747  <-> test loss :  0.9741638 |||\n",
      "epoch:  25 ||| train loss :  0.1848351  <-> test loss :  0.7611506 |||\n",
      "epoch:  26 ||| train loss :  0.2335773  <-> test loss :  0.4141327 |||\n",
      "epoch:  27 ||| train loss :  0.2138188  <-> test loss :  4.8652992 |||\n",
      "save model\n",
      "epoch:  28 ||| train loss :  0.2098238  <-> test loss :  0.2464941 |||\n",
      "epoch:  29 ||| train loss :  0.1644317  <-> test loss :  1.0472043 |||\n",
      "epoch:  30 ||| train loss :  0.1080919  <-> test loss :  0.9629591 |||\n",
      "epoch:  31 ||| train loss :  0.1626169  <-> test loss :  0.3340813 |||\n",
      "epoch:  32 ||| train loss :  0.2019945  <-> test loss :  0.4891419 |||\n",
      "epoch:  33 ||| train loss :  0.405968  <-> test loss :  5.2752028 |||\n",
      "epoch:  34 ||| train loss :  0.2720688  <-> test loss :  2.8701758 |||\n",
      "save model\n",
      "epoch:  35 ||| train loss :  0.1267923  <-> test loss :  0.0613717 |||\n",
      "epoch:  36 ||| train loss :  0.0747469  <-> test loss :  0.0613731 |||\n",
      "save model\n",
      "epoch:  37 ||| train loss :  0.0543266  <-> test loss :  0.0516913 |||\n",
      "epoch:  38 ||| train loss :  0.0645829  <-> test loss :  0.3030136 |||\n",
      "epoch:  39 ||| train loss :  0.0755224  <-> test loss :  0.0559558 |||\n",
      "save model\n",
      "epoch:  40 ||| train loss :  0.0463812  <-> test loss :  0.0169163 |||\n",
      "epoch:  41 ||| train loss :  0.0342057  <-> test loss :  0.0410937 |||\n",
      "epoch:  42 ||| train loss :  0.0332647  <-> test loss :  0.0666472 |||\n",
      "save model\n",
      "epoch:  43 ||| train loss :  0.0388716  <-> test loss :  0.0100239 |||\n",
      "epoch:  44 ||| train loss :  0.0289018  <-> test loss :  0.0232735 |||\n",
      "epoch:  45 ||| train loss :  0.02621  <-> test loss :  0.0137508 |||\n",
      "epoch:  46 ||| train loss :  0.0250976  <-> test loss :  0.0132344 |||\n",
      "epoch:  47 ||| train loss :  0.0220199  <-> test loss :  0.0243121 |||\n",
      "epoch:  48 ||| train loss :  0.0224394  <-> test loss :  0.0111116 |||\n",
      "save model\n",
      "epoch:  49 ||| train loss :  0.0175167  <-> test loss :  0.0088239 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.0169984  <-> test loss :  0.0062668 |||\n",
      "epoch:  51 ||| train loss :  0.0176268  <-> test loss :  0.0218172 |||\n",
      "epoch:  52 ||| train loss :  0.0150746  <-> test loss :  0.0186361 |||\n",
      "epoch:  53 ||| train loss :  0.0143687  <-> test loss :  0.0094691 |||\n",
      "epoch:  54 ||| train loss :  0.013851  <-> test loss :  0.009752 |||\n",
      "epoch:  55 ||| train loss :  0.0139355  <-> test loss :  0.0171047 |||\n",
      "save model\n",
      "epoch:  56 ||| train loss :  0.0152755  <-> test loss :  0.005067 |||\n",
      "epoch:  57 ||| train loss :  0.0175856  <-> test loss :  0.0255701 |||\n",
      "epoch:  58 ||| train loss :  0.016566  <-> test loss :  0.005959 |||\n",
      "epoch:  59 ||| train loss :  0.0169226  <-> test loss :  0.1632791 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165634849184.pth\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "1.0\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "0.90625\n",
      "0.9375\n",
      "1.0\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "0.875\n",
      "0.90625\n",
      "0.84375\n",
      "0.96875\n",
      "0.9375\n",
      "0.90625\n",
      "0.90625\n",
      "1.0\n",
      "0.938858695652174\n",
      "[0.9456521739130435, 0.9510869565217391, 0.9429347826086957, 0.9415760869565217, 0.938858695652174]\n",
      "0.9440217391304347\n",
      "0.944\n",
      "----------loss weight==5----------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  3.3260482  <-> test loss :  9.6164932 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  2.0324478  <-> test loss :  2.9795151 |||\n",
      "epoch:  2 ||| train loss :  1.503753  <-> test loss :  5.5646129 |||\n",
      "save model\n",
      "epoch:  3 ||| train loss :  1.1879008  <-> test loss :  1.4183397 |||\n",
      "save model\n",
      "epoch:  4 ||| train loss :  0.9861167  <-> test loss :  1.1257777 |||\n",
      "epoch:  5 ||| train loss :  0.7917624  <-> test loss :  3.0690751 |||\n",
      "epoch:  6 ||| train loss :  0.6863211  <-> test loss :  1.1338005 |||\n",
      "epoch:  7 ||| train loss :  0.5617649  <-> test loss :  1.9660716 |||\n",
      "epoch:  8 ||| train loss :  0.4574279  <-> test loss :  2.1588817 |||\n",
      "epoch:  9 ||| train loss :  0.4421886  <-> test loss :  4.1397686 |||\n",
      "save model\n",
      "epoch:  10 ||| train loss :  0.3553052  <-> test loss :  0.5634495 |||\n",
      "epoch:  11 ||| train loss :  0.2466524  <-> test loss :  2.6429014 |||\n",
      "epoch:  12 ||| train loss :  0.2457351  <-> test loss :  0.8222581 |||\n",
      "epoch:  13 ||| train loss :  0.2409835  <-> test loss :  5.2651882 |||\n",
      "save model\n",
      "epoch:  14 ||| train loss :  0.1962424  <-> test loss :  0.3821783 |||\n",
      "epoch:  15 ||| train loss :  0.1442814  <-> test loss :  0.4429271 |||\n",
      "save model\n",
      "epoch:  16 ||| train loss :  0.1133924  <-> test loss :  0.1422457 |||\n",
      "epoch:  17 ||| train loss :  0.1421573  <-> test loss :  0.410137 |||\n",
      "epoch:  18 ||| train loss :  0.1028242  <-> test loss :  0.3124864 |||\n",
      "epoch:  19 ||| train loss :  0.0623346  <-> test loss :  0.2713008 |||\n",
      "save model\n",
      "epoch:  20 ||| train loss :  0.0473072  <-> test loss :  0.0523166 |||\n",
      "epoch:  21 ||| train loss :  0.051781  <-> test loss :  0.3514416 |||\n",
      "epoch:  22 ||| train loss :  0.1468224  <-> test loss :  1.5644 |||\n",
      "epoch:  23 ||| train loss :  0.245679  <-> test loss :  0.3297313 |||\n",
      "epoch:  24 ||| train loss :  0.1725295  <-> test loss :  0.2252618 |||\n",
      "epoch:  25 ||| train loss :  0.0927244  <-> test loss :  0.2682293 |||\n",
      "epoch:  26 ||| train loss :  0.0528475  <-> test loss :  0.1439983 |||\n",
      "save model\n",
      "epoch:  27 ||| train loss :  0.0306718  <-> test loss :  0.0338044 |||\n",
      "save model\n",
      "epoch:  28 ||| train loss :  0.0288353  <-> test loss :  0.024814 |||\n",
      "epoch:  29 ||| train loss :  0.0218416  <-> test loss :  0.0307515 |||\n",
      "epoch:  30 ||| train loss :  0.0229311  <-> test loss :  0.0632371 |||\n",
      "epoch:  31 ||| train loss :  0.018778  <-> test loss :  0.0279091 |||\n",
      "save model\n",
      "epoch:  32 ||| train loss :  0.0215349  <-> test loss :  0.0223922 |||\n",
      "save model\n",
      "epoch:  33 ||| train loss :  0.0224931  <-> test loss :  0.0094473 |||\n",
      "epoch:  34 ||| train loss :  0.01334  <-> test loss :  0.0098335 |||\n",
      "save model\n",
      "epoch:  35 ||| train loss :  0.0122412  <-> test loss :  0.0066209 |||\n",
      "epoch:  36 ||| train loss :  0.0094207  <-> test loss :  0.0073728 |||\n",
      "epoch:  37 ||| train loss :  0.0089493  <-> test loss :  0.0115613 |||\n",
      "save model\n",
      "epoch:  38 ||| train loss :  0.0082606  <-> test loss :  0.003925 |||\n",
      "save model\n",
      "epoch:  39 ||| train loss :  0.0077875  <-> test loss :  0.0028142 |||\n",
      "epoch:  40 ||| train loss :  0.0081104  <-> test loss :  0.4510853 |||\n",
      "epoch:  41 ||| train loss :  0.017007  <-> test loss :  0.00728 |||\n",
      "epoch:  42 ||| train loss :  0.0134181  <-> test loss :  0.0159082 |||\n",
      "epoch:  43 ||| train loss :  0.007635  <-> test loss :  0.0031817 |||\n",
      "epoch:  44 ||| train loss :  0.0060151  <-> test loss :  0.0059555 |||\n",
      "epoch:  45 ||| train loss :  0.0048627  <-> test loss :  0.0028154 |||\n",
      "save model\n",
      "epoch:  46 ||| train loss :  0.0057664  <-> test loss :  0.0019355 |||\n",
      "save model\n",
      "epoch:  47 ||| train loss :  0.0054269  <-> test loss :  0.0018962 |||\n",
      "epoch:  48 ||| train loss :  0.0044415  <-> test loss :  0.0022099 |||\n",
      "epoch:  49 ||| train loss :  0.0045649  <-> test loss :  0.002108 |||\n",
      "epoch:  50 ||| train loss :  0.0044984  <-> test loss :  0.0021334 |||\n",
      "save model\n",
      "epoch:  51 ||| train loss :  0.0043408  <-> test loss :  0.0018233 |||\n",
      "save model\n",
      "epoch:  52 ||| train loss :  0.0035174  <-> test loss :  0.0014514 |||\n",
      "epoch:  53 ||| train loss :  0.0041636  <-> test loss :  0.0014733 |||\n",
      "epoch:  54 ||| train loss :  0.0032502  <-> test loss :  0.001661 |||\n",
      "epoch:  55 ||| train loss :  0.0044285  <-> test loss :  0.0036009 |||\n",
      "epoch:  56 ||| train loss :  0.003535  <-> test loss :  0.0019242 |||\n",
      "epoch:  57 ||| train loss :  0.0036324  <-> test loss :  0.0024982 |||\n",
      "epoch:  58 ||| train loss :  0.0037753  <-> test loss :  0.0017074 |||\n",
      "epoch:  59 ||| train loss :  0.0035704  <-> test loss :  0.0019994 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165634897069.pth\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "0.90625\n",
      "0.9375\n",
      "0.90625\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "0.875\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "0.9538043478260869\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  3.2877025  <-> test loss :  15.0337906 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  1.9875889  <-> test loss :  6.5384741 |||\n",
      "save model\n",
      "epoch:  2 ||| train loss :  1.5374849  <-> test loss :  5.4242649 |||\n",
      "epoch:  3 ||| train loss :  1.1414721  <-> test loss :  6.4533501 |||\n",
      "save model\n",
      "epoch:  4 ||| train loss :  0.9649233  <-> test loss :  1.7820029 |||\n",
      "epoch:  5 ||| train loss :  0.7408774  <-> test loss :  1.8987616 |||\n",
      "epoch:  6 ||| train loss :  0.6249063  <-> test loss :  4.5751462 |||\n",
      "epoch:  7 ||| train loss :  0.4994705  <-> test loss :  5.879118 |||\n",
      "save model\n",
      "epoch:  8 ||| train loss :  0.4025675  <-> test loss :  1.2720356 |||\n",
      "epoch:  9 ||| train loss :  0.3949147  <-> test loss :  6.7726183 |||\n",
      "epoch:  10 ||| train loss :  0.3408404  <-> test loss :  2.7879291 |||\n",
      "save model\n",
      "epoch:  11 ||| train loss :  0.3129241  <-> test loss :  1.0222603 |||\n",
      "epoch:  12 ||| train loss :  0.2919598  <-> test loss :  3.3888342 |||\n",
      "save model\n",
      "epoch:  13 ||| train loss :  0.2169748  <-> test loss :  0.5366784 |||\n",
      "save model\n",
      "epoch:  14 ||| train loss :  0.1449978  <-> test loss :  0.1973198 |||\n",
      "epoch:  15 ||| train loss :  0.1261956  <-> test loss :  0.5625567 |||\n",
      "epoch:  16 ||| train loss :  0.1036942  <-> test loss :  0.2996813 |||\n",
      "epoch:  17 ||| train loss :  0.1128672  <-> test loss :  0.2472614 |||\n",
      "epoch:  18 ||| train loss :  0.0891357  <-> test loss :  0.2272463 |||\n",
      "epoch:  19 ||| train loss :  0.218316  <-> test loss :  1.3573902 |||\n",
      "save model\n",
      "epoch:  20 ||| train loss :  0.1666101  <-> test loss :  0.1971249 |||\n",
      "epoch:  21 ||| train loss :  0.0998377  <-> test loss :  1.0135663 |||\n",
      "epoch:  22 ||| train loss :  0.0756221  <-> test loss :  0.2047152 |||\n",
      "epoch:  23 ||| train loss :  0.0609905  <-> test loss :  2.0845685 |||\n",
      "epoch:  24 ||| train loss :  0.0747014  <-> test loss :  0.3637608 |||\n",
      "save model\n",
      "epoch:  25 ||| train loss :  0.0683504  <-> test loss :  0.1830299 |||\n",
      "save model\n",
      "epoch:  26 ||| train loss :  0.0325947  <-> test loss :  0.0364992 |||\n",
      "save model\n",
      "epoch:  27 ||| train loss :  0.0260947  <-> test loss :  0.0163596 |||\n",
      "epoch:  28 ||| train loss :  0.020187  <-> test loss :  0.030731 |||\n",
      "epoch:  29 ||| train loss :  0.0159949  <-> test loss :  0.0238237 |||\n",
      "save model\n",
      "epoch:  30 ||| train loss :  0.013488  <-> test loss :  0.0114483 |||\n",
      "save model\n",
      "epoch:  31 ||| train loss :  0.0119552  <-> test loss :  0.0057747 |||\n",
      "epoch:  32 ||| train loss :  0.0118848  <-> test loss :  0.0113156 |||\n",
      "save model\n",
      "epoch:  33 ||| train loss :  0.0103602  <-> test loss :  0.0049852 |||\n",
      "epoch:  34 ||| train loss :  0.0088643  <-> test loss :  0.0069571 |||\n",
      "save model\n",
      "epoch:  35 ||| train loss :  0.0094997  <-> test loss :  0.0039559 |||\n",
      "save model\n",
      "epoch:  36 ||| train loss :  0.0096791  <-> test loss :  0.0034028 |||\n",
      "epoch:  37 ||| train loss :  0.0087446  <-> test loss :  0.0040423 |||\n",
      "epoch:  38 ||| train loss :  0.0080826  <-> test loss :  0.0100369 |||\n",
      "epoch:  39 ||| train loss :  0.0285464  <-> test loss :  1.5879221 |||\n",
      "epoch:  40 ||| train loss :  0.143096  <-> test loss :  0.5007361 |||\n",
      "epoch:  41 ||| train loss :  0.1643666  <-> test loss :  0.5144753 |||\n",
      "epoch:  42 ||| train loss :  0.1212222  <-> test loss :  0.0606695 |||\n",
      "epoch:  43 ||| train loss :  0.0805773  <-> test loss :  0.91083 |||\n",
      "epoch:  44 ||| train loss :  0.082588  <-> test loss :  0.2541714 |||\n",
      "epoch:  45 ||| train loss :  0.0401083  <-> test loss :  0.0177133 |||\n",
      "epoch:  46 ||| train loss :  0.0162288  <-> test loss :  0.0138603 |||\n",
      "epoch:  47 ||| train loss :  0.0164872  <-> test loss :  0.0174846 |||\n",
      "epoch:  48 ||| train loss :  0.0174977  <-> test loss :  0.0319763 |||\n",
      "epoch:  49 ||| train loss :  0.0143625  <-> test loss :  0.0055745 |||\n",
      "epoch:  50 ||| train loss :  0.00839  <-> test loss :  0.0053871 |||\n",
      "epoch:  51 ||| train loss :  0.0067152  <-> test loss :  0.004528 |||\n",
      "epoch:  52 ||| train loss :  0.0061966  <-> test loss :  0.0035406 |||\n",
      "save model\n",
      "epoch:  53 ||| train loss :  0.0071282  <-> test loss :  0.0030565 |||\n",
      "epoch:  54 ||| train loss :  0.0055043  <-> test loss :  0.0054238 |||\n",
      "epoch:  55 ||| train loss :  0.0054966  <-> test loss :  0.0039733 |||\n",
      "save model\n",
      "epoch:  56 ||| train loss :  0.0052129  <-> test loss :  0.0021281 |||\n",
      "save model\n",
      "epoch:  57 ||| train loss :  0.0055099  <-> test loss :  0.0012589 |||\n",
      "epoch:  58 ||| train loss :  0.0057786  <-> test loss :  0.0069206 |||\n",
      "epoch:  59 ||| train loss :  0.0048124  <-> test loss :  0.0024064 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165634944981.pth\n",
      "0.9375\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "0.875\n",
      "0.9375\n",
      "1.0\n",
      "0.90625\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "0.9375\n",
      "0.84375\n",
      "0.9375\n",
      "1.0\n",
      "0.90625\n",
      "0.9375\n",
      "1.0\n",
      "0.9375\n",
      "0.9375\n",
      "0.9497282608695652\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  3.2923894  <-> test loss :  7.0541759 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  2.1065014  <-> test loss :  5.9256587 |||\n",
      "save model\n",
      "epoch:  2 ||| train loss :  1.4976141  <-> test loss :  2.9324727 |||\n",
      "epoch:  3 ||| train loss :  1.2045679  <-> test loss :  3.121794 |||\n",
      "save model\n",
      "epoch:  4 ||| train loss :  1.0389426  <-> test loss :  2.2400141 |||\n",
      "epoch:  5 ||| train loss :  0.8044585  <-> test loss :  6.5657988 |||\n",
      "save model\n",
      "epoch:  6 ||| train loss :  0.6433009  <-> test loss :  0.9459122 |||\n",
      "save model\n",
      "epoch:  7 ||| train loss :  0.5668377  <-> test loss :  0.6512002 |||\n",
      "epoch:  8 ||| train loss :  0.5050008  <-> test loss :  0.6736593 |||\n",
      "epoch:  9 ||| train loss :  0.4445068  <-> test loss :  0.8964458 |||\n",
      "epoch:  10 ||| train loss :  0.3250022  <-> test loss :  0.7340137 |||\n",
      "epoch:  11 ||| train loss :  0.3208603  <-> test loss :  1.4229875 |||\n",
      "save model\n",
      "epoch:  12 ||| train loss :  0.3016469  <-> test loss :  0.2392552 |||\n",
      "epoch:  13 ||| train loss :  0.2552647  <-> test loss :  1.0633116 |||\n",
      "epoch:  14 ||| train loss :  0.206213  <-> test loss :  0.3701864 |||\n",
      "epoch:  15 ||| train loss :  0.1822709  <-> test loss :  7.0332766 |||\n",
      "save model\n",
      "epoch:  16 ||| train loss :  0.144169  <-> test loss :  0.1078436 |||\n",
      "epoch:  17 ||| train loss :  0.1056392  <-> test loss :  1.0516001 |||\n",
      "epoch:  18 ||| train loss :  0.2383126  <-> test loss :  4.8687196 |||\n",
      "epoch:  19 ||| train loss :  0.1505746  <-> test loss :  0.474842 |||\n",
      "epoch:  20 ||| train loss :  0.132794  <-> test loss :  1.0538509 |||\n",
      "epoch:  21 ||| train loss :  0.1231039  <-> test loss :  0.531307 |||\n",
      "epoch:  22 ||| train loss :  0.1089792  <-> test loss :  1.1175351 |||\n",
      "epoch:  23 ||| train loss :  0.1133881  <-> test loss :  0.1133192 |||\n",
      "epoch:  24 ||| train loss :  0.0904564  <-> test loss :  0.2585154 |||\n",
      "save model\n",
      "epoch:  25 ||| train loss :  0.0552197  <-> test loss :  0.0960959 |||\n",
      "save model\n",
      "epoch:  26 ||| train loss :  0.0376926  <-> test loss :  0.0566244 |||\n",
      "save model\n",
      "epoch:  27 ||| train loss :  0.0256921  <-> test loss :  0.0073476 |||\n",
      "epoch:  28 ||| train loss :  0.0194487  <-> test loss :  0.0156166 |||\n",
      "save model\n",
      "epoch:  29 ||| train loss :  0.016936  <-> test loss :  0.0056966 |||\n",
      "epoch:  30 ||| train loss :  0.0163136  <-> test loss :  0.0126928 |||\n",
      "epoch:  31 ||| train loss :  0.0115962  <-> test loss :  0.0112057 |||\n",
      "epoch:  32 ||| train loss :  0.0117812  <-> test loss :  0.0068096 |||\n",
      "epoch:  33 ||| train loss :  0.0116125  <-> test loss :  0.0082234 |||\n",
      "save model\n",
      "epoch:  34 ||| train loss :  0.0106177  <-> test loss :  0.0025876 |||\n",
      "epoch:  35 ||| train loss :  0.0104578  <-> test loss :  0.0113249 |||\n",
      "epoch:  36 ||| train loss :  0.0112128  <-> test loss :  0.0055705 |||\n",
      "epoch:  37 ||| train loss :  0.0139874  <-> test loss :  0.0078888 |||\n",
      "epoch:  38 ||| train loss :  0.0100904  <-> test loss :  0.0107076 |||\n",
      "epoch:  39 ||| train loss :  0.009655  <-> test loss :  0.7501847 |||\n",
      "epoch:  40 ||| train loss :  0.0165599  <-> test loss :  0.0250192 |||\n",
      "epoch:  41 ||| train loss :  0.0105503  <-> test loss :  0.003508 |||\n",
      "epoch:  42 ||| train loss :  0.0158482  <-> test loss :  0.0809524 |||\n",
      "epoch:  43 ||| train loss :  0.010567  <-> test loss :  0.0033656 |||\n",
      "epoch:  44 ||| train loss :  0.0129375  <-> test loss :  0.0045732 |||\n",
      "epoch:  45 ||| train loss :  0.0064713  <-> test loss :  0.0047452 |||\n",
      "epoch:  46 ||| train loss :  0.0094626  <-> test loss :  0.0098406 |||\n",
      "epoch:  47 ||| train loss :  0.008909  <-> test loss :  0.0040413 |||\n",
      "epoch:  48 ||| train loss :  0.0058809  <-> test loss :  0.0034287 |||\n",
      "epoch:  49 ||| train loss :  0.0062091  <-> test loss :  0.0026281 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.0061331  <-> test loss :  0.0020742 |||\n",
      "epoch:  51 ||| train loss :  0.0044944  <-> test loss :  0.0021094 |||\n",
      "save model\n",
      "epoch:  52 ||| train loss :  0.004314  <-> test loss :  0.0010913 |||\n",
      "epoch:  53 ||| train loss :  0.0046654  <-> test loss :  0.0019808 |||\n",
      "epoch:  54 ||| train loss :  0.0036005  <-> test loss :  0.0012867 |||\n",
      "epoch:  55 ||| train loss :  0.0038446  <-> test loss :  0.0014859 |||\n",
      "save model\n",
      "epoch:  56 ||| train loss :  0.0041402  <-> test loss :  0.0009699 |||\n",
      "epoch:  57 ||| train loss :  0.0030058  <-> test loss :  0.0013112 |||\n",
      "epoch:  58 ||| train loss :  0.0029952  <-> test loss :  0.0019382 |||\n",
      "epoch:  59 ||| train loss :  0.0038011  <-> test loss :  0.0017427 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165634993036.pth\n",
      "0.90625\n",
      "0.96875\n",
      "1.0\n",
      "1.0\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.90625\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9375\n",
      "1.0\n",
      "0.96875\n",
      "0.90625\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "0.90625\n",
      "1.0\n",
      "0.9619565217391305\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  3.2849811  <-> test loss :  7.7479248 |||\n",
      "epoch:  1 ||| train loss :  2.1673058  <-> test loss :  11.2840862 |||\n",
      "save model\n",
      "epoch:  2 ||| train loss :  1.6887627  <-> test loss :  5.8270378 |||\n",
      "epoch:  3 ||| train loss :  1.2968532  <-> test loss :  17.5960732 |||\n",
      "save model\n",
      "epoch:  4 ||| train loss :  1.0009789  <-> test loss :  2.136132 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  0.8512042  <-> test loss :  1.8001894 |||\n",
      "epoch:  6 ||| train loss :  0.7821193  <-> test loss :  4.2219157 |||\n",
      "epoch:  7 ||| train loss :  0.6314351  <-> test loss :  3.2876222 |||\n",
      "epoch:  8 ||| train loss :  0.5216328  <-> test loss :  4.9328208 |||\n",
      "save model\n",
      "epoch:  9 ||| train loss :  0.4603441  <-> test loss :  0.5285014 |||\n",
      "epoch:  10 ||| train loss :  0.3451719  <-> test loss :  4.8387198 |||\n",
      "epoch:  11 ||| train loss :  0.3189593  <-> test loss :  0.5832891 |||\n",
      "epoch:  12 ||| train loss :  0.2863921  <-> test loss :  1.9046669 |||\n",
      "epoch:  13 ||| train loss :  0.2132243  <-> test loss :  1.3229113 |||\n",
      "save model\n",
      "epoch:  14 ||| train loss :  0.1940054  <-> test loss :  0.178713 |||\n",
      "epoch:  15 ||| train loss :  0.1823573  <-> test loss :  1.2641518 |||\n",
      "epoch:  16 ||| train loss :  0.1994908  <-> test loss :  6.6182308 |||\n",
      "save model\n",
      "epoch:  17 ||| train loss :  0.1949878  <-> test loss :  0.0633673 |||\n",
      "epoch:  18 ||| train loss :  0.0933889  <-> test loss :  0.4616568 |||\n",
      "epoch:  19 ||| train loss :  0.0741228  <-> test loss :  0.1082042 |||\n",
      "epoch:  20 ||| train loss :  0.0583935  <-> test loss :  1.0060786 |||\n",
      "epoch:  21 ||| train loss :  0.0829509  <-> test loss :  0.270471 |||\n",
      "epoch:  22 ||| train loss :  0.0719027  <-> test loss :  0.2383649 |||\n",
      "epoch:  23 ||| train loss :  0.0444329  <-> test loss :  0.0854039 |||\n",
      "epoch:  24 ||| train loss :  0.033314  <-> test loss :  0.109648 |||\n",
      "save model\n",
      "epoch:  25 ||| train loss :  0.0300276  <-> test loss :  0.042232 |||\n",
      "save model\n",
      "epoch:  26 ||| train loss :  0.0213064  <-> test loss :  0.0185448 |||\n",
      "save model\n",
      "epoch:  27 ||| train loss :  0.018137  <-> test loss :  0.0175016 |||\n",
      "save model\n",
      "epoch:  28 ||| train loss :  0.0156545  <-> test loss :  0.0157941 |||\n",
      "epoch:  29 ||| train loss :  0.0137606  <-> test loss :  0.0194163 |||\n",
      "save model\n",
      "epoch:  30 ||| train loss :  0.0128892  <-> test loss :  0.0064584 |||\n",
      "epoch:  31 ||| train loss :  0.0108114  <-> test loss :  0.0114074 |||\n",
      "epoch:  32 ||| train loss :  0.0100183  <-> test loss :  0.0091693 |||\n",
      "save model\n",
      "epoch:  33 ||| train loss :  0.0092409  <-> test loss :  0.0055823 |||\n",
      "epoch:  34 ||| train loss :  0.0085806  <-> test loss :  0.0057445 |||\n",
      "epoch:  35 ||| train loss :  0.0085017  <-> test loss :  0.0069688 |||\n",
      "save model\n",
      "epoch:  36 ||| train loss :  0.0114297  <-> test loss :  0.0047659 |||\n",
      "epoch:  37 ||| train loss :  0.0118112  <-> test loss :  0.0064257 |||\n",
      "epoch:  38 ||| train loss :  0.0117513  <-> test loss :  0.0052698 |||\n",
      "epoch:  39 ||| train loss :  0.0125134  <-> test loss :  0.0171862 |||\n",
      "epoch:  40 ||| train loss :  0.0081582  <-> test loss :  0.0162718 |||\n",
      "save model\n",
      "epoch:  41 ||| train loss :  0.0087846  <-> test loss :  0.0039679 |||\n",
      "epoch:  42 ||| train loss :  0.0070998  <-> test loss :  0.0050618 |||\n",
      "save model\n",
      "epoch:  43 ||| train loss :  0.0074465  <-> test loss :  0.0030834 |||\n",
      "epoch:  44 ||| train loss :  0.0073553  <-> test loss :  0.0042653 |||\n",
      "save model\n",
      "epoch:  45 ||| train loss :  0.0051773  <-> test loss :  0.0025125 |||\n",
      "epoch:  46 ||| train loss :  0.00534  <-> test loss :  0.0040171 |||\n",
      "epoch:  47 ||| train loss :  0.0043819  <-> test loss :  0.0040467 |||\n",
      "epoch:  48 ||| train loss :  0.0049238  <-> test loss :  0.0036564 |||\n",
      "epoch:  49 ||| train loss :  0.0039681  <-> test loss :  0.0026607 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.0047255  <-> test loss :  0.0014198 |||\n",
      "epoch:  51 ||| train loss :  0.0035997  <-> test loss :  0.0026439 |||\n",
      "epoch:  52 ||| train loss :  0.0032847  <-> test loss :  0.0019162 |||\n",
      "save model\n",
      "epoch:  53 ||| train loss :  0.0039051  <-> test loss :  0.0012097 |||\n",
      "epoch:  54 ||| train loss :  0.00323  <-> test loss :  0.0012445 |||\n",
      "epoch:  55 ||| train loss :  0.0035583  <-> test loss :  0.0015032 |||\n",
      "epoch:  56 ||| train loss :  0.0032773  <-> test loss :  0.0013524 |||\n",
      "epoch:  57 ||| train loss :  0.0032607  <-> test loss :  0.0013611 |||\n",
      "epoch:  58 ||| train loss :  0.0028515  <-> test loss :  0.0018804 |||\n",
      "save model\n",
      "epoch:  59 ||| train loss :  0.002922  <-> test loss :  0.0011528 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635040808.pth\n",
      "0.9375\n",
      "0.96875\n",
      "0.90625\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "0.90625\n",
      "0.90625\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "0.90625\n",
      "0.90625\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "1.0\n",
      "0.9375\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "0.9538043478260869\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  3.3805749  <-> test loss :  3.1787682 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  2.1615154  <-> test loss :  2.7193837 |||\n",
      "epoch:  2 ||| train loss :  1.6218736  <-> test loss :  3.9621015 |||\n",
      "save model\n",
      "epoch:  3 ||| train loss :  1.2058826  <-> test loss :  2.293782 |||\n",
      "save model\n",
      "epoch:  4 ||| train loss :  0.9940905  <-> test loss :  1.9228109 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  0.8057828  <-> test loss :  1.0034447 |||\n",
      "epoch:  6 ||| train loss :  0.6163056  <-> test loss :  2.2765577 |||\n",
      "epoch:  7 ||| train loss :  0.5480546  <-> test loss :  1.9772553 |||\n",
      "epoch:  8 ||| train loss :  0.5136738  <-> test loss :  24.6288681 |||\n",
      "epoch:  9 ||| train loss :  0.4218107  <-> test loss :  4.2165432 |||\n",
      "epoch:  10 ||| train loss :  0.3117706  <-> test loss :  1.6200819 |||\n",
      "save model\n",
      "epoch:  11 ||| train loss :  0.2343671  <-> test loss :  0.4669787 |||\n",
      "save model\n",
      "epoch:  12 ||| train loss :  0.2302818  <-> test loss :  0.3063909 |||\n",
      "save model\n",
      "epoch:  13 ||| train loss :  0.1552319  <-> test loss :  0.2467635 |||\n",
      "epoch:  14 ||| train loss :  0.1717879  <-> test loss :  0.6623086 |||\n",
      "epoch:  15 ||| train loss :  0.1660213  <-> test loss :  0.55299 |||\n",
      "epoch:  16 ||| train loss :  0.1260436  <-> test loss :  0.4344841 |||\n",
      "epoch:  17 ||| train loss :  0.1096544  <-> test loss :  0.8265047 |||\n",
      "epoch:  18 ||| train loss :  0.0899038  <-> test loss :  1.1685318 |||\n",
      "save model\n",
      "epoch:  19 ||| train loss :  0.0928756  <-> test loss :  0.0843865 |||\n",
      "epoch:  20 ||| train loss :  0.0915568  <-> test loss :  0.1003483 |||\n",
      "epoch:  21 ||| train loss :  0.0526031  <-> test loss :  0.1571691 |||\n",
      "save model\n",
      "epoch:  22 ||| train loss :  0.0354095  <-> test loss :  0.0318368 |||\n",
      "epoch:  23 ||| train loss :  0.0264411  <-> test loss :  0.0341694 |||\n",
      "epoch:  24 ||| train loss :  0.0229936  <-> test loss :  0.0384802 |||\n",
      "save model\n",
      "epoch:  25 ||| train loss :  0.021876  <-> test loss :  0.0172816 |||\n",
      "save model\n",
      "epoch:  26 ||| train loss :  0.0188495  <-> test loss :  0.0135172 |||\n",
      "save model\n",
      "epoch:  27 ||| train loss :  0.0153124  <-> test loss :  0.0087459 |||\n",
      "epoch:  28 ||| train loss :  0.0153185  <-> test loss :  0.0106076 |||\n",
      "epoch:  29 ||| train loss :  0.0130106  <-> test loss :  0.0248161 |||\n",
      "save model\n",
      "epoch:  30 ||| train loss :  0.0145954  <-> test loss :  0.0066851 |||\n",
      "epoch:  31 ||| train loss :  0.012879  <-> test loss :  0.0077466 |||\n",
      "save model\n",
      "epoch:  32 ||| train loss :  0.0101946  <-> test loss :  0.0049428 |||\n",
      "epoch:  33 ||| train loss :  0.0093699  <-> test loss :  0.0107231 |||\n",
      "epoch:  34 ||| train loss :  0.0096749  <-> test loss :  0.0113033 |||\n",
      "epoch:  35 ||| train loss :  0.0096676  <-> test loss :  0.0144662 |||\n",
      "save model\n",
      "epoch:  36 ||| train loss :  0.0074714  <-> test loss :  0.0033697 |||\n",
      "epoch:  37 ||| train loss :  0.0078428  <-> test loss :  0.0065343 |||\n",
      "epoch:  38 ||| train loss :  0.0070656  <-> test loss :  0.0046122 |||\n",
      "save model\n",
      "epoch:  39 ||| train loss :  0.0082899  <-> test loss :  0.0028915 |||\n",
      "epoch:  40 ||| train loss :  0.0058805  <-> test loss :  0.0045226 |||\n",
      "epoch:  41 ||| train loss :  0.0058629  <-> test loss :  0.0035469 |||\n",
      "epoch:  42 ||| train loss :  0.0054961  <-> test loss :  0.0056577 |||\n",
      "save model\n",
      "epoch:  43 ||| train loss :  0.0058618  <-> test loss :  0.0017268 |||\n",
      "epoch:  44 ||| train loss :  0.004877  <-> test loss :  0.0022378 |||\n",
      "epoch:  45 ||| train loss :  0.0057718  <-> test loss :  0.0041898 |||\n",
      "epoch:  46 ||| train loss :  0.0064518  <-> test loss :  0.0021773 |||\n",
      "epoch:  47 ||| train loss :  0.0043185  <-> test loss :  0.002437 |||\n",
      "epoch:  48 ||| train loss :  0.0043535  <-> test loss :  0.0026582 |||\n",
      "epoch:  49 ||| train loss :  0.0041433  <-> test loss :  0.010001 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.0057536  <-> test loss :  0.000979 |||\n",
      "epoch:  51 ||| train loss :  0.0043375  <-> test loss :  0.0023036 |||\n",
      "epoch:  52 ||| train loss :  0.0035566  <-> test loss :  0.001331 |||\n",
      "save model\n",
      "epoch:  53 ||| train loss :  0.0034829  <-> test loss :  0.0008978 |||\n",
      "epoch:  54 ||| train loss :  0.0037155  <-> test loss :  0.0009916 |||\n",
      "epoch:  55 ||| train loss :  0.0029405  <-> test loss :  0.0014908 |||\n",
      "epoch:  56 ||| train loss :  0.0038642  <-> test loss :  0.0018994 |||\n",
      "epoch:  57 ||| train loss :  0.0036582  <-> test loss :  0.0009255 |||\n",
      "epoch:  58 ||| train loss :  0.0030546  <-> test loss :  0.0015822 |||\n",
      "epoch:  59 ||| train loss :  0.0027953  <-> test loss :  0.0011816 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635088705.pth\n",
      "0.875\n",
      "0.9375\n",
      "0.96875\n",
      "1.0\n",
      "0.9375\n",
      "1.0\n",
      "0.90625\n",
      "0.9375\n",
      "0.875\n",
      "0.90625\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "0.9375\n",
      "0.90625\n",
      "0.90625\n",
      "0.875\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "0.9402173913043478\n",
      "[0.9538043478260869, 0.9497282608695652, 0.9619565217391305, 0.9538043478260869, 0.9402173913043478]\n",
      "0.9519021739130435\n",
      "0.9519\n",
      "----------loss weight==2----------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  2.5612252  <-> test loss :  2.7211542 |||\n",
      "epoch:  1 ||| train loss :  1.6134696  <-> test loss :  4.5125666 |||\n",
      "epoch:  2 ||| train loss :  1.1824409  <-> test loss :  3.3134212 |||\n",
      "epoch:  3 ||| train loss :  0.9155769  <-> test loss :  6.6982694 |||\n",
      "epoch:  4 ||| train loss :  0.7123723  <-> test loss :  3.1709952 |||\n",
      "epoch:  5 ||| train loss :  0.6246327  <-> test loss :  4.7903929 |||\n",
      "epoch:  6 ||| train loss :  0.4969795  <-> test loss :  8.8688412 |||\n",
      "epoch:  7 ||| train loss :  0.4330071  <-> test loss :  4.8979282 |||\n",
      "epoch:  8 ||| train loss :  0.3559686  <-> test loss :  13.7516556 |||\n",
      "save model\n",
      "epoch:  9 ||| train loss :  0.2801446  <-> test loss :  1.0660357 |||\n",
      "save model\n",
      "epoch:  10 ||| train loss :  0.2519542  <-> test loss :  0.7531384 |||\n",
      "save model\n",
      "epoch:  11 ||| train loss :  0.2107297  <-> test loss :  0.4350811 |||\n",
      "save model\n",
      "epoch:  12 ||| train loss :  0.1992472  <-> test loss :  0.2256753 |||\n",
      "epoch:  13 ||| train loss :  0.1682923  <-> test loss :  3.6788468 |||\n",
      "epoch:  14 ||| train loss :  0.1978172  <-> test loss :  0.4320543 |||\n",
      "epoch:  15 ||| train loss :  0.1086443  <-> test loss :  0.5047608 |||\n",
      "epoch:  16 ||| train loss :  0.0870023  <-> test loss :  1.6423798 |||\n",
      "epoch:  17 ||| train loss :  0.0857692  <-> test loss :  0.4511892 |||\n",
      "save model\n",
      "epoch:  18 ||| train loss :  0.0783521  <-> test loss :  0.0982824 |||\n",
      "save model\n",
      "epoch:  19 ||| train loss :  0.0467757  <-> test loss :  0.0391609 |||\n",
      "epoch:  20 ||| train loss :  0.0472413  <-> test loss :  0.0548173 |||\n",
      "epoch:  21 ||| train loss :  0.0351147  <-> test loss :  0.0936782 |||\n",
      "epoch:  22 ||| train loss :  0.0488631  <-> test loss :  0.8900877 |||\n",
      "epoch:  23 ||| train loss :  0.0925032  <-> test loss :  1.0479029 |||\n",
      "save model\n",
      "epoch:  24 ||| train loss :  0.0495883  <-> test loss :  0.0206004 |||\n",
      "epoch:  25 ||| train loss :  0.0259329  <-> test loss :  0.0993007 |||\n",
      "epoch:  26 ||| train loss :  0.0262311  <-> test loss :  0.3004323 |||\n",
      "save model\n",
      "epoch:  27 ||| train loss :  0.03556  <-> test loss :  0.0170987 |||\n",
      "epoch:  28 ||| train loss :  0.0187802  <-> test loss :  0.0196498 |||\n",
      "save model\n",
      "epoch:  29 ||| train loss :  0.0182417  <-> test loss :  0.0097677 |||\n",
      "epoch:  30 ||| train loss :  0.0204084  <-> test loss :  0.1968497 |||\n",
      "epoch:  31 ||| train loss :  0.0210279  <-> test loss :  0.0328571 |||\n",
      "epoch:  32 ||| train loss :  0.0176186  <-> test loss :  0.2563578 |||\n",
      "epoch:  33 ||| train loss :  0.0193459  <-> test loss :  0.0177617 |||\n",
      "epoch:  34 ||| train loss :  0.015748  <-> test loss :  0.0337452 |||\n",
      "save model\n",
      "epoch:  35 ||| train loss :  0.0099048  <-> test loss :  0.0061335 |||\n",
      "save model\n",
      "epoch:  36 ||| train loss :  0.0135216  <-> test loss :  0.0059489 |||\n",
      "save model\n",
      "epoch:  37 ||| train loss :  0.0069293  <-> test loss :  0.0032746 |||\n",
      "save model\n",
      "epoch:  38 ||| train loss :  0.0053471  <-> test loss :  0.0018396 |||\n",
      "epoch:  39 ||| train loss :  0.0045708  <-> test loss :  0.0042138 |||\n",
      "epoch:  40 ||| train loss :  0.0047099  <-> test loss :  0.0324363 |||\n",
      "epoch:  41 ||| train loss :  0.0058789  <-> test loss :  0.0031014 |||\n",
      "epoch:  42 ||| train loss :  0.0047689  <-> test loss :  0.0022782 |||\n",
      "save model\n",
      "epoch:  43 ||| train loss :  0.0042981  <-> test loss :  0.0011025 |||\n",
      "epoch:  44 ||| train loss :  0.0041623  <-> test loss :  0.0027143 |||\n",
      "epoch:  45 ||| train loss :  0.0035238  <-> test loss :  0.0024207 |||\n",
      "epoch:  46 ||| train loss :  0.0034009  <-> test loss :  0.0026529 |||\n",
      "epoch:  47 ||| train loss :  0.0028935  <-> test loss :  0.0014376 |||\n",
      "epoch:  48 ||| train loss :  0.0031644  <-> test loss :  0.0016359 |||\n",
      "epoch:  49 ||| train loss :  0.0028734  <-> test loss :  0.0015283 |||\n",
      "epoch:  50 ||| train loss :  0.0029943  <-> test loss :  0.0016341 |||\n",
      "save model\n",
      "epoch:  51 ||| train loss :  0.0023434  <-> test loss :  0.0007079 |||\n",
      "epoch:  52 ||| train loss :  0.0023609  <-> test loss :  0.0014498 |||\n",
      "epoch:  53 ||| train loss :  0.002622  <-> test loss :  0.0009958 |||\n",
      "epoch:  54 ||| train loss :  0.0027313  <-> test loss :  0.0007199 |||\n",
      "save model\n",
      "epoch:  55 ||| train loss :  0.0028275  <-> test loss :  0.0003981 |||\n",
      "epoch:  56 ||| train loss :  0.0027175  <-> test loss :  0.0011024 |||\n",
      "epoch:  57 ||| train loss :  0.0021476  <-> test loss :  0.0015438 |||\n",
      "epoch:  58 ||| train loss :  0.0022322  <-> test loss :  0.0007093 |||\n",
      "epoch:  59 ||| train loss :  0.0018015  <-> test loss :  0.0007445 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635136450.pth\n",
      "0.9375\n",
      "0.96875\n",
      "0.90625\n",
      "1.0\n",
      "0.9375\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.90625\n",
      "0.90625\n",
      "1.0\n",
      "0.9375\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.96875\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  2.5978955  <-> test loss :  2.9733844 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  1.6959148  <-> test loss :  1.6945915 |||\n",
      "epoch:  2 ||| train loss :  1.2207972  <-> test loss :  6.1402235 |||\n",
      "epoch:  3 ||| train loss :  1.0229274  <-> test loss :  3.5830626 |||\n",
      "epoch:  4 ||| train loss :  0.7498745  <-> test loss :  2.5402961 |||\n",
      "epoch:  5 ||| train loss :  0.6244393  <-> test loss :  5.7792225 |||\n",
      "save model\n",
      "epoch:  6 ||| train loss :  0.5328749  <-> test loss :  0.4773517 |||\n",
      "epoch:  7 ||| train loss :  0.4378365  <-> test loss :  0.6328878 |||\n",
      "epoch:  8 ||| train loss :  0.3507174  <-> test loss :  1.280248 |||\n",
      "epoch:  9 ||| train loss :  0.3014392  <-> test loss :  0.7070901 |||\n",
      "epoch:  10 ||| train loss :  0.2806923  <-> test loss :  1.9499314 |||\n",
      "epoch:  11 ||| train loss :  0.241883  <-> test loss :  1.64204 |||\n",
      "save model\n",
      "epoch:  12 ||| train loss :  0.1808831  <-> test loss :  0.4249812 |||\n",
      "epoch:  13 ||| train loss :  0.1780958  <-> test loss :  1.3058045 |||\n",
      "save model\n",
      "epoch:  14 ||| train loss :  0.1391654  <-> test loss :  0.2966898 |||\n",
      "epoch:  15 ||| train loss :  0.1149896  <-> test loss :  1.6946619 |||\n",
      "save model\n",
      "epoch:  16 ||| train loss :  0.092925  <-> test loss :  0.0863187 |||\n",
      "epoch:  17 ||| train loss :  0.0630113  <-> test loss :  0.4654289 |||\n",
      "save model\n",
      "epoch:  18 ||| train loss :  0.0492643  <-> test loss :  0.0587068 |||\n",
      "epoch:  19 ||| train loss :  0.0386657  <-> test loss :  1.0349625 |||\n",
      "epoch:  20 ||| train loss :  0.0358253  <-> test loss :  0.0608602 |||\n",
      "epoch:  21 ||| train loss :  0.0665525  <-> test loss :  9.2138748 |||\n",
      "epoch:  22 ||| train loss :  0.1743225  <-> test loss :  2.7429287 |||\n",
      "epoch:  23 ||| train loss :  0.1048759  <-> test loss :  0.0642608 |||\n",
      "epoch:  24 ||| train loss :  0.0456063  <-> test loss :  1.8099103 |||\n",
      "save model\n",
      "epoch:  25 ||| train loss :  0.0471787  <-> test loss :  0.0248259 |||\n",
      "epoch:  26 ||| train loss :  0.0227679  <-> test loss :  0.045492 |||\n",
      "epoch:  27 ||| train loss :  0.0232481  <-> test loss :  0.0363775 |||\n",
      "save model\n",
      "epoch:  28 ||| train loss :  0.0134882  <-> test loss :  0.0113349 |||\n",
      "epoch:  29 ||| train loss :  0.0157422  <-> test loss :  0.024232 |||\n",
      "save model\n",
      "epoch:  30 ||| train loss :  0.0140829  <-> test loss :  0.0087053 |||\n",
      "save model\n",
      "epoch:  31 ||| train loss :  0.0087555  <-> test loss :  0.0050763 |||\n",
      "epoch:  32 ||| train loss :  0.0086666  <-> test loss :  0.0053644 |||\n",
      "epoch:  33 ||| train loss :  0.0088388  <-> test loss :  0.0081417 |||\n",
      "save model\n",
      "epoch:  34 ||| train loss :  0.007156  <-> test loss :  0.004066 |||\n",
      "save model\n",
      "epoch:  35 ||| train loss :  0.0099543  <-> test loss :  0.0035064 |||\n",
      "epoch:  36 ||| train loss :  0.0068939  <-> test loss :  0.0066176 |||\n",
      "epoch:  37 ||| train loss :  0.0063748  <-> test loss :  0.0040654 |||\n",
      "save model\n",
      "epoch:  38 ||| train loss :  0.005418  <-> test loss :  0.00262 |||\n",
      "epoch:  39 ||| train loss :  0.0059158  <-> test loss :  0.0144699 |||\n",
      "epoch:  40 ||| train loss :  0.0064836  <-> test loss :  0.0122236 |||\n",
      "save model\n",
      "epoch:  41 ||| train loss :  0.0053518  <-> test loss :  0.0022338 |||\n",
      "epoch:  42 ||| train loss :  0.0049865  <-> test loss :  0.0063935 |||\n",
      "save model\n",
      "epoch:  43 ||| train loss :  0.0048721  <-> test loss :  0.002 |||\n",
      "epoch:  44 ||| train loss :  0.004986  <-> test loss :  0.0027848 |||\n",
      "save model\n",
      "epoch:  45 ||| train loss :  0.0043748  <-> test loss :  0.0014315 |||\n",
      "epoch:  46 ||| train loss :  0.0048533  <-> test loss :  0.0015799 |||\n",
      "epoch:  47 ||| train loss :  0.003465  <-> test loss :  0.001495 |||\n",
      "save model\n",
      "epoch:  48 ||| train loss :  0.003108  <-> test loss :  0.0011119 |||\n",
      "epoch:  49 ||| train loss :  0.0028471  <-> test loss :  0.0013716 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.0029796  <-> test loss :  0.0010235 |||\n",
      "epoch:  51 ||| train loss :  0.0025103  <-> test loss :  0.0020162 |||\n",
      "save model\n",
      "epoch:  52 ||| train loss :  0.0030509  <-> test loss :  0.0007204 |||\n",
      "epoch:  53 ||| train loss :  0.0025002  <-> test loss :  0.0010778 |||\n",
      "epoch:  54 ||| train loss :  0.0023517  <-> test loss :  0.0010729 |||\n",
      "epoch:  55 ||| train loss :  0.0023679  <-> test loss :  0.0009395 |||\n",
      "epoch:  56 ||| train loss :  0.0025892  <-> test loss :  0.0011841 |||\n",
      "epoch:  57 ||| train loss :  0.0027601  <-> test loss :  0.0012152 |||\n",
      "epoch:  58 ||| train loss :  0.002401  <-> test loss :  0.0007761 |||\n",
      "epoch:  59 ||| train loss :  0.002099  <-> test loss :  0.0013991 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635184197.pth\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "1.0\n",
      "1.0\n",
      "0.9375\n",
      "0.90625\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "0.967391304347826\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  2.5210973  <-> test loss :  25.567585 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  1.6451985  <-> test loss :  3.2259152 |||\n",
      "save model\n",
      "epoch:  2 ||| train loss :  1.2565775  <-> test loss :  1.9715161 |||\n",
      "epoch:  3 ||| train loss :  0.9316034  <-> test loss :  2.6819685 |||\n",
      "epoch:  4 ||| train loss :  0.8401869  <-> test loss :  4.2806764 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  0.6535851  <-> test loss :  0.8076971 |||\n",
      "epoch:  6 ||| train loss :  0.5182841  <-> test loss :  0.9163473 |||\n",
      "epoch:  7 ||| train loss :  0.4546892  <-> test loss :  0.9714994 |||\n",
      "epoch:  8 ||| train loss :  0.4397946  <-> test loss :  0.9186291 |||\n",
      "epoch:  9 ||| train loss :  0.3189161  <-> test loss :  1.6732618 |||\n",
      "epoch:  10 ||| train loss :  0.2877268  <-> test loss :  1.0116026 |||\n",
      "epoch:  11 ||| train loss :  0.2486781  <-> test loss :  4.8875861 |||\n",
      "epoch:  12 ||| train loss :  0.2219644  <-> test loss :  1.3062873 |||\n",
      "epoch:  13 ||| train loss :  0.2144451  <-> test loss :  1.2816105 |||\n",
      "save model\n",
      "epoch:  14 ||| train loss :  0.1616878  <-> test loss :  0.2333402 |||\n",
      "epoch:  15 ||| train loss :  0.1158886  <-> test loss :  0.4442855 |||\n",
      "epoch:  16 ||| train loss :  0.0863974  <-> test loss :  3.7864375 |||\n",
      "epoch:  17 ||| train loss :  0.0796318  <-> test loss :  0.3145355 |||\n",
      "save model\n",
      "epoch:  18 ||| train loss :  0.0861183  <-> test loss :  0.1561569 |||\n",
      "save model\n",
      "epoch:  19 ||| train loss :  0.0715473  <-> test loss :  0.0319766 |||\n",
      "epoch:  20 ||| train loss :  0.079665  <-> test loss :  5.2325182 |||\n",
      "epoch:  21 ||| train loss :  0.0911473  <-> test loss :  0.271604 |||\n",
      "save model\n",
      "epoch:  22 ||| train loss :  0.0688261  <-> test loss :  0.0306794 |||\n",
      "save model\n",
      "epoch:  23 ||| train loss :  0.0362294  <-> test loss :  0.0216068 |||\n",
      "epoch:  24 ||| train loss :  0.0344717  <-> test loss :  1.1374511 |||\n",
      "epoch:  25 ||| train loss :  0.0502191  <-> test loss :  0.2884153 |||\n",
      "epoch:  26 ||| train loss :  0.0641035  <-> test loss :  0.1089334 |||\n",
      "epoch:  27 ||| train loss :  0.0344081  <-> test loss :  0.0864093 |||\n",
      "save model\n",
      "epoch:  28 ||| train loss :  0.0238357  <-> test loss :  0.0214388 |||\n",
      "epoch:  29 ||| train loss :  0.1089179  <-> test loss :  0.0463871 |||\n",
      "epoch:  30 ||| train loss :  0.0674594  <-> test loss :  0.0487465 |||\n",
      "epoch:  31 ||| train loss :  0.0497845  <-> test loss :  0.6359543 |||\n",
      "epoch:  32 ||| train loss :  0.0305592  <-> test loss :  0.0351481 |||\n",
      "epoch:  33 ||| train loss :  0.0245388  <-> test loss :  0.0435472 |||\n",
      "epoch:  34 ||| train loss :  0.0167176  <-> test loss :  0.0255114 |||\n",
      "save model\n",
      "epoch:  35 ||| train loss :  0.0126258  <-> test loss :  0.0144473 |||\n",
      "epoch:  36 ||| train loss :  0.0124892  <-> test loss :  0.0389737 |||\n",
      "save model\n",
      "epoch:  37 ||| train loss :  0.0084882  <-> test loss :  0.0021008 |||\n",
      "epoch:  38 ||| train loss :  0.0074188  <-> test loss :  0.0043083 |||\n",
      "epoch:  39 ||| train loss :  0.0057107  <-> test loss :  0.004806 |||\n",
      "epoch:  40 ||| train loss :  0.006324  <-> test loss :  0.0033931 |||\n",
      "epoch:  41 ||| train loss :  0.0047072  <-> test loss :  0.0024465 |||\n",
      "epoch:  42 ||| train loss :  0.0057175  <-> test loss :  0.0030529 |||\n",
      "epoch:  43 ||| train loss :  0.0062724  <-> test loss :  0.0118513 |||\n",
      "save model\n",
      "epoch:  44 ||| train loss :  0.0039097  <-> test loss :  0.0017816 |||\n",
      "save model\n",
      "epoch:  45 ||| train loss :  0.0036114  <-> test loss :  0.0012264 |||\n",
      "save model\n",
      "epoch:  46 ||| train loss :  0.0031849  <-> test loss :  0.0011511 |||\n",
      "epoch:  47 ||| train loss :  0.0037431  <-> test loss :  0.0016206 |||\n",
      "epoch:  48 ||| train loss :  0.0032931  <-> test loss :  0.0014793 |||\n",
      "epoch:  49 ||| train loss :  0.0029743  <-> test loss :  0.0020895 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.0031989  <-> test loss :  0.0009827 |||\n",
      "epoch:  51 ||| train loss :  0.0031043  <-> test loss :  0.0010106 |||\n",
      "epoch:  52 ||| train loss :  0.0026786  <-> test loss :  0.0013166 |||\n",
      "epoch:  53 ||| train loss :  0.0026492  <-> test loss :  0.0011548 |||\n",
      "epoch:  54 ||| train loss :  0.0027903  <-> test loss :  0.0013287 |||\n",
      "save model\n",
      "epoch:  55 ||| train loss :  0.0023054  <-> test loss :  0.0009683 |||\n",
      "epoch:  56 ||| train loss :  0.0021866  <-> test loss :  0.0011515 |||\n",
      "save model\n",
      "epoch:  57 ||| train loss :  0.002153  <-> test loss :  0.0007586 |||\n",
      "epoch:  58 ||| train loss :  0.0023617  <-> test loss :  0.0009743 |||\n",
      "epoch:  59 ||| train loss :  0.0025171  <-> test loss :  0.0014168 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635232018.pth\n",
      "0.875\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "0.96875\n",
      "1.0\n",
      "0.84375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "1.0\n",
      "1.0\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "1.0\n",
      "1.0\n",
      "0.9551630434782609\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  2.4966805  <-> test loss :  3.6511056 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  1.5812182  <-> test loss :  2.2236009 |||\n",
      "epoch:  2 ||| train loss :  1.2021838  <-> test loss :  2.6290607 |||\n",
      "epoch:  3 ||| train loss :  0.903665  <-> test loss :  7.0852489 |||\n",
      "epoch:  4 ||| train loss :  0.7393811  <-> test loss :  3.7107933 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  0.5996233  <-> test loss :  2.0059917 |||\n",
      "epoch:  6 ||| train loss :  0.4677434  <-> test loss :  2.6837258 |||\n",
      "epoch:  7 ||| train loss :  0.4309165  <-> test loss :  4.9092741 |||\n",
      "save model\n",
      "epoch:  8 ||| train loss :  0.3404442  <-> test loss :  0.7868913 |||\n",
      "epoch:  9 ||| train loss :  0.3174452  <-> test loss :  0.8972527 |||\n",
      "epoch:  10 ||| train loss :  0.256585  <-> test loss :  5.0680342 |||\n",
      "epoch:  11 ||| train loss :  0.2238239  <-> test loss :  5.1274099 |||\n",
      "save model\n",
      "epoch:  12 ||| train loss :  0.1825925  <-> test loss :  0.5329377 |||\n",
      "epoch:  13 ||| train loss :  0.1227812  <-> test loss :  3.7481165 |||\n",
      "save model\n",
      "epoch:  14 ||| train loss :  0.1113306  <-> test loss :  0.416887 |||\n",
      "epoch:  15 ||| train loss :  0.128576  <-> test loss :  0.778477 |||\n",
      "epoch:  16 ||| train loss :  0.1228756  <-> test loss :  0.5328842 |||\n",
      "save model\n",
      "epoch:  17 ||| train loss :  0.0905367  <-> test loss :  0.1362648 |||\n",
      "epoch:  18 ||| train loss :  0.0564338  <-> test loss :  0.1528815 |||\n",
      "epoch:  19 ||| train loss :  0.0610058  <-> test loss :  1.1793571 |||\n",
      "save model\n",
      "epoch:  20 ||| train loss :  0.0468913  <-> test loss :  0.116005 |||\n",
      "save model\n",
      "epoch:  21 ||| train loss :  0.0297036  <-> test loss :  0.030587 |||\n",
      "save model\n",
      "epoch:  22 ||| train loss :  0.0257593  <-> test loss :  0.0149108 |||\n",
      "epoch:  23 ||| train loss :  0.0350488  <-> test loss :  1.8119856 |||\n",
      "epoch:  24 ||| train loss :  0.03374  <-> test loss :  0.1774708 |||\n",
      "epoch:  25 ||| train loss :  0.0279152  <-> test loss :  0.0388008 |||\n",
      "epoch:  26 ||| train loss :  0.020313  <-> test loss :  0.0333639 |||\n",
      "epoch:  27 ||| train loss :  0.0144225  <-> test loss :  0.1318156 |||\n",
      "save model\n",
      "epoch:  28 ||| train loss :  0.0129971  <-> test loss :  0.0069601 |||\n",
      "save model\n",
      "epoch:  29 ||| train loss :  0.0097583  <-> test loss :  0.0068697 |||\n",
      "save model\n",
      "epoch:  30 ||| train loss :  0.0090045  <-> test loss :  0.0063696 |||\n",
      "epoch:  31 ||| train loss :  0.0077195  <-> test loss :  0.012549 |||\n",
      "save model\n",
      "epoch:  32 ||| train loss :  0.0062388  <-> test loss :  0.0032425 |||\n",
      "epoch:  33 ||| train loss :  0.0079041  <-> test loss :  0.0032578 |||\n",
      "epoch:  34 ||| train loss :  0.0058859  <-> test loss :  0.0037412 |||\n",
      "epoch:  35 ||| train loss :  0.005719  <-> test loss :  0.003253 |||\n",
      "epoch:  36 ||| train loss :  0.0059564  <-> test loss :  0.0042593 |||\n",
      "save model\n",
      "epoch:  37 ||| train loss :  0.0054369  <-> test loss :  0.0027766 |||\n",
      "epoch:  38 ||| train loss :  0.0044992  <-> test loss :  0.0039377 |||\n",
      "save model\n",
      "epoch:  39 ||| train loss :  0.0049407  <-> test loss :  0.0026894 |||\n",
      "epoch:  40 ||| train loss :  0.0074711  <-> test loss :  0.0051348 |||\n",
      "save model\n",
      "epoch:  41 ||| train loss :  0.0046  <-> test loss :  0.0020358 |||\n",
      "epoch:  42 ||| train loss :  0.0068263  <-> test loss :  0.0033721 |||\n",
      "epoch:  43 ||| train loss :  0.0056262  <-> test loss :  0.0072348 |||\n",
      "epoch:  44 ||| train loss :  0.0046326  <-> test loss :  0.0038581 |||\n",
      "save model\n",
      "epoch:  45 ||| train loss :  0.0047694  <-> test loss :  0.0017152 |||\n",
      "epoch:  46 ||| train loss :  0.0048643  <-> test loss :  0.0026637 |||\n",
      "save model\n",
      "epoch:  47 ||| train loss :  0.0037879  <-> test loss :  0.0014313 |||\n",
      "epoch:  48 ||| train loss :  0.0034511  <-> test loss :  0.0061175 |||\n",
      "save model\n",
      "epoch:  49 ||| train loss :  0.002763  <-> test loss :  0.0013498 |||\n",
      "epoch:  50 ||| train loss :  0.0023936  <-> test loss :  0.0015103 |||\n",
      "epoch:  51 ||| train loss :  0.0026128  <-> test loss :  0.0013716 |||\n",
      "save model\n",
      "epoch:  52 ||| train loss :  0.0025903  <-> test loss :  0.0011504 |||\n",
      "save model\n",
      "epoch:  53 ||| train loss :  0.0029164  <-> test loss :  0.0010961 |||\n",
      "save model\n",
      "epoch:  54 ||| train loss :  0.0021663  <-> test loss :  0.000729 |||\n",
      "epoch:  55 ||| train loss :  0.002295  <-> test loss :  0.0035156 |||\n",
      "epoch:  56 ||| train loss :  0.0020813  <-> test loss :  0.0008621 |||\n",
      "epoch:  57 ||| train loss :  0.0028573  <-> test loss :  0.0018585 |||\n",
      "epoch:  58 ||| train loss :  0.0018773  <-> test loss :  0.0008063 |||\n",
      "epoch:  59 ||| train loss :  0.0029191  <-> test loss :  0.0012572 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635279813.pth\n",
      "0.90625\n",
      "0.9375\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "0.875\n",
      "0.9375\n",
      "1.0\n",
      "0.9375\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "0.90625\n",
      "0.875\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "0.9497282608695652\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  2.5673485  <-> test loss :  5.0929346 |||\n",
      "epoch:  1 ||| train loss :  1.7051739  <-> test loss :  7.3630905 |||\n",
      "epoch:  2 ||| train loss :  1.2103553  <-> test loss :  5.2149119 |||\n",
      "save model\n",
      "epoch:  3 ||| train loss :  1.0036372  <-> test loss :  2.6874051 |||\n",
      "epoch:  4 ||| train loss :  0.7438112  <-> test loss :  6.1077609 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  0.6354342  <-> test loss :  2.2256753 |||\n",
      "save model\n",
      "epoch:  6 ||| train loss :  0.5132784  <-> test loss :  1.3262012 |||\n",
      "epoch:  7 ||| train loss :  0.4365961  <-> test loss :  1.5382073 |||\n",
      "save model\n",
      "epoch:  8 ||| train loss :  0.3457654  <-> test loss :  0.6561983 |||\n",
      "epoch:  9 ||| train loss :  0.3023025  <-> test loss :  2.9373288 |||\n",
      "epoch:  10 ||| train loss :  0.3013684  <-> test loss :  1.6798216 |||\n",
      "epoch:  11 ||| train loss :  0.221789  <-> test loss :  1.4871809 |||\n",
      "epoch:  12 ||| train loss :  0.1832525  <-> test loss :  1.5444305 |||\n",
      "save model\n",
      "epoch:  13 ||| train loss :  0.1612931  <-> test loss :  0.5311474 |||\n",
      "epoch:  14 ||| train loss :  0.1805602  <-> test loss :  1.8076642 |||\n",
      "save model\n",
      "epoch:  15 ||| train loss :  0.1408092  <-> test loss :  0.4024729 |||\n",
      "epoch:  16 ||| train loss :  0.1097993  <-> test loss :  1.5050678 |||\n",
      "save model\n",
      "epoch:  17 ||| train loss :  0.0943649  <-> test loss :  0.0973993 |||\n",
      "epoch:  18 ||| train loss :  0.0851331  <-> test loss :  2.163178 |||\n",
      "save model\n",
      "epoch:  19 ||| train loss :  0.0795578  <-> test loss :  0.036699 |||\n",
      "epoch:  20 ||| train loss :  0.0635951  <-> test loss :  0.2368652 |||\n",
      "save model\n",
      "epoch:  21 ||| train loss :  0.0446587  <-> test loss :  0.0300961 |||\n",
      "epoch:  22 ||| train loss :  0.0313212  <-> test loss :  0.0399886 |||\n",
      "epoch:  23 ||| train loss :  0.0242369  <-> test loss :  0.0555268 |||\n",
      "epoch:  24 ||| train loss :  0.0193131  <-> test loss :  0.3246222 |||\n",
      "epoch:  25 ||| train loss :  0.019995  <-> test loss :  0.1183459 |||\n",
      "save model\n",
      "epoch:  26 ||| train loss :  0.0197051  <-> test loss :  0.0118524 |||\n",
      "save model\n",
      "epoch:  27 ||| train loss :  0.0126824  <-> test loss :  0.0083148 |||\n",
      "epoch:  28 ||| train loss :  0.0120949  <-> test loss :  0.0364439 |||\n",
      "save model\n",
      "epoch:  29 ||| train loss :  0.0128775  <-> test loss :  0.0060316 |||\n",
      "epoch:  30 ||| train loss :  0.0108367  <-> test loss :  0.0122437 |||\n",
      "save model\n",
      "epoch:  31 ||| train loss :  0.0087998  <-> test loss :  0.0040117 |||\n",
      "save model\n",
      "epoch:  32 ||| train loss :  0.0077829  <-> test loss :  0.0026281 |||\n",
      "epoch:  33 ||| train loss :  0.006366  <-> test loss :  0.0041264 |||\n",
      "epoch:  34 ||| train loss :  0.0055804  <-> test loss :  0.0066198 |||\n",
      "epoch:  35 ||| train loss :  0.0075889  <-> test loss :  0.0791008 |||\n",
      "epoch:  36 ||| train loss :  0.0219962  <-> test loss :  1.1193092 |||\n",
      "epoch:  37 ||| train loss :  0.0239229  <-> test loss :  0.2124144 |||\n",
      "save model\n",
      "epoch:  38 ||| train loss :  0.0105187  <-> test loss :  0.0024846 |||\n",
      "epoch:  39 ||| train loss :  0.0115788  <-> test loss :  0.0068194 |||\n",
      "epoch:  40 ||| train loss :  0.0068647  <-> test loss :  0.0173144 |||\n",
      "epoch:  41 ||| train loss :  0.0059404  <-> test loss :  0.0054121 |||\n",
      "save model\n",
      "epoch:  42 ||| train loss :  0.005082  <-> test loss :  0.0024222 |||\n",
      "epoch:  43 ||| train loss :  0.0053147  <-> test loss :  0.0057434 |||\n",
      "save model\n",
      "epoch:  44 ||| train loss :  0.0038656  <-> test loss :  0.0018151 |||\n",
      "epoch:  45 ||| train loss :  0.0114265  <-> test loss :  2.3162482 |||\n",
      "epoch:  46 ||| train loss :  0.0384351  <-> test loss :  0.7154057 |||\n",
      "epoch:  47 ||| train loss :  0.0812477  <-> test loss :  1.3010492 |||\n",
      "epoch:  48 ||| train loss :  0.0832031  <-> test loss :  0.0985373 |||\n",
      "epoch:  49 ||| train loss :  0.0587445  <-> test loss :  1.2976801 |||\n",
      "epoch:  50 ||| train loss :  0.0248101  <-> test loss :  0.0071832 |||\n",
      "epoch:  51 ||| train loss :  0.0093709  <-> test loss :  0.0043216 |||\n",
      "epoch:  52 ||| train loss :  0.0085738  <-> test loss :  0.0032317 |||\n",
      "epoch:  53 ||| train loss :  0.0063891  <-> test loss :  0.0020116 |||\n",
      "epoch:  54 ||| train loss :  0.0050407  <-> test loss :  0.0038397 |||\n",
      "epoch:  55 ||| train loss :  0.006445  <-> test loss :  0.0021859 |||\n",
      "save model\n",
      "epoch:  56 ||| train loss :  0.0050133  <-> test loss :  0.0016065 |||\n",
      "save model\n",
      "epoch:  57 ||| train loss :  0.0046333  <-> test loss :  0.0015927 |||\n",
      "epoch:  58 ||| train loss :  0.0045345  <-> test loss :  0.0022486 |||\n",
      "epoch:  59 ||| train loss :  0.0040321  <-> test loss :  0.0016249 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635327685.pth\n",
      "0.90625\n",
      "0.9375\n",
      "0.9375\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "0.90625\n",
      "0.9375\n",
      "0.90625\n",
      "0.9375\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "0.9375\n",
      "0.90625\n",
      "1.0\n",
      "0.9375\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "1.0\n",
      "0.9551630434782609\n",
      "[0.96875, 0.967391304347826, 0.9551630434782609, 0.9497282608695652, 0.9551630434782609]\n",
      "0.9592391304347826\n",
      "0.9592\n",
      "----------loss weight==1.25----------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  2.3116836  <-> test loss :  6.8295283 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  1.5097443  <-> test loss :  1.619761 |||\n",
      "save model\n",
      "epoch:  2 ||| train loss :  1.0825347  <-> test loss :  0.9346505 |||\n",
      "epoch:  3 ||| train loss :  0.8475413  <-> test loss :  8.9683065 |||\n",
      "epoch:  4 ||| train loss :  0.6790833  <-> test loss :  1.1034251 |||\n",
      "epoch:  5 ||| train loss :  0.5938799  <-> test loss :  3.2069111 |||\n",
      "epoch:  6 ||| train loss :  0.4684399  <-> test loss :  2.5159588 |||\n",
      "epoch:  7 ||| train loss :  0.4087348  <-> test loss :  1.7287174 |||\n",
      "save model\n",
      "epoch:  8 ||| train loss :  0.3279132  <-> test loss :  0.4530762 |||\n",
      "epoch:  9 ||| train loss :  0.255697  <-> test loss :  1.1632922 |||\n",
      "epoch:  10 ||| train loss :  0.2626059  <-> test loss :  1.5298345 |||\n",
      "epoch:  11 ||| train loss :  0.2145312  <-> test loss :  0.9274086 |||\n",
      "epoch:  12 ||| train loss :  0.1646778  <-> test loss :  0.5950201 |||\n",
      "save model\n",
      "epoch:  13 ||| train loss :  0.135807  <-> test loss :  0.4140761 |||\n",
      "save model\n",
      "epoch:  14 ||| train loss :  0.1254254  <-> test loss :  0.1339859 |||\n",
      "epoch:  15 ||| train loss :  0.1210346  <-> test loss :  0.5057161 |||\n",
      "epoch:  16 ||| train loss :  0.1076174  <-> test loss :  0.1815594 |||\n",
      "epoch:  17 ||| train loss :  0.0806692  <-> test loss :  0.2322811 |||\n",
      "save model\n",
      "epoch:  18 ||| train loss :  0.0655806  <-> test loss :  0.1226925 |||\n",
      "epoch:  19 ||| train loss :  0.0428433  <-> test loss :  0.757246 |||\n",
      "epoch:  20 ||| train loss :  0.0412987  <-> test loss :  0.1927961 |||\n",
      "epoch:  21 ||| train loss :  0.0445425  <-> test loss :  0.5682331 |||\n",
      "save model\n",
      "epoch:  22 ||| train loss :  0.0331251  <-> test loss :  0.0196724 |||\n",
      "epoch:  23 ||| train loss :  0.0157062  <-> test loss :  0.0304456 |||\n",
      "epoch:  24 ||| train loss :  0.0292045  <-> test loss :  0.4324467 |||\n",
      "epoch:  25 ||| train loss :  0.0676516  <-> test loss :  0.1920701 |||\n",
      "epoch:  26 ||| train loss :  0.0407422  <-> test loss :  0.0635078 |||\n",
      "epoch:  27 ||| train loss :  0.0222892  <-> test loss :  0.0895618 |||\n",
      "save model\n",
      "epoch:  28 ||| train loss :  0.017098  <-> test loss :  0.011326 |||\n",
      "save model\n",
      "epoch:  29 ||| train loss :  0.012468  <-> test loss :  0.0101263 |||\n",
      "epoch:  30 ||| train loss :  0.0078073  <-> test loss :  0.0187015 |||\n",
      "epoch:  31 ||| train loss :  0.0096111  <-> test loss :  0.0159709 |||\n",
      "save model\n",
      "epoch:  32 ||| train loss :  0.0095304  <-> test loss :  0.0078032 |||\n",
      "epoch:  33 ||| train loss :  0.0114422  <-> test loss :  0.0829746 |||\n",
      "epoch:  34 ||| train loss :  0.0536752  <-> test loss :  0.3111882 |||\n",
      "epoch:  35 ||| train loss :  0.0147178  <-> test loss :  0.0100179 |||\n",
      "epoch:  36 ||| train loss :  0.0164867  <-> test loss :  0.337382 |||\n",
      "epoch:  37 ||| train loss :  0.0263414  <-> test loss :  0.0854017 |||\n",
      "epoch:  38 ||| train loss :  0.0137272  <-> test loss :  0.0161168 |||\n",
      "save model\n",
      "epoch:  39 ||| train loss :  0.0071036  <-> test loss :  0.0034403 |||\n",
      "save model\n",
      "epoch:  40 ||| train loss :  0.0047694  <-> test loss :  0.002861 |||\n",
      "save model\n",
      "epoch:  41 ||| train loss :  0.0038859  <-> test loss :  0.0019077 |||\n",
      "epoch:  42 ||| train loss :  0.0040158  <-> test loss :  0.0019851 |||\n",
      "save model\n",
      "epoch:  43 ||| train loss :  0.0035467  <-> test loss :  0.0011842 |||\n",
      "epoch:  44 ||| train loss :  0.0035188  <-> test loss :  0.0018482 |||\n",
      "epoch:  45 ||| train loss :  0.004495  <-> test loss :  0.0081356 |||\n",
      "epoch:  46 ||| train loss :  0.0032393  <-> test loss :  0.0020162 |||\n",
      "epoch:  47 ||| train loss :  0.0036992  <-> test loss :  0.0025628 |||\n",
      "epoch:  48 ||| train loss :  0.003356  <-> test loss :  0.0024602 |||\n",
      "epoch:  49 ||| train loss :  0.0030128  <-> test loss :  0.0016332 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.0023048  <-> test loss :  0.0009011 |||\n",
      "epoch:  51 ||| train loss :  0.0023172  <-> test loss :  0.0012387 |||\n",
      "save model\n",
      "epoch:  52 ||| train loss :  0.0022212  <-> test loss :  0.0004996 |||\n",
      "epoch:  53 ||| train loss :  0.0023102  <-> test loss :  0.0012857 |||\n",
      "epoch:  54 ||| train loss :  0.002243  <-> test loss :  0.001111 |||\n",
      "epoch:  55 ||| train loss :  0.0023908  <-> test loss :  0.0010258 |||\n",
      "epoch:  56 ||| train loss :  0.0022549  <-> test loss :  0.0006801 |||\n",
      "epoch:  57 ||| train loss :  0.001914  <-> test loss :  0.0010401 |||\n",
      "epoch:  58 ||| train loss :  0.001911  <-> test loss :  0.0007183 |||\n",
      "epoch:  59 ||| train loss :  0.0019452  <-> test loss :  0.0009652 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635375452.pth\n",
      "0.90625\n",
      "0.9375\n",
      "0.96875\n",
      "1.0\n",
      "1.0\n",
      "0.96875\n",
      "0.90625\n",
      "0.96875\n",
      "0.84375\n",
      "1.0\n",
      "1.0\n",
      "0.9375\n",
      "0.96875\n",
      "0.90625\n",
      "0.96875\n",
      "0.875\n",
      "0.96875\n",
      "1.0\n",
      "0.90625\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "0.9538043478260869\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  2.2925781  <-> test loss :  2.6912704 |||\n",
      "epoch:  1 ||| train loss :  1.5118368  <-> test loss :  7.1413946 |||\n",
      "epoch:  2 ||| train loss :  1.120793  <-> test loss :  3.8726921 |||\n",
      "save model\n",
      "epoch:  3 ||| train loss :  0.8295446  <-> test loss :  0.7630054 |||\n",
      "epoch:  4 ||| train loss :  0.6336248  <-> test loss :  4.2426233 |||\n",
      "epoch:  5 ||| train loss :  0.5490838  <-> test loss :  0.9345667 |||\n",
      "epoch:  6 ||| train loss :  0.4553769  <-> test loss :  1.1387169 |||\n",
      "save model\n",
      "epoch:  7 ||| train loss :  0.3690224  <-> test loss :  0.3026129 |||\n",
      "epoch:  8 ||| train loss :  0.2962947  <-> test loss :  1.0287875 |||\n",
      "epoch:  9 ||| train loss :  0.2919117  <-> test loss :  2.0832796 |||\n",
      "epoch:  10 ||| train loss :  0.2584055  <-> test loss :  1.2070689 |||\n",
      "epoch:  11 ||| train loss :  0.188011  <-> test loss :  1.0865366 |||\n",
      "epoch:  12 ||| train loss :  0.1491498  <-> test loss :  0.69084 |||\n",
      "epoch:  13 ||| train loss :  0.1314865  <-> test loss :  0.649968 |||\n",
      "save model\n",
      "epoch:  14 ||| train loss :  0.1246506  <-> test loss :  0.2451167 |||\n",
      "epoch:  15 ||| train loss :  0.1110635  <-> test loss :  1.3452909 |||\n",
      "epoch:  16 ||| train loss :  0.1049249  <-> test loss :  1.6334394 |||\n",
      "epoch:  17 ||| train loss :  0.1012245  <-> test loss :  0.3951753 |||\n",
      "save model\n",
      "epoch:  18 ||| train loss :  0.0737369  <-> test loss :  0.0803196 |||\n",
      "epoch:  19 ||| train loss :  0.058759  <-> test loss :  1.0146331 |||\n",
      "epoch:  20 ||| train loss :  0.0542936  <-> test loss :  0.2414171 |||\n",
      "save model\n",
      "epoch:  21 ||| train loss :  0.0409158  <-> test loss :  0.0280077 |||\n",
      "epoch:  22 ||| train loss :  0.0370441  <-> test loss :  0.1331253 |||\n",
      "epoch:  23 ||| train loss :  0.0366687  <-> test loss :  0.1024906 |||\n",
      "save model\n",
      "epoch:  24 ||| train loss :  0.0206677  <-> test loss :  0.012016 |||\n",
      "epoch:  25 ||| train loss :  0.0185805  <-> test loss :  0.0131612 |||\n",
      "save model\n",
      "epoch:  26 ||| train loss :  0.0133236  <-> test loss :  0.0041276 |||\n",
      "epoch:  27 ||| train loss :  0.0109525  <-> test loss :  0.0102589 |||\n",
      "epoch:  28 ||| train loss :  0.0111054  <-> test loss :  0.004369 |||\n",
      "epoch:  29 ||| train loss :  0.0118184  <-> test loss :  0.0094316 |||\n",
      "epoch:  30 ||| train loss :  0.007803  <-> test loss :  0.0071841 |||\n",
      "save model\n",
      "epoch:  31 ||| train loss :  0.0062181  <-> test loss :  0.0035115 |||\n",
      "epoch:  32 ||| train loss :  0.006799  <-> test loss :  0.0359598 |||\n",
      "save model\n",
      "epoch:  33 ||| train loss :  0.0079096  <-> test loss :  0.0024192 |||\n",
      "save model\n",
      "epoch:  34 ||| train loss :  0.0064178  <-> test loss :  0.0017037 |||\n",
      "epoch:  35 ||| train loss :  0.0050086  <-> test loss :  0.002598 |||\n",
      "epoch:  36 ||| train loss :  0.0043791  <-> test loss :  0.0036527 |||\n",
      "epoch:  37 ||| train loss :  0.0043222  <-> test loss :  0.0025208 |||\n",
      "epoch:  38 ||| train loss :  0.0051342  <-> test loss :  0.0026637 |||\n",
      "epoch:  39 ||| train loss :  0.0042557  <-> test loss :  0.0082644 |||\n",
      "epoch:  40 ||| train loss :  0.0043369  <-> test loss :  0.0021334 |||\n",
      "epoch:  41 ||| train loss :  0.0043986  <-> test loss :  0.0024718 |||\n",
      "epoch:  42 ||| train loss :  0.0037309  <-> test loss :  0.0030605 |||\n",
      "epoch:  43 ||| train loss :  0.0038922  <-> test loss :  0.0019143 |||\n",
      "epoch:  44 ||| train loss :  0.0041878  <-> test loss :  0.0021541 |||\n",
      "save model\n",
      "epoch:  45 ||| train loss :  0.0028398  <-> test loss :  0.0012581 |||\n",
      "epoch:  46 ||| train loss :  0.0031185  <-> test loss :  0.0019179 |||\n",
      "epoch:  47 ||| train loss :  0.0064014  <-> test loss :  0.0062257 |||\n",
      "epoch:  48 ||| train loss :  0.0086229  <-> test loss :  0.0048569 |||\n",
      "epoch:  49 ||| train loss :  0.0061108  <-> test loss :  0.0099529 |||\n",
      "epoch:  50 ||| train loss :  0.0037424  <-> test loss :  0.0024804 |||\n",
      "epoch:  51 ||| train loss :  0.0030712  <-> test loss :  0.001667 |||\n",
      "epoch:  52 ||| train loss :  0.0022609  <-> test loss :  0.0019739 |||\n",
      "epoch:  53 ||| train loss :  0.002696  <-> test loss :  0.0012915 |||\n",
      "save model\n",
      "epoch:  54 ||| train loss :  0.0025755  <-> test loss :  0.0009155 |||\n",
      "save model\n",
      "epoch:  55 ||| train loss :  0.0023504  <-> test loss :  0.000907 |||\n",
      "epoch:  56 ||| train loss :  0.0020897  <-> test loss :  0.0010419 |||\n",
      "epoch:  57 ||| train loss :  0.0021806  <-> test loss :  0.0010901 |||\n",
      "save model\n",
      "epoch:  58 ||| train loss :  0.0024676  <-> test loss :  0.0008223 |||\n",
      "save model\n",
      "epoch:  59 ||| train loss :  0.0022091  <-> test loss :  0.0006061 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635423257.pth\n",
      "0.90625\n",
      "1.0\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "0.9375\n",
      "0.90625\n",
      "0.96875\n",
      "1.0\n",
      "0.9375\n",
      "1.0\n",
      "0.9375\n",
      "0.96875\n",
      "0.96875\n",
      "0.90625\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "0.9375\n",
      "1.0\n",
      "0.9660326086956522\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  2.2397622  <-> test loss :  2.9120326 |||\n",
      "epoch:  1 ||| train loss :  1.4283285  <-> test loss :  6.4937115 |||\n",
      "epoch:  2 ||| train loss :  1.0653329  <-> test loss :  4.1674757 |||\n",
      "save model\n",
      "epoch:  3 ||| train loss :  0.7729636  <-> test loss :  2.7694602 |||\n",
      "epoch:  4 ||| train loss :  0.6040684  <-> test loss :  3.1312809 |||\n",
      "epoch:  5 ||| train loss :  0.511947  <-> test loss :  6.9859409 |||\n",
      "save model\n",
      "epoch:  6 ||| train loss :  0.4190834  <-> test loss :  1.3527666 |||\n",
      "save model\n",
      "epoch:  7 ||| train loss :  0.3792854  <-> test loss :  0.9988381 |||\n",
      "epoch:  8 ||| train loss :  0.3144615  <-> test loss :  3.9270716 |||\n",
      "epoch:  9 ||| train loss :  0.2649206  <-> test loss :  3.2665095 |||\n",
      "epoch:  10 ||| train loss :  0.2249355  <-> test loss :  2.1853452 |||\n",
      "save model\n",
      "epoch:  11 ||| train loss :  0.1675355  <-> test loss :  0.272563 |||\n",
      "epoch:  12 ||| train loss :  0.1570568  <-> test loss :  10.5746441 |||\n",
      "epoch:  13 ||| train loss :  0.1733232  <-> test loss :  0.306728 |||\n",
      "epoch:  14 ||| train loss :  0.1108637  <-> test loss :  0.3295842 |||\n",
      "save model\n",
      "epoch:  15 ||| train loss :  0.0690596  <-> test loss :  0.2643408 |||\n",
      "save model\n",
      "epoch:  16 ||| train loss :  0.0608757  <-> test loss :  0.1928498 |||\n",
      "save model\n",
      "epoch:  17 ||| train loss :  0.0547974  <-> test loss :  0.1737304 |||\n",
      "epoch:  18 ||| train loss :  0.0599129  <-> test loss :  0.6140695 |||\n",
      "epoch:  19 ||| train loss :  0.068347  <-> test loss :  0.2061785 |||\n",
      "save model\n",
      "epoch:  20 ||| train loss :  0.0871367  <-> test loss :  0.0962335 |||\n",
      "epoch:  21 ||| train loss :  0.0658083  <-> test loss :  2.018616 |||\n",
      "save model\n",
      "epoch:  22 ||| train loss :  0.0408005  <-> test loss :  0.0226471 |||\n",
      "epoch:  23 ||| train loss :  0.0302046  <-> test loss :  1.0755903 |||\n",
      "epoch:  24 ||| train loss :  0.0244264  <-> test loss :  0.0910395 |||\n",
      "epoch:  25 ||| train loss :  0.020195  <-> test loss :  0.1308918 |||\n",
      "epoch:  26 ||| train loss :  0.0246713  <-> test loss :  0.0483699 |||\n",
      "save model\n",
      "epoch:  27 ||| train loss :  0.0140105  <-> test loss :  0.0139027 |||\n",
      "epoch:  28 ||| train loss :  0.0098619  <-> test loss :  0.1004949 |||\n",
      "save model\n",
      "epoch:  29 ||| train loss :  0.0084537  <-> test loss :  0.006816 |||\n",
      "save model\n",
      "epoch:  30 ||| train loss :  0.0071872  <-> test loss :  0.0047924 |||\n",
      "epoch:  31 ||| train loss :  0.012022  <-> test loss :  0.4888227 |||\n",
      "save model\n",
      "epoch:  32 ||| train loss :  0.0079282  <-> test loss :  0.00266 |||\n",
      "epoch:  33 ||| train loss :  0.0067402  <-> test loss :  0.0042196 |||\n",
      "epoch:  34 ||| train loss :  0.0082406  <-> test loss :  0.0085438 |||\n",
      "epoch:  35 ||| train loss :  0.0063523  <-> test loss :  0.0078158 |||\n",
      "epoch:  36 ||| train loss :  0.0048467  <-> test loss :  0.0027076 |||\n",
      "save model\n",
      "epoch:  37 ||| train loss :  0.0046791  <-> test loss :  0.002242 |||\n",
      "epoch:  38 ||| train loss :  0.0039932  <-> test loss :  0.0039482 |||\n",
      "epoch:  39 ||| train loss :  0.0044401  <-> test loss :  0.0030595 |||\n",
      "epoch:  40 ||| train loss :  0.0034568  <-> test loss :  0.0053252 |||\n",
      "save model\n",
      "epoch:  41 ||| train loss :  0.0039351  <-> test loss :  0.0012666 |||\n",
      "epoch:  42 ||| train loss :  0.0033748  <-> test loss :  0.0023186 |||\n",
      "epoch:  43 ||| train loss :  0.0055448  <-> test loss :  0.8123379 |||\n",
      "epoch:  44 ||| train loss :  0.0153782  <-> test loss :  0.0271021 |||\n",
      "epoch:  45 ||| train loss :  0.0099109  <-> test loss :  0.131443 |||\n",
      "epoch:  46 ||| train loss :  0.009438  <-> test loss :  0.0043944 |||\n",
      "epoch:  47 ||| train loss :  0.0044469  <-> test loss :  0.0031098 |||\n",
      "save model\n",
      "epoch:  48 ||| train loss :  0.0035936  <-> test loss :  0.000838 |||\n",
      "epoch:  49 ||| train loss :  0.0102939  <-> test loss :  0.4811201 |||\n",
      "epoch:  50 ||| train loss :  0.0057277  <-> test loss :  0.0014854 |||\n",
      "epoch:  51 ||| train loss :  0.0039589  <-> test loss :  0.0018136 |||\n",
      "epoch:  52 ||| train loss :  0.0029257  <-> test loss :  0.0010297 |||\n",
      "epoch:  53 ||| train loss :  0.0029156  <-> test loss :  0.0013572 |||\n",
      "epoch:  54 ||| train loss :  0.003242  <-> test loss :  0.0015237 |||\n",
      "epoch:  55 ||| train loss :  0.0030198  <-> test loss :  0.0014595 |||\n",
      "epoch:  56 ||| train loss :  0.0021563  <-> test loss :  0.0011234 |||\n",
      "epoch:  57 ||| train loss :  0.0021059  <-> test loss :  0.0009768 |||\n",
      "epoch:  58 ||| train loss :  0.0020922  <-> test loss :  0.0010414 |||\n",
      "epoch:  59 ||| train loss :  0.002515  <-> test loss :  0.0011317 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635471039.pth\n",
      "0.90625\n",
      "1.0\n",
      "1.0\n",
      "0.96875\n",
      "0.90625\n",
      "0.96875\n",
      "0.9375\n",
      "0.96875\n",
      "0.875\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.90625\n",
      "0.96875\n",
      "0.875\n",
      "0.875\n",
      "0.90625\n",
      "0.9375\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "0.9442934782608695\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  2.3053092  <-> test loss :  2.9987569 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  1.5125954  <-> test loss :  1.3154089 |||\n",
      "epoch:  2 ||| train loss :  1.1222697  <-> test loss :  4.8761044 |||\n",
      "epoch:  3 ||| train loss :  0.9383699  <-> test loss :  2.5375104 |||\n",
      "epoch:  4 ||| train loss :  0.7057248  <-> test loss :  1.859292 |||\n",
      "epoch:  5 ||| train loss :  0.5679386  <-> test loss :  1.990211 |||\n",
      "epoch:  6 ||| train loss :  0.4821178  <-> test loss :  2.8603706 |||\n",
      "save model\n",
      "epoch:  7 ||| train loss :  0.4152455  <-> test loss :  0.6131093 |||\n",
      "epoch:  8 ||| train loss :  0.3242011  <-> test loss :  2.0159898 |||\n",
      "epoch:  9 ||| train loss :  0.286304  <-> test loss :  1.6501217 |||\n",
      "save model\n",
      "epoch:  10 ||| train loss :  0.2306291  <-> test loss :  0.4052534 |||\n",
      "epoch:  11 ||| train loss :  0.1985694  <-> test loss :  1.4023745 |||\n",
      "save model\n",
      "epoch:  12 ||| train loss :  0.2318503  <-> test loss :  0.1880811 |||\n",
      "save model\n",
      "epoch:  13 ||| train loss :  0.1422047  <-> test loss :  0.167968 |||\n",
      "epoch:  14 ||| train loss :  0.1160444  <-> test loss :  0.4217221 |||\n",
      "epoch:  15 ||| train loss :  0.0829024  <-> test loss :  3.7651491 |||\n",
      "save model\n",
      "epoch:  16 ||| train loss :  0.0927947  <-> test loss :  0.1390786 |||\n",
      "epoch:  17 ||| train loss :  0.0610207  <-> test loss :  0.2686228 |||\n",
      "save model\n",
      "epoch:  18 ||| train loss :  0.0529909  <-> test loss :  0.1134701 |||\n",
      "epoch:  19 ||| train loss :  0.0489073  <-> test loss :  0.6266351 |||\n",
      "save model\n",
      "epoch:  20 ||| train loss :  0.0424079  <-> test loss :  0.1040923 |||\n",
      "epoch:  21 ||| train loss :  0.0412197  <-> test loss :  0.4085699 |||\n",
      "epoch:  22 ||| train loss :  0.0426186  <-> test loss :  0.6508955 |||\n",
      "epoch:  23 ||| train loss :  0.0391849  <-> test loss :  1.4264443 |||\n",
      "save model\n",
      "epoch:  24 ||| train loss :  0.0275441  <-> test loss :  0.0492929 |||\n",
      "save model\n",
      "epoch:  25 ||| train loss :  0.0193285  <-> test loss :  0.0110714 |||\n",
      "epoch:  26 ||| train loss :  0.0152655  <-> test loss :  0.0165686 |||\n",
      "epoch:  27 ||| train loss :  0.0123363  <-> test loss :  0.0114788 |||\n",
      "save model\n",
      "epoch:  28 ||| train loss :  0.0115825  <-> test loss :  0.0080985 |||\n",
      "epoch:  29 ||| train loss :  0.0159311  <-> test loss :  0.0084296 |||\n",
      "epoch:  30 ||| train loss :  0.0138904  <-> test loss :  0.0137166 |||\n",
      "save model\n",
      "epoch:  31 ||| train loss :  0.0112447  <-> test loss :  0.0028467 |||\n",
      "epoch:  32 ||| train loss :  0.0073267  <-> test loss :  0.0036818 |||\n",
      "epoch:  33 ||| train loss :  0.0084801  <-> test loss :  0.0048239 |||\n",
      "save model\n",
      "epoch:  34 ||| train loss :  0.0067562  <-> test loss :  0.0015771 |||\n",
      "epoch:  35 ||| train loss :  0.0054611  <-> test loss :  0.003171 |||\n",
      "epoch:  36 ||| train loss :  0.0047416  <-> test loss :  0.0050478 |||\n",
      "epoch:  37 ||| train loss :  0.0060195  <-> test loss :  0.0025173 |||\n",
      "epoch:  38 ||| train loss :  0.0049732  <-> test loss :  0.0038584 |||\n",
      "epoch:  39 ||| train loss :  0.0119889  <-> test loss :  0.4831073 |||\n",
      "epoch:  40 ||| train loss :  0.0138405  <-> test loss :  0.0620311 |||\n",
      "epoch:  41 ||| train loss :  0.0175381  <-> test loss :  0.1317833 |||\n",
      "epoch:  42 ||| train loss :  0.0768997  <-> test loss :  0.5342706 |||\n",
      "epoch:  43 ||| train loss :  0.0312535  <-> test loss :  0.1536292 |||\n",
      "epoch:  44 ||| train loss :  0.0170098  <-> test loss :  0.0030865 |||\n",
      "epoch:  45 ||| train loss :  0.0078783  <-> test loss :  0.0027616 |||\n",
      "epoch:  46 ||| train loss :  0.0063231  <-> test loss :  0.0035562 |||\n",
      "epoch:  47 ||| train loss :  0.0050145  <-> test loss :  0.0044053 |||\n",
      "epoch:  48 ||| train loss :  0.0060276  <-> test loss :  0.0109862 |||\n",
      "epoch:  49 ||| train loss :  0.0043302  <-> test loss :  0.002481 |||\n",
      "epoch:  50 ||| train loss :  0.0045703  <-> test loss :  0.0021222 |||\n",
      "save model\n",
      "epoch:  51 ||| train loss :  0.0034695  <-> test loss :  0.0014781 |||\n",
      "epoch:  52 ||| train loss :  0.0027551  <-> test loss :  0.0014916 |||\n",
      "epoch:  53 ||| train loss :  0.0024426  <-> test loss :  0.0016417 |||\n",
      "save model\n",
      "epoch:  54 ||| train loss :  0.0024402  <-> test loss :  0.0011866 |||\n",
      "epoch:  55 ||| train loss :  0.0027518  <-> test loss :  0.0012943 |||\n",
      "save model\n",
      "epoch:  56 ||| train loss :  0.0025663  <-> test loss :  0.0008831 |||\n",
      "save model\n",
      "epoch:  57 ||| train loss :  0.0041686  <-> test loss :  0.0008583 |||\n",
      "epoch:  58 ||| train loss :  0.0026096  <-> test loss :  0.0015335 |||\n",
      "save model\n",
      "epoch:  59 ||| train loss :  0.0023136  <-> test loss :  0.0007187 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635518885.pth\n",
      "0.90625\n",
      "0.9375\n",
      "0.875\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9375\n",
      "1.0\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "0.9633152173913043\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  2.2766348  <-> test loss :  13.7589045 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  1.5103674  <-> test loss :  1.3538964 |||\n",
      "epoch:  2 ||| train loss :  1.1622054  <-> test loss :  5.0062089 |||\n",
      "epoch:  3 ||| train loss :  0.9012303  <-> test loss :  1.3805532 |||\n",
      "epoch:  4 ||| train loss :  0.7337469  <-> test loss :  3.3348417 |||\n",
      "epoch:  5 ||| train loss :  0.6031122  <-> test loss :  2.4450557 |||\n",
      "epoch:  6 ||| train loss :  0.5665852  <-> test loss :  2.1642301 |||\n",
      "save model\n",
      "epoch:  7 ||| train loss :  0.435821  <-> test loss :  1.0719743 |||\n",
      "epoch:  8 ||| train loss :  0.3198204  <-> test loss :  1.0993974 |||\n",
      "epoch:  9 ||| train loss :  0.290216  <-> test loss :  1.1237491 |||\n",
      "save model\n",
      "epoch:  10 ||| train loss :  0.2256704  <-> test loss :  0.5655228 |||\n",
      "save model\n",
      "epoch:  11 ||| train loss :  0.2076772  <-> test loss :  0.407611 |||\n",
      "epoch:  12 ||| train loss :  0.2015134  <-> test loss :  0.6746278 |||\n",
      "save model\n",
      "epoch:  13 ||| train loss :  0.1930591  <-> test loss :  0.0908571 |||\n",
      "epoch:  14 ||| train loss :  0.1324559  <-> test loss :  0.9083313 |||\n",
      "save model\n",
      "epoch:  15 ||| train loss :  0.1071118  <-> test loss :  0.0831306 |||\n",
      "epoch:  16 ||| train loss :  0.0819561  <-> test loss :  0.5837414 |||\n",
      "epoch:  17 ||| train loss :  0.0670599  <-> test loss :  0.4790862 |||\n",
      "epoch:  18 ||| train loss :  0.0506172  <-> test loss :  0.6220435 |||\n",
      "epoch:  19 ||| train loss :  0.048476  <-> test loss :  0.4909298 |||\n",
      "epoch:  20 ||| train loss :  0.0910356  <-> test loss :  0.1416213 |||\n",
      "epoch:  21 ||| train loss :  0.0883198  <-> test loss :  0.9163988 |||\n",
      "epoch:  22 ||| train loss :  0.0755896  <-> test loss :  0.782221 |||\n",
      "epoch:  23 ||| train loss :  0.0363587  <-> test loss :  0.2997474 |||\n",
      "save model\n",
      "epoch:  24 ||| train loss :  0.0238845  <-> test loss :  0.0093666 |||\n",
      "epoch:  25 ||| train loss :  0.0190889  <-> test loss :  8.9848623 |||\n",
      "epoch:  26 ||| train loss :  0.0489742  <-> test loss :  0.9225112 |||\n",
      "epoch:  27 ||| train loss :  0.0365193  <-> test loss :  0.1179486 |||\n",
      "epoch:  28 ||| train loss :  0.015888  <-> test loss :  0.0220052 |||\n",
      "save model\n",
      "epoch:  29 ||| train loss :  0.0133918  <-> test loss :  0.0051696 |||\n",
      "save model\n",
      "epoch:  30 ||| train loss :  0.0093073  <-> test loss :  0.0039351 |||\n",
      "epoch:  31 ||| train loss :  0.0071573  <-> test loss :  0.0056322 |||\n",
      "save model\n",
      "epoch:  32 ||| train loss :  0.0077066  <-> test loss :  0.0039061 |||\n",
      "epoch:  33 ||| train loss :  0.0059802  <-> test loss :  0.0048161 |||\n",
      "epoch:  34 ||| train loss :  0.0060315  <-> test loss :  0.0040659 |||\n",
      "save model\n",
      "epoch:  35 ||| train loss :  0.0053436  <-> test loss :  0.003282 |||\n",
      "epoch:  36 ||| train loss :  0.0052671  <-> test loss :  0.0034678 |||\n",
      "save model\n",
      "epoch:  37 ||| train loss :  0.0043171  <-> test loss :  0.0015988 |||\n",
      "epoch:  38 ||| train loss :  0.0050334  <-> test loss :  0.0027884 |||\n",
      "epoch:  39 ||| train loss :  0.0044056  <-> test loss :  0.0019965 |||\n",
      "epoch:  40 ||| train loss :  0.0042447  <-> test loss :  0.0021011 |||\n",
      "epoch:  41 ||| train loss :  0.0035422  <-> test loss :  0.001832 |||\n",
      "epoch:  42 ||| train loss :  0.004081  <-> test loss :  0.0017608 |||\n",
      "epoch:  43 ||| train loss :  0.0043866  <-> test loss :  0.0031511 |||\n",
      "epoch:  44 ||| train loss :  0.0039831  <-> test loss :  0.0025948 |||\n",
      "epoch:  45 ||| train loss :  0.0064713  <-> test loss :  0.0111701 |||\n",
      "epoch:  46 ||| train loss :  0.0042049  <-> test loss :  0.0016842 |||\n",
      "epoch:  47 ||| train loss :  0.0038837  <-> test loss :  0.0044397 |||\n",
      "epoch:  48 ||| train loss :  0.0073842  <-> test loss :  0.0267366 |||\n",
      "epoch:  49 ||| train loss :  0.0050651  <-> test loss :  0.0033921 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.0031141  <-> test loss :  0.0013925 |||\n",
      "save model\n",
      "epoch:  51 ||| train loss :  0.002381  <-> test loss :  0.0011583 |||\n",
      "epoch:  52 ||| train loss :  0.003133  <-> test loss :  0.0022917 |||\n",
      "save model\n",
      "epoch:  53 ||| train loss :  0.0026078  <-> test loss :  0.0009931 |||\n",
      "epoch:  54 ||| train loss :  0.0023743  <-> test loss :  0.0017081 |||\n",
      "save model\n",
      "epoch:  55 ||| train loss :  0.002768  <-> test loss :  0.0007725 |||\n",
      "epoch:  56 ||| train loss :  0.0022342  <-> test loss :  0.0009561 |||\n",
      "epoch:  57 ||| train loss :  0.0049791  <-> test loss :  0.0012277 |||\n",
      "epoch:  58 ||| train loss :  0.0024445  <-> test loss :  0.0008075 |||\n",
      "epoch:  59 ||| train loss :  0.0025054  <-> test loss :  0.0013325 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635566693.pth\n",
      "0.9375\n",
      "0.9375\n",
      "1.0\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "0.90625\n",
      "0.9375\n",
      "0.875\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "0.90625\n",
      "0.96875\n",
      "1.0\n",
      "0.9375\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "1.0\n",
      "0.9592391304347826\n",
      "[0.9538043478260869, 0.9660326086956522, 0.9442934782608695, 0.9633152173913043, 0.9592391304347826]\n",
      "0.9573369565217391\n",
      "0.9573\n",
      "----------loss weight==1----------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  2.1005366  <-> test loss :  5.9932847 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  1.3622379  <-> test loss :  1.4012867 |||\n",
      "save model\n",
      "epoch:  2 ||| train loss :  0.9934309  <-> test loss :  1.2930882 |||\n",
      "epoch:  3 ||| train loss :  0.7629813  <-> test loss :  1.494869 |||\n",
      "save model\n",
      "epoch:  4 ||| train loss :  0.6053139  <-> test loss :  0.6893949 |||\n",
      "epoch:  5 ||| train loss :  0.5017814  <-> test loss :  1.9178698 |||\n",
      "save model\n",
      "epoch:  6 ||| train loss :  0.4274445  <-> test loss :  0.5846279 |||\n",
      "epoch:  7 ||| train loss :  0.3386993  <-> test loss :  2.9310153 |||\n",
      "save model\n",
      "epoch:  8 ||| train loss :  0.2979907  <-> test loss :  0.4667453 |||\n",
      "epoch:  9 ||| train loss :  0.2519149  <-> test loss :  1.5496917 |||\n",
      "epoch:  10 ||| train loss :  0.2001702  <-> test loss :  1.8684962 |||\n",
      "epoch:  11 ||| train loss :  0.1809288  <-> test loss :  2.329704 |||\n",
      "epoch:  12 ||| train loss :  0.1546587  <-> test loss :  3.880775 |||\n",
      "epoch:  13 ||| train loss :  0.1193228  <-> test loss :  0.5846578 |||\n",
      "epoch:  14 ||| train loss :  0.0855166  <-> test loss :  0.7011483 |||\n",
      "save model\n",
      "epoch:  15 ||| train loss :  0.0731153  <-> test loss :  0.3732537 |||\n",
      "epoch:  16 ||| train loss :  0.0759657  <-> test loss :  0.3967199 |||\n",
      "epoch:  17 ||| train loss :  0.0670436  <-> test loss :  0.5260383 |||\n",
      "save model\n",
      "epoch:  18 ||| train loss :  0.0565572  <-> test loss :  0.0933998 |||\n",
      "epoch:  19 ||| train loss :  0.0475664  <-> test loss :  0.5961519 |||\n",
      "epoch:  20 ||| train loss :  0.0509454  <-> test loss :  0.1285624 |||\n",
      "save model\n",
      "epoch:  21 ||| train loss :  0.0745378  <-> test loss :  0.0636154 |||\n",
      "epoch:  22 ||| train loss :  0.0545915  <-> test loss :  0.1880215 |||\n",
      "save model\n",
      "epoch:  23 ||| train loss :  0.0596433  <-> test loss :  0.0546228 |||\n",
      "save model\n",
      "epoch:  24 ||| train loss :  0.0283217  <-> test loss :  0.0378515 |||\n",
      "save model\n",
      "epoch:  25 ||| train loss :  0.0187308  <-> test loss :  0.0092043 |||\n",
      "epoch:  26 ||| train loss :  0.0139654  <-> test loss :  0.0727738 |||\n",
      "save model\n",
      "epoch:  27 ||| train loss :  0.0124841  <-> test loss :  0.0038982 |||\n",
      "epoch:  28 ||| train loss :  0.0087883  <-> test loss :  0.029639 |||\n",
      "epoch:  29 ||| train loss :  0.0076121  <-> test loss :  0.0060855 |||\n",
      "epoch:  30 ||| train loss :  0.0064048  <-> test loss :  0.0074708 |||\n",
      "save model\n",
      "epoch:  31 ||| train loss :  0.0061861  <-> test loss :  0.0031342 |||\n",
      "save model\n",
      "epoch:  32 ||| train loss :  0.0071481  <-> test loss :  0.002958 |||\n",
      "epoch:  33 ||| train loss :  0.00701  <-> test loss :  0.041068 |||\n",
      "epoch:  34 ||| train loss :  0.0062876  <-> test loss :  0.0030918 |||\n",
      "epoch:  35 ||| train loss :  0.0051186  <-> test loss :  0.0035089 |||\n",
      "epoch:  36 ||| train loss :  0.0044452  <-> test loss :  0.0461927 |||\n",
      "epoch:  37 ||| train loss :  0.0236677  <-> test loss :  0.2098562 |||\n",
      "epoch:  38 ||| train loss :  0.058652  <-> test loss :  0.0655145 |||\n",
      "epoch:  39 ||| train loss :  0.0529215  <-> test loss :  0.3099861 |||\n",
      "epoch:  40 ||| train loss :  0.0634625  <-> test loss :  0.2857698 |||\n",
      "epoch:  41 ||| train loss :  0.0168139  <-> test loss :  0.0179385 |||\n",
      "save model\n",
      "epoch:  42 ||| train loss :  0.0103981  <-> test loss :  0.002706 |||\n",
      "epoch:  43 ||| train loss :  0.0072308  <-> test loss :  0.0091744 |||\n",
      "save model\n",
      "epoch:  44 ||| train loss :  0.0054618  <-> test loss :  0.0020868 |||\n",
      "save model\n",
      "epoch:  45 ||| train loss :  0.0035457  <-> test loss :  0.0020217 |||\n",
      "save model\n",
      "epoch:  46 ||| train loss :  0.004283  <-> test loss :  0.0008129 |||\n",
      "epoch:  47 ||| train loss :  0.0033875  <-> test loss :  0.0033311 |||\n",
      "epoch:  48 ||| train loss :  0.0032254  <-> test loss :  0.0024281 |||\n",
      "epoch:  49 ||| train loss :  0.0031508  <-> test loss :  0.0011358 |||\n",
      "epoch:  50 ||| train loss :  0.0030148  <-> test loss :  0.0011856 |||\n",
      "save model\n",
      "epoch:  51 ||| train loss :  0.0026141  <-> test loss :  0.0007796 |||\n",
      "epoch:  52 ||| train loss :  0.0047203  <-> test loss :  0.006727 |||\n",
      "epoch:  53 ||| train loss :  0.0027975  <-> test loss :  0.0016166 |||\n",
      "epoch:  54 ||| train loss :  0.0027926  <-> test loss :  0.0011362 |||\n",
      "epoch:  55 ||| train loss :  0.0026239  <-> test loss :  0.0015902 |||\n",
      "epoch:  56 ||| train loss :  0.0034469  <-> test loss :  0.0019794 |||\n",
      "epoch:  57 ||| train loss :  0.0023377  <-> test loss :  0.0008378 |||\n",
      "save model\n",
      "epoch:  58 ||| train loss :  0.002312  <-> test loss :  0.0006409 |||\n",
      "epoch:  59 ||| train loss :  0.0022247  <-> test loss :  0.0007028 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635614503.pth\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "1.0\n",
      "0.875\n",
      "0.96875\n",
      "0.90625\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "0.90625\n",
      "1.0\n",
      "0.90625\n",
      "0.9375\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9578804347826086\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  2.0794478  <-> test loss :  2.3430395 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  1.3625448  <-> test loss :  2.1101017 |||\n",
      "epoch:  2 ||| train loss :  1.0054368  <-> test loss :  2.3988538 |||\n",
      "epoch:  3 ||| train loss :  0.7891929  <-> test loss :  2.3786142 |||\n",
      "epoch:  4 ||| train loss :  0.6342997  <-> test loss :  2.8633866 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  0.5462834  <-> test loss :  1.3494467 |||\n",
      "save model\n",
      "epoch:  6 ||| train loss :  0.445066  <-> test loss :  1.1536949 |||\n",
      "epoch:  7 ||| train loss :  0.3800022  <-> test loss :  1.5685484 |||\n",
      "epoch:  8 ||| train loss :  0.3265535  <-> test loss :  7.8494353 |||\n",
      "epoch:  9 ||| train loss :  0.2915177  <-> test loss :  2.9695692 |||\n",
      "save model\n",
      "epoch:  10 ||| train loss :  0.2259025  <-> test loss :  0.9282303 |||\n",
      "epoch:  11 ||| train loss :  0.2113752  <-> test loss :  1.6742842 |||\n",
      "save model\n",
      "epoch:  12 ||| train loss :  0.1670097  <-> test loss :  0.4265024 |||\n",
      "epoch:  13 ||| train loss :  0.1531495  <-> test loss :  0.4297494 |||\n",
      "epoch:  14 ||| train loss :  0.1330235  <-> test loss :  1.1714864 |||\n",
      "save model\n",
      "epoch:  15 ||| train loss :  0.1072054  <-> test loss :  0.3287084 |||\n",
      "epoch:  16 ||| train loss :  0.1007984  <-> test loss :  0.3967571 |||\n",
      "save model\n",
      "epoch:  17 ||| train loss :  0.0710809  <-> test loss :  0.0664203 |||\n",
      "epoch:  18 ||| train loss :  0.069184  <-> test loss :  0.7312736 |||\n",
      "epoch:  19 ||| train loss :  0.0461663  <-> test loss :  0.1850189 |||\n",
      "epoch:  20 ||| train loss :  0.0372562  <-> test loss :  0.2445429 |||\n",
      "epoch:  21 ||| train loss :  0.0348811  <-> test loss :  1.1042862 |||\n",
      "epoch:  22 ||| train loss :  0.0392461  <-> test loss :  0.8744102 |||\n",
      "save model\n",
      "epoch:  23 ||| train loss :  0.0328338  <-> test loss :  0.0328618 |||\n",
      "epoch:  24 ||| train loss :  0.0406288  <-> test loss :  0.2688545 |||\n",
      "epoch:  25 ||| train loss :  0.0282447  <-> test loss :  3.5129774 |||\n",
      "save model\n",
      "epoch:  26 ||| train loss :  0.0279402  <-> test loss :  0.0089893 |||\n",
      "epoch:  27 ||| train loss :  0.0136862  <-> test loss :  0.0684761 |||\n",
      "epoch:  28 ||| train loss :  0.011418  <-> test loss :  0.0105481 |||\n",
      "save model\n",
      "epoch:  29 ||| train loss :  0.0116535  <-> test loss :  0.0045175 |||\n",
      "epoch:  30 ||| train loss :  0.0104418  <-> test loss :  0.306711 |||\n",
      "epoch:  31 ||| train loss :  0.0113422  <-> test loss :  0.0045727 |||\n",
      "epoch:  32 ||| train loss :  0.0075018  <-> test loss :  0.0224862 |||\n",
      "epoch:  33 ||| train loss :  0.0058925  <-> test loss :  0.0047536 |||\n",
      "save model\n",
      "epoch:  34 ||| train loss :  0.0063872  <-> test loss :  0.002431 |||\n",
      "epoch:  35 ||| train loss :  0.0086623  <-> test loss :  0.0291644 |||\n",
      "epoch:  36 ||| train loss :  0.0073416  <-> test loss :  0.0077369 |||\n",
      "save model\n",
      "epoch:  37 ||| train loss :  0.0053786  <-> test loss :  0.0021136 |||\n",
      "epoch:  38 ||| train loss :  0.0056823  <-> test loss :  0.002801 |||\n",
      "epoch:  39 ||| train loss :  0.0042166  <-> test loss :  0.002951 |||\n",
      "epoch:  40 ||| train loss :  0.0108604  <-> test loss :  0.1350736 |||\n",
      "epoch:  41 ||| train loss :  0.0103816  <-> test loss :  0.0038019 |||\n",
      "epoch:  42 ||| train loss :  0.0041341  <-> test loss :  0.0022737 |||\n",
      "epoch:  43 ||| train loss :  0.0057116  <-> test loss :  0.0061929 |||\n",
      "epoch:  44 ||| train loss :  0.0048604  <-> test loss :  0.0040106 |||\n",
      "epoch:  45 ||| train loss :  0.0040495  <-> test loss :  0.0166496 |||\n",
      "epoch:  46 ||| train loss :  0.0041202  <-> test loss :  0.0090788 |||\n",
      "save model\n",
      "epoch:  47 ||| train loss :  0.0031922  <-> test loss :  0.0011881 |||\n",
      "epoch:  48 ||| train loss :  0.0027164  <-> test loss :  0.0023149 |||\n",
      "save model\n",
      "epoch:  49 ||| train loss :  0.0023916  <-> test loss :  0.0009531 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.0024945  <-> test loss :  0.0007511 |||\n",
      "epoch:  51 ||| train loss :  0.0020274  <-> test loss :  0.0010939 |||\n",
      "epoch:  52 ||| train loss :  0.0019962  <-> test loss :  0.0010641 |||\n",
      "epoch:  53 ||| train loss :  0.0021726  <-> test loss :  0.0008559 |||\n",
      "epoch:  54 ||| train loss :  0.0020661  <-> test loss :  0.0012644 |||\n",
      "epoch:  55 ||| train loss :  0.0031041  <-> test loss :  0.0014108 |||\n",
      "save model\n",
      "epoch:  56 ||| train loss :  0.0021006  <-> test loss :  0.0006391 |||\n",
      "epoch:  57 ||| train loss :  0.002032  <-> test loss :  0.0009568 |||\n",
      "epoch:  58 ||| train loss :  0.0017457  <-> test loss :  0.0008068 |||\n",
      "epoch:  59 ||| train loss :  0.0020012  <-> test loss :  0.0011203 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635662351.pth\n",
      "0.84375\n",
      "0.96875\n",
      "1.0\n",
      "1.0\n",
      "0.96875\n",
      "1.0\n",
      "0.90625\n",
      "0.96875\n",
      "0.875\n",
      "0.96875\n",
      "1.0\n",
      "0.90625\n",
      "1.0\n",
      "0.9375\n",
      "0.90625\n",
      "0.9375\n",
      "0.90625\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "0.90625\n",
      "1.0\n",
      "0.9483695652173914\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  2.0641705  <-> test loss :  1.9900649 |||\n",
      "epoch:  1 ||| train loss :  1.389552  <-> test loss :  2.8869729 |||\n",
      "epoch:  2 ||| train loss :  1.0194438  <-> test loss :  3.1003504 |||\n",
      "save model\n",
      "epoch:  3 ||| train loss :  0.756532  <-> test loss :  1.2357941 |||\n",
      "epoch:  4 ||| train loss :  0.6415226  <-> test loss :  13.9932661 |||\n",
      "epoch:  5 ||| train loss :  0.5532277  <-> test loss :  6.8894682 |||\n",
      "save model\n",
      "epoch:  6 ||| train loss :  0.4130299  <-> test loss :  0.1924019 |||\n",
      "epoch:  7 ||| train loss :  0.3795745  <-> test loss :  1.3726163 |||\n",
      "epoch:  8 ||| train loss :  0.3047697  <-> test loss :  1.0903047 |||\n",
      "epoch:  9 ||| train loss :  0.2455774  <-> test loss :  1.125875 |||\n",
      "epoch:  10 ||| train loss :  0.2341758  <-> test loss :  1.0815983 |||\n",
      "epoch:  11 ||| train loss :  0.1990722  <-> test loss :  2.6731701 |||\n",
      "epoch:  12 ||| train loss :  0.1523803  <-> test loss :  2.1490965 |||\n",
      "epoch:  13 ||| train loss :  0.1397914  <-> test loss :  4.9241457 |||\n",
      "save model\n",
      "epoch:  14 ||| train loss :  0.1111312  <-> test loss :  0.1061932 |||\n",
      "epoch:  15 ||| train loss :  0.0763356  <-> test loss :  2.8559618 |||\n",
      "epoch:  16 ||| train loss :  0.0831489  <-> test loss :  0.1351213 |||\n",
      "epoch:  17 ||| train loss :  0.0722685  <-> test loss :  1.4279292 |||\n",
      "epoch:  18 ||| train loss :  0.0502404  <-> test loss :  0.2744182 |||\n",
      "epoch:  19 ||| train loss :  0.074674  <-> test loss :  0.2273176 |||\n",
      "epoch:  20 ||| train loss :  0.0510781  <-> test loss :  0.1147857 |||\n",
      "epoch:  21 ||| train loss :  0.0438356  <-> test loss :  0.1524227 |||\n",
      "epoch:  22 ||| train loss :  0.0403836  <-> test loss :  0.1175697 |||\n",
      "epoch:  23 ||| train loss :  0.0529763  <-> test loss :  3.2139394 |||\n",
      "save model\n",
      "epoch:  24 ||| train loss :  0.0460847  <-> test loss :  0.0656092 |||\n",
      "save model\n",
      "epoch:  25 ||| train loss :  0.024079  <-> test loss :  0.0352774 |||\n",
      "save model\n",
      "epoch:  26 ||| train loss :  0.0135592  <-> test loss :  0.0102613 |||\n",
      "epoch:  27 ||| train loss :  0.013348  <-> test loss :  0.0429927 |||\n",
      "save model\n",
      "epoch:  28 ||| train loss :  0.010231  <-> test loss :  0.0096651 |||\n",
      "save model\n",
      "epoch:  29 ||| train loss :  0.0075044  <-> test loss :  0.0053363 |||\n",
      "epoch:  30 ||| train loss :  0.0074227  <-> test loss :  0.0062831 |||\n",
      "save model\n",
      "epoch:  31 ||| train loss :  0.0060927  <-> test loss :  0.0031777 |||\n",
      "epoch:  32 ||| train loss :  0.0091833  <-> test loss :  0.0106594 |||\n",
      "save model\n",
      "epoch:  33 ||| train loss :  0.0063531  <-> test loss :  0.0017193 |||\n",
      "epoch:  34 ||| train loss :  0.0058124  <-> test loss :  0.0028443 |||\n",
      "epoch:  35 ||| train loss :  0.0069832  <-> test loss :  0.0081778 |||\n",
      "epoch:  36 ||| train loss :  0.0044489  <-> test loss :  0.0045384 |||\n",
      "save model\n",
      "epoch:  37 ||| train loss :  0.0050707  <-> test loss :  0.0014367 |||\n",
      "epoch:  38 ||| train loss :  0.0036836  <-> test loss :  0.0036465 |||\n",
      "epoch:  39 ||| train loss :  0.0042623  <-> test loss :  0.0132717 |||\n",
      "epoch:  40 ||| train loss :  0.0040073  <-> test loss :  0.0030664 |||\n",
      "epoch:  41 ||| train loss :  0.0031597  <-> test loss :  0.0033768 |||\n",
      "epoch:  42 ||| train loss :  0.0031324  <-> test loss :  0.002317 |||\n",
      "save model\n",
      "epoch:  43 ||| train loss :  0.0028776  <-> test loss :  0.0008543 |||\n",
      "epoch:  44 ||| train loss :  0.0027589  <-> test loss :  0.0012004 |||\n",
      "epoch:  45 ||| train loss :  0.0026483  <-> test loss :  0.0009125 |||\n",
      "epoch:  46 ||| train loss :  0.0026661  <-> test loss :  0.0010252 |||\n",
      "epoch:  47 ||| train loss :  0.0026576  <-> test loss :  0.0010871 |||\n",
      "save model\n",
      "epoch:  48 ||| train loss :  0.0025314  <-> test loss :  0.0007446 |||\n",
      "epoch:  49 ||| train loss :  0.003  <-> test loss :  0.0033979 |||\n",
      "epoch:  50 ||| train loss :  0.0024672  <-> test loss :  0.0008553 |||\n",
      "epoch:  51 ||| train loss :  0.0022727  <-> test loss :  0.0010629 |||\n",
      "epoch:  52 ||| train loss :  0.0023424  <-> test loss :  0.0011598 |||\n",
      "epoch:  53 ||| train loss :  0.0019113  <-> test loss :  0.0008736 |||\n",
      "save model\n",
      "epoch:  54 ||| train loss :  0.001982  <-> test loss :  0.0006085 |||\n",
      "epoch:  55 ||| train loss :  0.0021634  <-> test loss :  0.000703 |||\n",
      "save model\n",
      "epoch:  56 ||| train loss :  0.0022973  <-> test loss :  0.000527 |||\n",
      "epoch:  57 ||| train loss :  0.0021301  <-> test loss :  0.0009781 |||\n",
      "epoch:  58 ||| train loss :  0.0024531  <-> test loss :  0.0016183 |||\n",
      "epoch:  59 ||| train loss :  0.001638  <-> test loss :  0.0009087 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635710186.pth\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "0.90625\n",
      "0.96875\n",
      "1.0\n",
      "0.9375\n",
      "1.0\n",
      "0.90625\n",
      "0.9375\n",
      "0.90625\n",
      "0.90625\n",
      "0.96875\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.96875\n",
      "1.0\n",
      "0.9565217391304348\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  2.0827115  <-> test loss :  3.582118 |||\n",
      "epoch:  1 ||| train loss :  1.4293117  <-> test loss :  4.506423 |||\n",
      "save model\n",
      "epoch:  2 ||| train loss :  1.0585352  <-> test loss :  3.0541971 |||\n",
      "save model\n",
      "epoch:  3 ||| train loss :  0.8452491  <-> test loss :  0.9441286 |||\n",
      "epoch:  4 ||| train loss :  0.6608139  <-> test loss :  1.3171504 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  0.5323489  <-> test loss :  0.5469936 |||\n",
      "epoch:  6 ||| train loss :  0.4818547  <-> test loss :  0.9581816 |||\n",
      "epoch:  7 ||| train loss :  0.369682  <-> test loss :  1.8051503 |||\n",
      "save model\n",
      "epoch:  8 ||| train loss :  0.3114714  <-> test loss :  0.5205025 |||\n",
      "epoch:  9 ||| train loss :  0.2865589  <-> test loss :  1.1940857 |||\n",
      "save model\n",
      "epoch:  10 ||| train loss :  0.2302167  <-> test loss :  0.3309247 |||\n",
      "save model\n",
      "epoch:  11 ||| train loss :  0.2031249  <-> test loss :  0.2542154 |||\n",
      "epoch:  12 ||| train loss :  0.173258  <-> test loss :  1.0077899 |||\n",
      "epoch:  13 ||| train loss :  0.1568369  <-> test loss :  1.4309474 |||\n",
      "epoch:  14 ||| train loss :  0.1249933  <-> test loss :  0.4604898 |||\n",
      "epoch:  15 ||| train loss :  0.0894986  <-> test loss :  0.4462335 |||\n",
      "epoch:  16 ||| train loss :  0.0778016  <-> test loss :  0.440931 |||\n",
      "epoch:  17 ||| train loss :  0.0749984  <-> test loss :  5.1086006 |||\n",
      "save model\n",
      "epoch:  18 ||| train loss :  0.0473303  <-> test loss :  0.0386297 |||\n",
      "epoch:  19 ||| train loss :  0.0354325  <-> test loss :  0.0545163 |||\n",
      "epoch:  20 ||| train loss :  0.0595642  <-> test loss :  0.7964843 |||\n",
      "epoch:  21 ||| train loss :  0.0716922  <-> test loss :  0.7132586 |||\n",
      "epoch:  22 ||| train loss :  0.1028174  <-> test loss :  0.0566874 |||\n",
      "epoch:  23 ||| train loss :  0.0468748  <-> test loss :  0.0648132 |||\n",
      "epoch:  24 ||| train loss :  0.0428062  <-> test loss :  1.6974572 |||\n",
      "epoch:  25 ||| train loss :  0.0347266  <-> test loss :  0.7413625 |||\n",
      "epoch:  26 ||| train loss :  0.0220058  <-> test loss :  0.1835675 |||\n",
      "epoch:  27 ||| train loss :  0.0184275  <-> test loss :  0.1904652 |||\n",
      "save model\n",
      "epoch:  28 ||| train loss :  0.014948  <-> test loss :  0.007684 |||\n",
      "epoch:  29 ||| train loss :  0.0086071  <-> test loss :  0.0164985 |||\n",
      "save model\n",
      "epoch:  30 ||| train loss :  0.0083494  <-> test loss :  0.0071018 |||\n",
      "epoch:  31 ||| train loss :  0.009186  <-> test loss :  0.0130773 |||\n",
      "save model\n",
      "epoch:  32 ||| train loss :  0.0074716  <-> test loss :  0.0061667 |||\n",
      "save model\n",
      "epoch:  33 ||| train loss :  0.0069493  <-> test loss :  0.004741 |||\n",
      "epoch:  34 ||| train loss :  0.006633  <-> test loss :  0.0106813 |||\n",
      "save model\n",
      "epoch:  35 ||| train loss :  0.0056259  <-> test loss :  0.0020726 |||\n",
      "epoch:  36 ||| train loss :  0.0054199  <-> test loss :  0.0073305 |||\n",
      "epoch:  37 ||| train loss :  0.0047061  <-> test loss :  0.0070131 |||\n",
      "epoch:  38 ||| train loss :  0.004341  <-> test loss :  0.0051082 |||\n",
      "epoch:  39 ||| train loss :  0.0134334  <-> test loss :  0.7616807 |||\n",
      "epoch:  40 ||| train loss :  0.0086186  <-> test loss :  0.0062105 |||\n",
      "epoch:  41 ||| train loss :  0.0056597  <-> test loss :  0.0047018 |||\n",
      "epoch:  42 ||| train loss :  0.0038417  <-> test loss :  0.0031856 |||\n",
      "epoch:  43 ||| train loss :  0.0039256  <-> test loss :  0.0044989 |||\n",
      "epoch:  44 ||| train loss :  0.0089233  <-> test loss :  0.6920094 |||\n",
      "epoch:  45 ||| train loss :  0.078299  <-> test loss :  2.4695616 |||\n",
      "epoch:  46 ||| train loss :  0.0590054  <-> test loss :  0.4379908 |||\n",
      "epoch:  47 ||| train loss :  0.0295308  <-> test loss :  0.0724251 |||\n",
      "epoch:  48 ||| train loss :  0.0177458  <-> test loss :  0.0110931 |||\n",
      "epoch:  49 ||| train loss :  0.0076042  <-> test loss :  0.0096252 |||\n",
      "epoch:  50 ||| train loss :  0.0047394  <-> test loss :  0.0021538 |||\n",
      "save model\n",
      "epoch:  51 ||| train loss :  0.0046339  <-> test loss :  0.0016073 |||\n",
      "epoch:  52 ||| train loss :  0.0051608  <-> test loss :  0.0021649 |||\n",
      "epoch:  53 ||| train loss :  0.0036624  <-> test loss :  0.0016303 |||\n",
      "save model\n",
      "epoch:  54 ||| train loss :  0.0028931  <-> test loss :  0.0016065 |||\n",
      "save model\n",
      "epoch:  55 ||| train loss :  0.0030783  <-> test loss :  0.0010208 |||\n",
      "epoch:  56 ||| train loss :  0.0026546  <-> test loss :  0.0011103 |||\n",
      "epoch:  57 ||| train loss :  0.0026681  <-> test loss :  0.00119 |||\n",
      "epoch:  58 ||| train loss :  0.0027719  <-> test loss :  0.001288 |||\n",
      "epoch:  59 ||| train loss :  0.0035957  <-> test loss :  0.0013124 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635758071.pth\n",
      "0.90625\n",
      "0.9375\n",
      "1.0\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "1.0\n",
      "0.96875\n",
      "1.0\n",
      "0.9375\n",
      "0.96875\n",
      "0.96875\n",
      "0.875\n",
      "0.96875\n",
      "0.96875\n",
      "0.90625\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "0.9551630434782609\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  2.0822929  <-> test loss :  1.5457616 |||\n",
      "epoch:  1 ||| train loss :  1.409125  <-> test loss :  7.7149906 |||\n",
      "epoch:  2 ||| train loss :  1.1075335  <-> test loss :  2.1249933 |||\n",
      "epoch:  3 ||| train loss :  0.8723853  <-> test loss :  1.8911933 |||\n",
      "save model\n",
      "epoch:  4 ||| train loss :  0.6940192  <-> test loss :  1.4635518 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  0.5711529  <-> test loss :  0.8622375 |||\n",
      "save model\n",
      "epoch:  6 ||| train loss :  0.4743999  <-> test loss :  0.7470502 |||\n",
      "epoch:  7 ||| train loss :  0.3825109  <-> test loss :  2.0418746 |||\n",
      "epoch:  8 ||| train loss :  0.3359957  <-> test loss :  0.8726352 |||\n",
      "save model\n",
      "epoch:  9 ||| train loss :  0.2970405  <-> test loss :  0.3306914 |||\n",
      "epoch:  10 ||| train loss :  0.2341227  <-> test loss :  2.0588109 |||\n",
      "epoch:  11 ||| train loss :  0.2358279  <-> test loss :  0.75929 |||\n",
      "epoch:  12 ||| train loss :  0.200381  <-> test loss :  2.5652568 |||\n",
      "epoch:  13 ||| train loss :  0.1893141  <-> test loss :  1.1692674 |||\n",
      "epoch:  14 ||| train loss :  0.1460309  <-> test loss :  1.6892694 |||\n",
      "epoch:  15 ||| train loss :  0.1001827  <-> test loss :  0.463591 |||\n",
      "save model\n",
      "epoch:  16 ||| train loss :  0.1189176  <-> test loss :  0.2130472 |||\n",
      "save model\n",
      "epoch:  17 ||| train loss :  0.0981783  <-> test loss :  0.1547087 |||\n",
      "save model\n",
      "epoch:  18 ||| train loss :  0.0765157  <-> test loss :  0.1386182 |||\n",
      "save model\n",
      "epoch:  19 ||| train loss :  0.0550411  <-> test loss :  0.0866617 |||\n",
      "epoch:  20 ||| train loss :  0.0433014  <-> test loss :  0.1159918 |||\n",
      "epoch:  21 ||| train loss :  0.0382778  <-> test loss :  0.2206179 |||\n",
      "epoch:  22 ||| train loss :  0.0421703  <-> test loss :  0.1178423 |||\n",
      "epoch:  23 ||| train loss :  0.034966  <-> test loss :  0.3039246 |||\n",
      "save model\n",
      "epoch:  24 ||| train loss :  0.0289111  <-> test loss :  0.0107764 |||\n",
      "epoch:  25 ||| train loss :  0.0233663  <-> test loss :  0.0715113 |||\n",
      "epoch:  26 ||| train loss :  0.0195484  <-> test loss :  0.0279375 |||\n",
      "epoch:  27 ||| train loss :  0.0257275  <-> test loss :  0.1342698 |||\n",
      "epoch:  28 ||| train loss :  0.0291254  <-> test loss :  0.025366 |||\n",
      "epoch:  29 ||| train loss :  0.0211681  <-> test loss :  0.0528478 |||\n",
      "save model\n",
      "epoch:  30 ||| train loss :  0.0108838  <-> test loss :  0.0106312 |||\n",
      "save model\n",
      "epoch:  31 ||| train loss :  0.0105683  <-> test loss :  0.0063978 |||\n",
      "epoch:  32 ||| train loss :  0.0080983  <-> test loss :  0.0092946 |||\n",
      "save model\n",
      "epoch:  33 ||| train loss :  0.0055892  <-> test loss :  0.0054616 |||\n",
      "epoch:  34 ||| train loss :  0.0058431  <-> test loss :  0.1904742 |||\n",
      "save model\n",
      "epoch:  35 ||| train loss :  0.0061937  <-> test loss :  0.0016721 |||\n",
      "epoch:  36 ||| train loss :  0.0048373  <-> test loss :  0.0033992 |||\n",
      "epoch:  37 ||| train loss :  0.0042236  <-> test loss :  0.0017605 |||\n",
      "epoch:  38 ||| train loss :  0.0046579  <-> test loss :  0.0025864 |||\n",
      "epoch:  39 ||| train loss :  0.0048273  <-> test loss :  0.0030895 |||\n",
      "epoch:  40 ||| train loss :  0.0043912  <-> test loss :  0.0021729 |||\n",
      "epoch:  41 ||| train loss :  0.0034371  <-> test loss :  0.0017301 |||\n",
      "epoch:  42 ||| train loss :  0.0028707  <-> test loss :  0.0026299 |||\n",
      "save model\n",
      "epoch:  43 ||| train loss :  0.0029266  <-> test loss :  0.0013614 |||\n",
      "epoch:  44 ||| train loss :  0.0032132  <-> test loss :  0.0019415 |||\n",
      "save model\n",
      "epoch:  45 ||| train loss :  0.0026229  <-> test loss :  0.0012407 |||\n",
      "save model\n",
      "epoch:  46 ||| train loss :  0.0028225  <-> test loss :  0.0009893 |||\n",
      "epoch:  47 ||| train loss :  0.0022125  <-> test loss :  0.0011945 |||\n",
      "epoch:  48 ||| train loss :  0.0027602  <-> test loss :  0.0028447 |||\n",
      "epoch:  49 ||| train loss :  0.0025348  <-> test loss :  0.0011374 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.0021458  <-> test loss :  0.0007161 |||\n",
      "epoch:  51 ||| train loss :  0.0026349  <-> test loss :  0.0011227 |||\n",
      "epoch:  52 ||| train loss :  0.0022959  <-> test loss :  0.0011109 |||\n",
      "epoch:  53 ||| train loss :  0.0021767  <-> test loss :  0.0011175 |||\n",
      "save model\n",
      "epoch:  54 ||| train loss :  0.0026041  <-> test loss :  0.0006578 |||\n",
      "epoch:  55 ||| train loss :  0.0019986  <-> test loss :  0.0008556 |||\n",
      "epoch:  56 ||| train loss :  0.002045  <-> test loss :  0.0009042 |||\n",
      "save model\n",
      "epoch:  57 ||| train loss :  0.0025699  <-> test loss :  0.000595 |||\n",
      "epoch:  58 ||| train loss :  0.0018842  <-> test loss :  0.0007187 |||\n",
      "epoch:  59 ||| train loss :  0.0022707  <-> test loss :  0.0008184 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635805814.pth\n",
      "0.90625\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "0.96875\n",
      "0.84375\n",
      "0.90625\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.96875\n",
      "1.0\n",
      "0.9375\n",
      "1.0\n",
      "0.96875\n",
      "1.0\n",
      "0.9551630434782609\n",
      "[0.9578804347826086, 0.9483695652173914, 0.9565217391304348, 0.9551630434782609, 0.9551630434782609]\n",
      "0.9546195652173912\n",
      "0.9546\n",
      "----------loss weight==0.8----------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  1.8002781  <-> test loss :  1.4942071 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  1.2019065  <-> test loss :  1.4170078 |||\n",
      "epoch:  2 ||| train loss :  0.9147524  <-> test loss :  1.7083802 |||\n",
      "epoch:  3 ||| train loss :  0.6878358  <-> test loss :  2.6415117 |||\n",
      "save model\n",
      "epoch:  4 ||| train loss :  0.5370065  <-> test loss :  0.8080961 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  0.4579614  <-> test loss :  0.7295125 |||\n",
      "epoch:  6 ||| train loss :  0.3647137  <-> test loss :  2.3496845 |||\n",
      "save model\n",
      "epoch:  7 ||| train loss :  0.3088781  <-> test loss :  0.6874529 |||\n",
      "save model\n",
      "epoch:  8 ||| train loss :  0.2630196  <-> test loss :  0.3197187 |||\n",
      "epoch:  9 ||| train loss :  0.2136593  <-> test loss :  3.9502945 |||\n",
      "epoch:  10 ||| train loss :  0.2190689  <-> test loss :  0.5015734 |||\n",
      "epoch:  11 ||| train loss :  0.1641728  <-> test loss :  0.4983402 |||\n",
      "epoch:  12 ||| train loss :  0.1343133  <-> test loss :  2.6495452 |||\n",
      "epoch:  13 ||| train loss :  0.112008  <-> test loss :  1.6129577 |||\n",
      "save model\n",
      "epoch:  14 ||| train loss :  0.0934842  <-> test loss :  0.1011514 |||\n",
      "epoch:  15 ||| train loss :  0.0838779  <-> test loss :  0.3139064 |||\n",
      "epoch:  16 ||| train loss :  0.0643803  <-> test loss :  0.2055955 |||\n",
      "epoch:  17 ||| train loss :  0.0487715  <-> test loss :  0.9068483 |||\n",
      "epoch:  18 ||| train loss :  0.0378269  <-> test loss :  0.2908171 |||\n",
      "epoch:  19 ||| train loss :  0.0278134  <-> test loss :  0.1833449 |||\n",
      "save model\n",
      "epoch:  20 ||| train loss :  0.0304138  <-> test loss :  0.0249993 |||\n",
      "epoch:  21 ||| train loss :  0.0230992  <-> test loss :  0.1225473 |||\n",
      "save model\n",
      "epoch:  22 ||| train loss :  0.020179  <-> test loss :  0.022351 |||\n",
      "epoch:  23 ||| train loss :  0.0216789  <-> test loss :  0.0824962 |||\n",
      "save model\n",
      "epoch:  24 ||| train loss :  0.0184334  <-> test loss :  0.0130076 |||\n",
      "epoch:  25 ||| train loss :  0.0156494  <-> test loss :  0.039831 |||\n",
      "save model\n",
      "epoch:  26 ||| train loss :  0.0135829  <-> test loss :  0.0063253 |||\n",
      "epoch:  27 ||| train loss :  0.0089695  <-> test loss :  0.006961 |||\n",
      "epoch:  28 ||| train loss :  0.0096824  <-> test loss :  0.0072773 |||\n",
      "save model\n",
      "epoch:  29 ||| train loss :  0.0078454  <-> test loss :  0.0047012 |||\n",
      "epoch:  30 ||| train loss :  0.0078417  <-> test loss :  0.0510598 |||\n",
      "save model\n",
      "epoch:  31 ||| train loss :  0.005693  <-> test loss :  0.0036571 |||\n",
      "save model\n",
      "epoch:  32 ||| train loss :  0.0061634  <-> test loss :  0.0031049 |||\n",
      "save model\n",
      "epoch:  33 ||| train loss :  0.0055726  <-> test loss :  0.0026351 |||\n",
      "epoch:  34 ||| train loss :  0.0132729  <-> test loss :  2.849761 |||\n",
      "epoch:  35 ||| train loss :  0.0239215  <-> test loss :  0.0598849 |||\n",
      "epoch:  36 ||| train loss :  0.0190882  <-> test loss :  0.0194586 |||\n",
      "epoch:  37 ||| train loss :  0.0073792  <-> test loss :  0.0216241 |||\n",
      "epoch:  38 ||| train loss :  0.00568  <-> test loss :  0.0740992 |||\n",
      "save model\n",
      "epoch:  39 ||| train loss :  0.004522  <-> test loss :  0.0019207 |||\n",
      "epoch:  40 ||| train loss :  0.0049073  <-> test loss :  0.002969 |||\n",
      "epoch:  41 ||| train loss :  0.0061773  <-> test loss :  0.0041397 |||\n",
      "save model\n",
      "epoch:  42 ||| train loss :  0.0038745  <-> test loss :  0.0016506 |||\n",
      "epoch:  43 ||| train loss :  0.0036581  <-> test loss :  0.0016934 |||\n",
      "epoch:  44 ||| train loss :  0.0048021  <-> test loss :  0.0088692 |||\n",
      "save model\n",
      "epoch:  45 ||| train loss :  0.0033751  <-> test loss :  0.0010263 |||\n",
      "epoch:  46 ||| train loss :  0.0030646  <-> test loss :  0.0012371 |||\n",
      "save model\n",
      "epoch:  47 ||| train loss :  0.0022275  <-> test loss :  0.0010051 |||\n",
      "epoch:  48 ||| train loss :  0.0021606  <-> test loss :  0.0010313 |||\n",
      "save model\n",
      "epoch:  49 ||| train loss :  0.0021799  <-> test loss :  0.0009772 |||\n",
      "epoch:  50 ||| train loss :  0.0017555  <-> test loss :  0.0013344 |||\n",
      "save model\n",
      "epoch:  51 ||| train loss :  0.0017437  <-> test loss :  0.000805 |||\n",
      "save model\n",
      "epoch:  52 ||| train loss :  0.0021052  <-> test loss :  0.0006367 |||\n",
      "epoch:  53 ||| train loss :  0.0015321  <-> test loss :  0.0007166 |||\n",
      "save model\n",
      "epoch:  54 ||| train loss :  0.001668  <-> test loss :  0.0005121 |||\n",
      "epoch:  55 ||| train loss :  0.0023014  <-> test loss :  0.0009237 |||\n",
      "epoch:  56 ||| train loss :  0.0017328  <-> test loss :  0.0009654 |||\n",
      "epoch:  57 ||| train loss :  0.0016411  <-> test loss :  0.0008033 |||\n",
      "epoch:  58 ||| train loss :  0.001439  <-> test loss :  0.0009086 |||\n",
      "epoch:  59 ||| train loss :  0.0016982  <-> test loss :  0.0007396 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635853464.pth\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "1.0\n",
      "0.9375\n",
      "0.96875\n",
      "0.90625\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9375\n",
      "0.9375\n",
      "1.0\n",
      "0.9375\n",
      "0.9375\n",
      "0.90625\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9619565217391305\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  1.7729811  <-> test loss :  2.69875 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  1.239385  <-> test loss :  1.5428159 |||\n",
      "save model\n",
      "epoch:  2 ||| train loss :  0.8994837  <-> test loss :  0.9703523 |||\n",
      "epoch:  3 ||| train loss :  0.6745477  <-> test loss :  1.9471445 |||\n",
      "epoch:  4 ||| train loss :  0.5396892  <-> test loss :  1.0837629 |||\n",
      "epoch:  5 ||| train loss :  0.4759026  <-> test loss :  1.2190912 |||\n",
      "epoch:  6 ||| train loss :  0.3593577  <-> test loss :  2.4332626 |||\n",
      "epoch:  7 ||| train loss :  0.2926752  <-> test loss :  2.1952412 |||\n",
      "epoch:  8 ||| train loss :  0.2634754  <-> test loss :  1.1609732 |||\n",
      "epoch:  9 ||| train loss :  0.2300537  <-> test loss :  1.2422869 |||\n",
      "save model\n",
      "epoch:  10 ||| train loss :  0.1807736  <-> test loss :  0.2663514 |||\n",
      "epoch:  11 ||| train loss :  0.1798306  <-> test loss :  0.3506424 |||\n",
      "epoch:  12 ||| train loss :  0.1465889  <-> test loss :  4.3710785 |||\n",
      "epoch:  13 ||| train loss :  0.1147584  <-> test loss :  1.6288981 |||\n",
      "epoch:  14 ||| train loss :  0.0895476  <-> test loss :  0.6209223 |||\n",
      "epoch:  15 ||| train loss :  0.0839079  <-> test loss :  1.0310023 |||\n",
      "save model\n",
      "epoch:  16 ||| train loss :  0.0627692  <-> test loss :  0.1940035 |||\n",
      "epoch:  17 ||| train loss :  0.0728949  <-> test loss :  0.828631 |||\n",
      "epoch:  18 ||| train loss :  0.0597301  <-> test loss :  0.5984184 |||\n",
      "save model\n",
      "epoch:  19 ||| train loss :  0.0369526  <-> test loss :  0.1417619 |||\n",
      "save model\n",
      "epoch:  20 ||| train loss :  0.0340737  <-> test loss :  0.0448014 |||\n",
      "save model\n",
      "epoch:  21 ||| train loss :  0.0261163  <-> test loss :  0.0268627 |||\n",
      "epoch:  22 ||| train loss :  0.022326  <-> test loss :  0.2727022 |||\n",
      "save model\n",
      "epoch:  23 ||| train loss :  0.0186236  <-> test loss :  0.0156543 |||\n",
      "epoch:  24 ||| train loss :  0.016771  <-> test loss :  0.1932138 |||\n",
      "epoch:  25 ||| train loss :  0.0118996  <-> test loss :  0.0170946 |||\n",
      "save model\n",
      "epoch:  26 ||| train loss :  0.009065  <-> test loss :  0.0060272 |||\n",
      "epoch:  27 ||| train loss :  0.0095019  <-> test loss :  0.0103668 |||\n",
      "save model\n",
      "epoch:  28 ||| train loss :  0.0070892  <-> test loss :  0.0039668 |||\n",
      "epoch:  29 ||| train loss :  0.0064849  <-> test loss :  0.0117665 |||\n",
      "epoch:  30 ||| train loss :  0.0092209  <-> test loss :  0.0284985 |||\n",
      "epoch:  31 ||| train loss :  0.0149595  <-> test loss :  0.0692616 |||\n",
      "epoch:  32 ||| train loss :  0.0118056  <-> test loss :  0.0577224 |||\n",
      "epoch:  33 ||| train loss :  0.0100698  <-> test loss :  0.0479349 |||\n",
      "epoch:  34 ||| train loss :  0.0058834  <-> test loss :  0.00623 |||\n",
      "save model\n",
      "epoch:  35 ||| train loss :  0.005869  <-> test loss :  0.002955 |||\n",
      "epoch:  36 ||| train loss :  0.004825  <-> test loss :  0.0044701 |||\n",
      "epoch:  37 ||| train loss :  0.0055144  <-> test loss :  0.0168183 |||\n",
      "epoch:  38 ||| train loss :  0.0232366  <-> test loss :  0.5834303 |||\n",
      "epoch:  39 ||| train loss :  0.0364667  <-> test loss :  1.1581653 |||\n",
      "epoch:  40 ||| train loss :  0.0255789  <-> test loss :  0.0942345 |||\n",
      "epoch:  41 ||| train loss :  0.0184659  <-> test loss :  0.0304664 |||\n",
      "epoch:  42 ||| train loss :  0.0142638  <-> test loss :  0.1120753 |||\n",
      "epoch:  43 ||| train loss :  0.0197901  <-> test loss :  0.019636 |||\n",
      "epoch:  44 ||| train loss :  0.0097578  <-> test loss :  0.0134634 |||\n",
      "save model\n",
      "epoch:  45 ||| train loss :  0.004659  <-> test loss :  0.0010397 |||\n",
      "epoch:  46 ||| train loss :  0.0035989  <-> test loss :  0.004345 |||\n",
      "epoch:  47 ||| train loss :  0.0033372  <-> test loss :  0.0021156 |||\n",
      "epoch:  48 ||| train loss :  0.0047394  <-> test loss :  0.0024525 |||\n",
      "epoch:  49 ||| train loss :  0.0027882  <-> test loss :  0.0014832 |||\n",
      "epoch:  50 ||| train loss :  0.0020426  <-> test loss :  0.0014211 |||\n",
      "save model\n",
      "epoch:  51 ||| train loss :  0.0022474  <-> test loss :  0.0008959 |||\n",
      "save model\n",
      "epoch:  52 ||| train loss :  0.0022438  <-> test loss :  0.000818 |||\n",
      "save model\n",
      "epoch:  53 ||| train loss :  0.0019109  <-> test loss :  0.0007116 |||\n",
      "epoch:  54 ||| train loss :  0.0026023  <-> test loss :  0.0013108 |||\n",
      "save model\n",
      "epoch:  55 ||| train loss :  0.0021015  <-> test loss :  0.0005401 |||\n",
      "epoch:  56 ||| train loss :  0.0017481  <-> test loss :  0.0009289 |||\n",
      "epoch:  57 ||| train loss :  0.0017054  <-> test loss :  0.0010481 |||\n",
      "epoch:  58 ||| train loss :  0.001637  <-> test loss :  0.001104 |||\n",
      "epoch:  59 ||| train loss :  0.0020229  <-> test loss :  0.0006126 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635901237.pth\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.875\n",
      "1.0\n",
      "0.90625\n",
      "0.90625\n",
      "0.96875\n",
      "0.96875\n",
      "0.90625\n",
      "1.0\n",
      "0.9375\n",
      "1.0\n",
      "0.9660326086956522\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  1.8229904  <-> test loss :  1.6795288 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  1.2269235  <-> test loss :  1.4566603 |||\n",
      "epoch:  2 ||| train loss :  0.9132829  <-> test loss :  2.3145235 |||\n",
      "epoch:  3 ||| train loss :  0.7122351  <-> test loss :  1.7688208 |||\n",
      "epoch:  4 ||| train loss :  0.5606957  <-> test loss :  3.1012714 |||\n",
      "epoch:  5 ||| train loss :  0.5016179  <-> test loss :  4.5985084 |||\n",
      "epoch:  6 ||| train loss :  0.4057628  <-> test loss :  4.6769476 |||\n",
      "epoch:  7 ||| train loss :  0.3672463  <-> test loss :  1.7371795 |||\n",
      "epoch:  8 ||| train loss :  0.2759996  <-> test loss :  1.5927708 |||\n",
      "epoch:  9 ||| train loss :  0.2394539  <-> test loss :  4.6826062 |||\n",
      "save model\n",
      "epoch:  10 ||| train loss :  0.2174214  <-> test loss :  1.2118731 |||\n",
      "save model\n",
      "epoch:  11 ||| train loss :  0.1810368  <-> test loss :  0.4756271 |||\n",
      "save model\n",
      "epoch:  12 ||| train loss :  0.1607595  <-> test loss :  0.4342306 |||\n",
      "epoch:  13 ||| train loss :  0.1140443  <-> test loss :  1.2824321 |||\n",
      "epoch:  14 ||| train loss :  0.1146042  <-> test loss :  0.709425 |||\n",
      "epoch:  15 ||| train loss :  0.1017537  <-> test loss :  1.237916 |||\n",
      "epoch:  16 ||| train loss :  0.1129693  <-> test loss :  0.8135229 |||\n",
      "epoch:  17 ||| train loss :  0.0822386  <-> test loss :  1.1517024 |||\n",
      "epoch:  18 ||| train loss :  0.0495817  <-> test loss :  0.729627 |||\n",
      "save model\n",
      "epoch:  19 ||| train loss :  0.0345625  <-> test loss :  0.3291754 |||\n",
      "save model\n",
      "epoch:  20 ||| train loss :  0.0440954  <-> test loss :  0.1481548 |||\n",
      "save model\n",
      "epoch:  21 ||| train loss :  0.0333664  <-> test loss :  0.0667325 |||\n",
      "epoch:  22 ||| train loss :  0.0257298  <-> test loss :  0.0854273 |||\n",
      "epoch:  23 ||| train loss :  0.032311  <-> test loss :  0.1908552 |||\n",
      "epoch:  24 ||| train loss :  0.0272402  <-> test loss :  0.8312817 |||\n",
      "save model\n",
      "epoch:  25 ||| train loss :  0.018135  <-> test loss :  0.0215388 |||\n",
      "epoch:  26 ||| train loss :  0.0526846  <-> test loss :  0.7870733 |||\n",
      "epoch:  27 ||| train loss :  0.0486716  <-> test loss :  1.0871537 |||\n",
      "epoch:  28 ||| train loss :  0.0332672  <-> test loss :  0.4590727 |||\n",
      "epoch:  29 ||| train loss :  0.016815  <-> test loss :  0.0605169 |||\n",
      "epoch:  30 ||| train loss :  0.0231172  <-> test loss :  3.8203559 |||\n",
      "epoch:  31 ||| train loss :  0.0188056  <-> test loss :  0.0599207 |||\n",
      "epoch:  32 ||| train loss :  0.0143169  <-> test loss :  0.0586686 |||\n",
      "save model\n",
      "epoch:  33 ||| train loss :  0.0075198  <-> test loss :  0.0122484 |||\n",
      "save model\n",
      "epoch:  34 ||| train loss :  0.0084807  <-> test loss :  0.0116772 |||\n",
      "save model\n",
      "epoch:  35 ||| train loss :  0.0051552  <-> test loss :  0.0020582 |||\n",
      "epoch:  36 ||| train loss :  0.0051981  <-> test loss :  0.0037326 |||\n",
      "save model\n",
      "epoch:  37 ||| train loss :  0.0039887  <-> test loss :  0.0011235 |||\n",
      "epoch:  38 ||| train loss :  0.0033087  <-> test loss :  0.0016221 |||\n",
      "epoch:  39 ||| train loss :  0.0036981  <-> test loss :  0.0015828 |||\n",
      "epoch:  40 ||| train loss :  0.003388  <-> test loss :  0.0018036 |||\n",
      "epoch:  41 ||| train loss :  0.0030599  <-> test loss :  0.0012472 |||\n",
      "epoch:  42 ||| train loss :  0.0034056  <-> test loss :  0.0013386 |||\n",
      "epoch:  43 ||| train loss :  0.003973  <-> test loss :  0.0026142 |||\n",
      "epoch:  44 ||| train loss :  0.002516  <-> test loss :  0.0012983 |||\n",
      "epoch:  45 ||| train loss :  0.0028142  <-> test loss :  0.0017666 |||\n",
      "epoch:  46 ||| train loss :  0.0028558  <-> test loss :  0.0014278 |||\n",
      "save model\n",
      "epoch:  47 ||| train loss :  0.0021925  <-> test loss :  0.0007431 |||\n",
      "epoch:  48 ||| train loss :  0.0023296  <-> test loss :  0.0008262 |||\n",
      "epoch:  49 ||| train loss :  0.0037273  <-> test loss :  0.0168393 |||\n",
      "epoch:  50 ||| train loss :  0.0023716  <-> test loss :  0.0013672 |||\n",
      "epoch:  51 ||| train loss :  0.0022012  <-> test loss :  0.001043 |||\n",
      "epoch:  52 ||| train loss :  0.0018406  <-> test loss :  0.000775 |||\n",
      "save model\n",
      "epoch:  53 ||| train loss :  0.0017595  <-> test loss :  0.0005436 |||\n",
      "epoch:  54 ||| train loss :  0.0019633  <-> test loss :  0.0007181 |||\n",
      "save model\n",
      "epoch:  55 ||| train loss :  0.0016518  <-> test loss :  0.0004518 |||\n",
      "epoch:  56 ||| train loss :  0.0019252  <-> test loss :  0.0010959 |||\n",
      "epoch:  57 ||| train loss :  0.0018351  <-> test loss :  0.000733 |||\n",
      "epoch:  58 ||| train loss :  0.0023978  <-> test loss :  0.0007508 |||\n",
      "epoch:  59 ||| train loss :  0.00172  <-> test loss :  0.000809 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635948929.pth\n",
      "0.84375\n",
      "0.90625\n",
      "0.9375\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "0.90625\n",
      "0.96875\n",
      "0.875\n",
      "0.90625\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "0.875\n",
      "0.9375\n",
      "0.90625\n",
      "0.96875\n",
      "0.9375\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "0.9456521739130435\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  1.7927368  <-> test loss :  2.3364639 |||\n",
      "epoch:  1 ||| train loss :  1.2148028  <-> test loss :  3.4495692 |||\n",
      "save model\n",
      "epoch:  2 ||| train loss :  0.9135262  <-> test loss :  0.8861623 |||\n",
      "epoch:  3 ||| train loss :  0.6843001  <-> test loss :  0.897065 |||\n",
      "epoch:  4 ||| train loss :  0.5607295  <-> test loss :  1.9482846 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  0.4221067  <-> test loss :  0.6461757 |||\n",
      "epoch:  6 ||| train loss :  0.364654  <-> test loss :  1.5732737 |||\n",
      "epoch:  7 ||| train loss :  0.2935302  <-> test loss :  1.89555 |||\n",
      "epoch:  8 ||| train loss :  0.2727298  <-> test loss :  1.74195 |||\n",
      "save model\n",
      "epoch:  9 ||| train loss :  0.2317092  <-> test loss :  0.5110598 |||\n",
      "epoch:  10 ||| train loss :  0.1718273  <-> test loss :  4.8032246 |||\n",
      "save model\n",
      "epoch:  11 ||| train loss :  0.1541157  <-> test loss :  0.2275621 |||\n",
      "epoch:  12 ||| train loss :  0.1337204  <-> test loss :  0.4232063 |||\n",
      "epoch:  13 ||| train loss :  0.1227864  <-> test loss :  1.1269543 |||\n",
      "epoch:  14 ||| train loss :  0.0932784  <-> test loss :  0.363765 |||\n",
      "epoch:  15 ||| train loss :  0.1052144  <-> test loss :  0.3262373 |||\n",
      "epoch:  16 ||| train loss :  0.0596978  <-> test loss :  0.4205565 |||\n",
      "epoch:  17 ||| train loss :  0.04708  <-> test loss :  0.5320097 |||\n",
      "save model\n",
      "epoch:  18 ||| train loss :  0.0418191  <-> test loss :  0.0317209 |||\n",
      "epoch:  19 ||| train loss :  0.0345218  <-> test loss :  0.0964478 |||\n",
      "epoch:  20 ||| train loss :  0.040078  <-> test loss :  0.1233429 |||\n",
      "epoch:  21 ||| train loss :  0.0435968  <-> test loss :  1.6187727 |||\n",
      "epoch:  22 ||| train loss :  0.0360606  <-> test loss :  0.0789036 |||\n",
      "epoch:  23 ||| train loss :  0.0301362  <-> test loss :  0.0970879 |||\n",
      "epoch:  24 ||| train loss :  0.0168035  <-> test loss :  0.0951416 |||\n",
      "save model\n",
      "epoch:  25 ||| train loss :  0.014807  <-> test loss :  0.0119361 |||\n",
      "save model\n",
      "epoch:  26 ||| train loss :  0.0108593  <-> test loss :  0.0042426 |||\n",
      "epoch:  27 ||| train loss :  0.0096584  <-> test loss :  0.0113831 |||\n",
      "epoch:  28 ||| train loss :  0.0230646  <-> test loss :  0.0961245 |||\n",
      "epoch:  29 ||| train loss :  0.0124078  <-> test loss :  0.0164355 |||\n",
      "epoch:  30 ||| train loss :  0.0065026  <-> test loss :  0.032191 |||\n",
      "epoch:  31 ||| train loss :  0.0125501  <-> test loss :  0.0568544 |||\n",
      "save model\n",
      "epoch:  32 ||| train loss :  0.0076306  <-> test loss :  0.0029911 |||\n",
      "epoch:  33 ||| train loss :  0.0055709  <-> test loss :  0.0096972 |||\n",
      "epoch:  34 ||| train loss :  0.0051228  <-> test loss :  0.0043084 |||\n",
      "epoch:  35 ||| train loss :  0.0042156  <-> test loss :  0.0036422 |||\n",
      "save model\n",
      "epoch:  36 ||| train loss :  0.0038655  <-> test loss :  0.0021765 |||\n",
      "save model\n",
      "epoch:  37 ||| train loss :  0.0038392  <-> test loss :  0.0012406 |||\n",
      "epoch:  38 ||| train loss :  0.0035915  <-> test loss :  0.0029874 |||\n",
      "epoch:  39 ||| train loss :  0.0039111  <-> test loss :  0.0024233 |||\n",
      "epoch:  40 ||| train loss :  0.0037772  <-> test loss :  0.0459899 |||\n",
      "epoch:  41 ||| train loss :  0.0038212  <-> test loss :  0.0025711 |||\n",
      "epoch:  42 ||| train loss :  0.0024773  <-> test loss :  0.0016083 |||\n",
      "epoch:  43 ||| train loss :  0.0026827  <-> test loss :  0.0019869 |||\n",
      "epoch:  44 ||| train loss :  0.0033763  <-> test loss :  0.0263856 |||\n",
      "epoch:  45 ||| train loss :  0.0034309  <-> test loss :  0.0021819 |||\n",
      "save model\n",
      "epoch:  46 ||| train loss :  0.0028587  <-> test loss :  0.0010859 |||\n",
      "epoch:  47 ||| train loss :  0.0284555  <-> test loss :  0.2879854 |||\n",
      "epoch:  48 ||| train loss :  0.0413108  <-> test loss :  2.3567173 |||\n",
      "epoch:  49 ||| train loss :  0.0348989  <-> test loss :  0.3367942 |||\n",
      "epoch:  50 ||| train loss :  0.0163785  <-> test loss :  0.012381 |||\n",
      "epoch:  51 ||| train loss :  0.0054564  <-> test loss :  0.0021134 |||\n",
      "epoch:  52 ||| train loss :  0.0048909  <-> test loss :  0.0025843 |||\n",
      "epoch:  53 ||| train loss :  0.0042991  <-> test loss :  0.0031586 |||\n",
      "epoch:  54 ||| train loss :  0.0034896  <-> test loss :  0.0017316 |||\n",
      "epoch:  55 ||| train loss :  0.0029143  <-> test loss :  0.0020111 |||\n",
      "epoch:  56 ||| train loss :  0.0034253  <-> test loss :  0.0011282 |||\n",
      "epoch:  57 ||| train loss :  0.0023988  <-> test loss :  0.0011972 |||\n",
      "save model\n",
      "epoch:  58 ||| train loss :  0.0022179  <-> test loss :  0.0008756 |||\n",
      "epoch:  59 ||| train loss :  0.0022067  <-> test loss :  0.0014353 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165635996687.pth\n",
      "0.875\n",
      "0.9375\n",
      "1.0\n",
      "1.0\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "0.875\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "1.0\n",
      "0.9375\n",
      "1.0\n",
      "0.9375\n",
      "0.90625\n",
      "1.0\n",
      "0.9565217391304348\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  1.770628  <-> test loss :  2.6319249 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  1.2113623  <-> test loss :  1.8535864 |||\n",
      "epoch:  2 ||| train loss :  0.9375483  <-> test loss :  4.3141098 |||\n",
      "epoch:  3 ||| train loss :  0.7197669  <-> test loss :  4.782299 |||\n",
      "epoch:  4 ||| train loss :  0.5971889  <-> test loss :  4.5614119 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  0.498357  <-> test loss :  1.5021608 |||\n",
      "epoch:  6 ||| train loss :  0.421854  <-> test loss :  1.8524895 |||\n",
      "save model\n",
      "epoch:  7 ||| train loss :  0.3769134  <-> test loss :  1.2335095 |||\n",
      "save model\n",
      "epoch:  8 ||| train loss :  0.3036513  <-> test loss :  0.3099785 |||\n",
      "epoch:  9 ||| train loss :  0.2586211  <-> test loss :  2.6108825 |||\n",
      "epoch:  10 ||| train loss :  0.2028346  <-> test loss :  6.4597411 |||\n",
      "save model\n",
      "epoch:  11 ||| train loss :  0.1910887  <-> test loss :  0.2404893 |||\n",
      "epoch:  12 ||| train loss :  0.169962  <-> test loss :  0.6943892 |||\n",
      "epoch:  13 ||| train loss :  0.1552066  <-> test loss :  0.6954327 |||\n",
      "epoch:  14 ||| train loss :  0.1206691  <-> test loss :  0.4011319 |||\n",
      "epoch:  15 ||| train loss :  0.088738  <-> test loss :  0.4088882 |||\n",
      "epoch:  16 ||| train loss :  0.0836034  <-> test loss :  0.4847 |||\n",
      "save model\n",
      "epoch:  17 ||| train loss :  0.0791226  <-> test loss :  0.0916424 |||\n",
      "epoch:  18 ||| train loss :  0.0685465  <-> test loss :  0.3841341 |||\n",
      "epoch:  19 ||| train loss :  0.0580388  <-> test loss :  0.2409335 |||\n",
      "save model\n",
      "epoch:  20 ||| train loss :  0.0404001  <-> test loss :  0.0267314 |||\n",
      "epoch:  21 ||| train loss :  0.0390506  <-> test loss :  0.0991885 |||\n",
      "epoch:  22 ||| train loss :  0.0260062  <-> test loss :  0.0687546 |||\n",
      "save model\n",
      "epoch:  23 ||| train loss :  0.0176349  <-> test loss :  0.0193472 |||\n",
      "save model\n",
      "epoch:  24 ||| train loss :  0.0147498  <-> test loss :  0.0188281 |||\n",
      "save model\n",
      "epoch:  25 ||| train loss :  0.0138756  <-> test loss :  0.017803 |||\n",
      "epoch:  26 ||| train loss :  0.0148622  <-> test loss :  0.0191825 |||\n",
      "save model\n",
      "epoch:  27 ||| train loss :  0.009207  <-> test loss :  0.0080948 |||\n",
      "epoch:  28 ||| train loss :  0.0119314  <-> test loss :  0.0128489 |||\n",
      "epoch:  29 ||| train loss :  0.0133535  <-> test loss :  0.1411657 |||\n",
      "epoch:  30 ||| train loss :  0.0110063  <-> test loss :  0.0194109 |||\n",
      "save model\n",
      "epoch:  31 ||| train loss :  0.0086884  <-> test loss :  0.0074544 |||\n",
      "save model\n",
      "epoch:  32 ||| train loss :  0.0063167  <-> test loss :  0.0046128 |||\n",
      "save model\n",
      "epoch:  33 ||| train loss :  0.0074879  <-> test loss :  0.0037376 |||\n",
      "save model\n",
      "epoch:  34 ||| train loss :  0.0047968  <-> test loss :  0.0035182 |||\n",
      "save model\n",
      "epoch:  35 ||| train loss :  0.0042698  <-> test loss :  0.0020211 |||\n",
      "epoch:  36 ||| train loss :  0.0079065  <-> test loss :  0.0213854 |||\n",
      "epoch:  37 ||| train loss :  0.0050481  <-> test loss :  0.0030374 |||\n",
      "epoch:  38 ||| train loss :  0.004653  <-> test loss :  0.0046768 |||\n",
      "epoch:  39 ||| train loss :  0.0054158  <-> test loss :  0.0020638 |||\n",
      "epoch:  40 ||| train loss :  0.0045366  <-> test loss :  0.0069069 |||\n",
      "save model\n",
      "epoch:  41 ||| train loss :  0.0053195  <-> test loss :  0.0019375 |||\n",
      "save model\n",
      "epoch:  42 ||| train loss :  0.0040589  <-> test loss :  0.0012691 |||\n",
      "epoch:  43 ||| train loss :  0.0038128  <-> test loss :  0.0018155 |||\n",
      "epoch:  44 ||| train loss :  0.0034044  <-> test loss :  0.0046877 |||\n",
      "epoch:  45 ||| train loss :  0.0027033  <-> test loss :  0.0012718 |||\n",
      "epoch:  46 ||| train loss :  0.0027646  <-> test loss :  0.0014723 |||\n",
      "epoch:  47 ||| train loss :  0.0028327  <-> test loss :  0.001585 |||\n",
      "epoch:  48 ||| train loss :  0.0029553  <-> test loss :  0.0026082 |||\n",
      "save model\n",
      "epoch:  49 ||| train loss :  0.0022659  <-> test loss :  0.0008679 |||\n",
      "epoch:  50 ||| train loss :  0.0018913  <-> test loss :  0.0009598 |||\n",
      "epoch:  51 ||| train loss :  0.0018855  <-> test loss :  0.0010607 |||\n",
      "save model\n",
      "epoch:  52 ||| train loss :  0.0020336  <-> test loss :  0.0006912 |||\n",
      "epoch:  53 ||| train loss :  0.0022508  <-> test loss :  0.000803 |||\n",
      "epoch:  54 ||| train loss :  0.0016186  <-> test loss :  0.0010077 |||\n",
      "save model\n",
      "epoch:  55 ||| train loss :  0.0017099  <-> test loss :  0.0006689 |||\n",
      "epoch:  56 ||| train loss :  0.0019667  <-> test loss :  0.0006951 |||\n",
      "epoch:  57 ||| train loss :  0.0018479  <-> test loss :  0.0007554 |||\n",
      "save model\n",
      "epoch:  58 ||| train loss :  0.0018361  <-> test loss :  0.0006165 |||\n",
      "epoch:  59 ||| train loss :  0.0022984  <-> test loss :  0.0007876 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165636044399.pth\n",
      "0.90625\n",
      "0.9375\n",
      "0.96875\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9375\n",
      "1.0\n",
      "0.9375\n",
      "0.84375\n",
      "1.0\n",
      "0.96875\n",
      "0.90625\n",
      "0.9375\n",
      "0.9375\n",
      "1.0\n",
      "0.9538043478260869\n",
      "[0.9619565217391305, 0.9660326086956522, 0.9456521739130435, 0.9565217391304348, 0.9538043478260869]\n",
      "0.9567934782608696\n",
      "0.9568\n",
      "----------loss weight==0.5----------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  1.4282331  <-> test loss :  1.8419417 |||\n",
      "epoch:  1 ||| train loss :  0.9908162  <-> test loss :  2.1526535 |||\n",
      "save model\n",
      "epoch:  2 ||| train loss :  0.7190722  <-> test loss :  1.0856768 |||\n",
      "save model\n",
      "epoch:  3 ||| train loss :  0.5545364  <-> test loss :  0.5947067 |||\n",
      "epoch:  4 ||| train loss :  0.4549768  <-> test loss :  1.4566872 |||\n",
      "epoch:  5 ||| train loss :  0.3844985  <-> test loss :  2.2528777 |||\n",
      "epoch:  6 ||| train loss :  0.3123981  <-> test loss :  0.788948 |||\n",
      "save model\n",
      "epoch:  7 ||| train loss :  0.260886  <-> test loss :  0.4928114 |||\n",
      "epoch:  8 ||| train loss :  0.2199453  <-> test loss :  2.8262336 |||\n",
      "epoch:  9 ||| train loss :  0.2132784  <-> test loss :  0.76407 |||\n",
      "save model\n",
      "epoch:  10 ||| train loss :  0.1682845  <-> test loss :  0.2792412 |||\n",
      "epoch:  11 ||| train loss :  0.1470561  <-> test loss :  2.0110996 |||\n",
      "save model\n",
      "epoch:  12 ||| train loss :  0.1236259  <-> test loss :  0.0816006 |||\n",
      "epoch:  13 ||| train loss :  0.1062212  <-> test loss :  0.2244975 |||\n",
      "epoch:  14 ||| train loss :  0.0868316  <-> test loss :  0.5840241 |||\n",
      "epoch:  15 ||| train loss :  0.0779811  <-> test loss :  0.325748 |||\n",
      "epoch:  16 ||| train loss :  0.098298  <-> test loss :  0.9810624 |||\n",
      "epoch:  17 ||| train loss :  0.060015  <-> test loss :  0.1056373 |||\n",
      "epoch:  18 ||| train loss :  0.0453229  <-> test loss :  0.2469837 |||\n",
      "save model\n",
      "epoch:  19 ||| train loss :  0.0321742  <-> test loss :  0.0216955 |||\n",
      "epoch:  20 ||| train loss :  0.0246405  <-> test loss :  0.0784136 |||\n",
      "epoch:  21 ||| train loss :  0.0216468  <-> test loss :  0.0533028 |||\n",
      "save model\n",
      "epoch:  22 ||| train loss :  0.0226857  <-> test loss :  0.0131005 |||\n",
      "epoch:  23 ||| train loss :  0.0173026  <-> test loss :  0.0237327 |||\n",
      "epoch:  24 ||| train loss :  0.0129325  <-> test loss :  0.0171467 |||\n",
      "epoch:  25 ||| train loss :  0.0145722  <-> test loss :  0.1117174 |||\n",
      "save model\n",
      "epoch:  26 ||| train loss :  0.0111957  <-> test loss :  0.0030412 |||\n",
      "epoch:  27 ||| train loss :  0.0090487  <-> test loss :  0.1781102 |||\n",
      "epoch:  28 ||| train loss :  0.0078434  <-> test loss :  0.003918 |||\n",
      "epoch:  29 ||| train loss :  0.0081414  <-> test loss :  0.0227523 |||\n",
      "epoch:  30 ||| train loss :  0.0125902  <-> test loss :  0.0517719 |||\n",
      "save model\n",
      "epoch:  31 ||| train loss :  0.0076347  <-> test loss :  0.0027635 |||\n",
      "epoch:  32 ||| train loss :  0.0065605  <-> test loss :  0.0039359 |||\n",
      "epoch:  33 ||| train loss :  0.0056275  <-> test loss :  0.0061958 |||\n",
      "epoch:  34 ||| train loss :  0.0046352  <-> test loss :  0.0042241 |||\n",
      "epoch:  35 ||| train loss :  0.0079941  <-> test loss :  0.0743452 |||\n",
      "epoch:  36 ||| train loss :  0.0070625  <-> test loss :  0.0036793 |||\n",
      "save model\n",
      "epoch:  37 ||| train loss :  0.0035147  <-> test loss :  0.002021 |||\n",
      "save model\n",
      "epoch:  38 ||| train loss :  0.0036728  <-> test loss :  0.0013679 |||\n",
      "epoch:  39 ||| train loss :  0.0046319  <-> test loss :  0.0020421 |||\n",
      "epoch:  40 ||| train loss :  0.0035774  <-> test loss :  0.0062624 |||\n",
      "epoch:  41 ||| train loss :  0.0028143  <-> test loss :  0.0017265 |||\n",
      "epoch:  42 ||| train loss :  0.002394  <-> test loss :  0.0016665 |||\n",
      "epoch:  43 ||| train loss :  0.0054485  <-> test loss :  0.0949973 |||\n",
      "epoch:  44 ||| train loss :  0.0170501  <-> test loss :  0.6933304 |||\n",
      "epoch:  45 ||| train loss :  0.0265899  <-> test loss :  0.9657937 |||\n",
      "epoch:  46 ||| train loss :  0.0116612  <-> test loss :  0.2009161 |||\n",
      "epoch:  47 ||| train loss :  0.0076727  <-> test loss :  0.0242842 |||\n",
      "epoch:  48 ||| train loss :  0.003933  <-> test loss :  0.0031803 |||\n",
      "epoch:  49 ||| train loss :  0.0034189  <-> test loss :  0.0019155 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.0027231  <-> test loss :  0.0005506 |||\n",
      "epoch:  51 ||| train loss :  0.0021244  <-> test loss :  0.0008507 |||\n",
      "epoch:  52 ||| train loss :  0.0020585  <-> test loss :  0.0008153 |||\n",
      "epoch:  53 ||| train loss :  0.0018306  <-> test loss :  0.0007447 |||\n",
      "epoch:  54 ||| train loss :  0.0018941  <-> test loss :  0.0009559 |||\n",
      "epoch:  55 ||| train loss :  0.0018956  <-> test loss :  0.000843 |||\n",
      "save model\n",
      "epoch:  56 ||| train loss :  0.0018719  <-> test loss :  0.0005143 |||\n",
      "epoch:  57 ||| train loss :  0.0015989  <-> test loss :  0.0010884 |||\n",
      "epoch:  58 ||| train loss :  0.001803  <-> test loss :  0.0007962 |||\n",
      "epoch:  59 ||| train loss :  0.002073  <-> test loss :  0.0008023 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165636092084.pth\n",
      "0.875\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "1.0\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "0.84375\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "1.0\n",
      "0.90625\n",
      "0.96875\n",
      "0.90625\n",
      "0.84375\n",
      "0.96875\n",
      "0.875\n",
      "0.875\n",
      "1.0\n",
      "0.96875\n",
      "0.9375\n",
      "0.9456521739130435\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  1.4005331  <-> test loss :  1.5183694 |||\n",
      "epoch:  1 ||| train loss :  0.9914149  <-> test loss :  2.7647412 |||\n",
      "epoch:  2 ||| train loss :  0.751618  <-> test loss :  1.7808357 |||\n",
      "epoch:  3 ||| train loss :  0.5904554  <-> test loss :  2.0658092 |||\n",
      "save model\n",
      "epoch:  4 ||| train loss :  0.4846947  <-> test loss :  1.0481974 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  0.4064498  <-> test loss :  0.7513581 |||\n",
      "epoch:  6 ||| train loss :  0.3412582  <-> test loss :  6.6567354 |||\n",
      "epoch:  7 ||| train loss :  0.2928799  <-> test loss :  1.9266081 |||\n",
      "save model\n",
      "epoch:  8 ||| train loss :  0.2492017  <-> test loss :  0.5411118 |||\n",
      "epoch:  9 ||| train loss :  0.2065584  <-> test loss :  1.5378683 |||\n",
      "epoch:  10 ||| train loss :  0.1705405  <-> test loss :  2.8881176 |||\n",
      "epoch:  11 ||| train loss :  0.1614704  <-> test loss :  2.2240238 |||\n",
      "epoch:  12 ||| train loss :  0.1280151  <-> test loss :  0.7141647 |||\n",
      "epoch:  13 ||| train loss :  0.1120009  <-> test loss :  1.4121056 |||\n",
      "save model\n",
      "epoch:  14 ||| train loss :  0.0957988  <-> test loss :  0.203288 |||\n",
      "save model\n",
      "epoch:  15 ||| train loss :  0.0854535  <-> test loss :  0.18143 |||\n",
      "epoch:  16 ||| train loss :  0.0828003  <-> test loss :  1.0677212 |||\n",
      "save model\n",
      "epoch:  17 ||| train loss :  0.101044  <-> test loss :  0.0785762 |||\n",
      "epoch:  18 ||| train loss :  0.0726728  <-> test loss :  0.1286419 |||\n",
      "epoch:  19 ||| train loss :  0.0389716  <-> test loss :  1.6352867 |||\n",
      "epoch:  20 ||| train loss :  0.0370641  <-> test loss :  0.117598 |||\n",
      "epoch:  21 ||| train loss :  0.0328023  <-> test loss :  0.5594587 |||\n",
      "save model\n",
      "epoch:  22 ||| train loss :  0.0353347  <-> test loss :  0.0272368 |||\n",
      "epoch:  23 ||| train loss :  0.0271064  <-> test loss :  0.230147 |||\n",
      "epoch:  24 ||| train loss :  0.0242478  <-> test loss :  0.7008281 |||\n",
      "epoch:  25 ||| train loss :  0.0160897  <-> test loss :  0.7919783 |||\n",
      "epoch:  26 ||| train loss :  0.0251975  <-> test loss :  0.342983 |||\n",
      "save model\n",
      "epoch:  27 ||| train loss :  0.0140206  <-> test loss :  0.0068732 |||\n",
      "epoch:  28 ||| train loss :  0.01178  <-> test loss :  0.5265139 |||\n",
      "save model\n",
      "epoch:  29 ||| train loss :  0.01946  <-> test loss :  0.0066193 |||\n",
      "epoch:  30 ||| train loss :  0.0102191  <-> test loss :  0.0464371 |||\n",
      "epoch:  31 ||| train loss :  0.0063973  <-> test loss :  0.0194852 |||\n",
      "save model\n",
      "epoch:  32 ||| train loss :  0.0048307  <-> test loss :  0.003732 |||\n",
      "epoch:  33 ||| train loss :  0.0048973  <-> test loss :  0.0858541 |||\n",
      "epoch:  34 ||| train loss :  0.0110485  <-> test loss :  0.07795 |||\n",
      "epoch:  35 ||| train loss :  0.0054772  <-> test loss :  0.0305737 |||\n",
      "epoch:  36 ||| train loss :  0.005968  <-> test loss :  0.004714 |||\n",
      "save model\n",
      "epoch:  37 ||| train loss :  0.0055037  <-> test loss :  0.0023577 |||\n",
      "save model\n",
      "epoch:  38 ||| train loss :  0.0051196  <-> test loss :  0.0022964 |||\n",
      "save model\n",
      "epoch:  39 ||| train loss :  0.0038752  <-> test loss :  0.0016715 |||\n",
      "save model\n",
      "epoch:  40 ||| train loss :  0.0040587  <-> test loss :  0.0013221 |||\n",
      "epoch:  41 ||| train loss :  0.0030705  <-> test loss :  0.0016869 |||\n",
      "epoch:  42 ||| train loss :  0.0032786  <-> test loss :  0.0016086 |||\n",
      "save model\n",
      "epoch:  43 ||| train loss :  0.0023813  <-> test loss :  0.0008785 |||\n",
      "epoch:  44 ||| train loss :  0.0025192  <-> test loss :  0.0017867 |||\n",
      "save model\n",
      "epoch:  45 ||| train loss :  0.0027998  <-> test loss :  0.0006311 |||\n",
      "epoch:  46 ||| train loss :  0.0025674  <-> test loss :  0.0008945 |||\n",
      "epoch:  47 ||| train loss :  0.0022647  <-> test loss :  0.0016688 |||\n",
      "epoch:  48 ||| train loss :  0.0020175  <-> test loss :  0.0008269 |||\n",
      "epoch:  49 ||| train loss :  0.001815  <-> test loss :  0.0011702 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.0015414  <-> test loss :  0.0005493 |||\n",
      "epoch:  51 ||| train loss :  0.0017597  <-> test loss :  0.0005571 |||\n",
      "epoch:  52 ||| train loss :  0.0014436  <-> test loss :  0.0005778 |||\n",
      "epoch:  53 ||| train loss :  0.0022015  <-> test loss :  0.0011064 |||\n",
      "epoch:  54 ||| train loss :  0.0015933  <-> test loss :  0.0007092 |||\n",
      "epoch:  55 ||| train loss :  0.0018156  <-> test loss :  0.0007481 |||\n",
      "epoch:  56 ||| train loss :  0.0021453  <-> test loss :  0.0005604 |||\n",
      "epoch:  57 ||| train loss :  0.0019363  <-> test loss :  0.0007651 |||\n",
      "epoch:  58 ||| train loss :  0.0018816  <-> test loss :  0.0006122 |||\n",
      "epoch:  59 ||| train loss :  0.0018548  <-> test loss :  0.0006318 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165636139869.pth\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "0.9375\n",
      "0.96875\n",
      "1.0\n",
      "0.90625\n",
      "1.0\n",
      "1.0\n",
      "0.875\n",
      "1.0\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "0.875\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.9524456521739131\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  1.4387793  <-> test loss :  1.266096 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  0.9977015  <-> test loss :  0.8348821 |||\n",
      "epoch:  2 ||| train loss :  0.7342346  <-> test loss :  1.0624602 |||\n",
      "save model\n",
      "epoch:  3 ||| train loss :  0.5722491  <-> test loss :  0.7733858 |||\n",
      "epoch:  4 ||| train loss :  0.4724235  <-> test loss :  1.0931726 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  0.3602676  <-> test loss :  0.7128642 |||\n",
      "epoch:  6 ||| train loss :  0.3200737  <-> test loss :  2.0923228 |||\n",
      "epoch:  7 ||| train loss :  0.2738369  <-> test loss :  5.7641163 |||\n",
      "epoch:  8 ||| train loss :  0.2146762  <-> test loss :  3.1752696 |||\n",
      "epoch:  9 ||| train loss :  0.2185834  <-> test loss :  2.0445638 |||\n",
      "save model\n",
      "epoch:  10 ||| train loss :  0.1846613  <-> test loss :  0.3069367 |||\n",
      "epoch:  11 ||| train loss :  0.1301794  <-> test loss :  0.6327348 |||\n",
      "epoch:  12 ||| train loss :  0.128316  <-> test loss :  1.9084008 |||\n",
      "save model\n",
      "epoch:  13 ||| train loss :  0.1035194  <-> test loss :  0.1927005 |||\n",
      "epoch:  14 ||| train loss :  0.0867533  <-> test loss :  0.3348127 |||\n",
      "epoch:  15 ||| train loss :  0.0687164  <-> test loss :  1.1238413 |||\n",
      "epoch:  16 ||| train loss :  0.0569841  <-> test loss :  0.6632928 |||\n",
      "save model\n",
      "epoch:  17 ||| train loss :  0.0530704  <-> test loss :  0.1025276 |||\n",
      "epoch:  18 ||| train loss :  0.0334912  <-> test loss :  0.4461568 |||\n",
      "epoch:  19 ||| train loss :  0.0243264  <-> test loss :  0.1294211 |||\n",
      "save model\n",
      "epoch:  20 ||| train loss :  0.0193346  <-> test loss :  0.0353294 |||\n",
      "epoch:  21 ||| train loss :  0.0252591  <-> test loss :  0.2895943 |||\n",
      "epoch:  22 ||| train loss :  0.0230941  <-> test loss :  0.0454544 |||\n",
      "save model\n",
      "epoch:  23 ||| train loss :  0.0113902  <-> test loss :  0.0143207 |||\n",
      "epoch:  24 ||| train loss :  0.0140341  <-> test loss :  0.4036903 |||\n",
      "epoch:  25 ||| train loss :  0.0174382  <-> test loss :  0.0456386 |||\n",
      "save model\n",
      "epoch:  26 ||| train loss :  0.0099735  <-> test loss :  0.0070857 |||\n",
      "epoch:  27 ||| train loss :  0.0082101  <-> test loss :  0.0174119 |||\n",
      "epoch:  28 ||| train loss :  0.0075518  <-> test loss :  0.0088497 |||\n",
      "epoch:  29 ||| train loss :  0.007395  <-> test loss :  0.1000501 |||\n",
      "epoch:  30 ||| train loss :  0.0066879  <-> test loss :  0.0074643 |||\n",
      "save model\n",
      "epoch:  31 ||| train loss :  0.004371  <-> test loss :  0.0050828 |||\n",
      "epoch:  32 ||| train loss :  0.004824  <-> test loss :  0.0083257 |||\n",
      "save model\n",
      "epoch:  33 ||| train loss :  0.0045018  <-> test loss :  0.0026138 |||\n",
      "save model\n",
      "epoch:  34 ||| train loss :  0.0050711  <-> test loss :  0.0021096 |||\n",
      "epoch:  35 ||| train loss :  0.0039091  <-> test loss :  0.0121047 |||\n",
      "epoch:  36 ||| train loss :  0.0036857  <-> test loss :  0.0047537 |||\n",
      "save model\n",
      "epoch:  37 ||| train loss :  0.0038273  <-> test loss :  0.0012301 |||\n",
      "epoch:  38 ||| train loss :  0.0032699  <-> test loss :  0.0023623 |||\n",
      "epoch:  39 ||| train loss :  0.0031497  <-> test loss :  0.0109733 |||\n",
      "epoch:  40 ||| train loss :  0.0030029  <-> test loss :  0.0022292 |||\n",
      "epoch:  41 ||| train loss :  0.0029675  <-> test loss :  0.0012832 |||\n",
      "save model\n",
      "epoch:  42 ||| train loss :  0.0031267  <-> test loss :  0.0008433 |||\n",
      "epoch:  43 ||| train loss :  0.0026236  <-> test loss :  0.0012336 |||\n",
      "epoch:  44 ||| train loss :  0.0026475  <-> test loss :  0.0015309 |||\n",
      "epoch:  45 ||| train loss :  0.0026328  <-> test loss :  0.0011743 |||\n",
      "epoch:  46 ||| train loss :  0.0026427  <-> test loss :  0.0056337 |||\n",
      "save model\n",
      "epoch:  47 ||| train loss :  0.0028458  <-> test loss :  0.0007823 |||\n",
      "epoch:  48 ||| train loss :  0.0026659  <-> test loss :  0.0021334 |||\n",
      "epoch:  49 ||| train loss :  0.0021807  <-> test loss :  0.0007967 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.0019038  <-> test loss :  0.0007577 |||\n",
      "epoch:  51 ||| train loss :  0.0028854  <-> test loss :  0.0011705 |||\n",
      "epoch:  52 ||| train loss :  0.0018305  <-> test loss :  0.0008207 |||\n",
      "save model\n",
      "epoch:  53 ||| train loss :  0.0017048  <-> test loss :  0.0007406 |||\n",
      "epoch:  54 ||| train loss :  0.0017948  <-> test loss :  0.0010599 |||\n",
      "epoch:  55 ||| train loss :  0.0017237  <-> test loss :  0.0007454 |||\n",
      "save model\n",
      "epoch:  56 ||| train loss :  0.0017599  <-> test loss :  0.0006774 |||\n",
      "epoch:  57 ||| train loss :  0.0019015  <-> test loss :  0.0007921 |||\n",
      "epoch:  58 ||| train loss :  0.0016583  <-> test loss :  0.0007088 |||\n",
      "save model\n",
      "epoch:  59 ||| train loss :  0.0016006  <-> test loss :  0.0005621 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165636187610.pth\n",
      "0.90625\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "0.90625\n",
      "0.9375\n",
      "0.875\n",
      "0.96875\n",
      "0.875\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "1.0\n",
      "0.90625\n",
      "0.96875\n",
      "0.90625\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "0.875\n",
      "1.0\n",
      "0.9375\n",
      "1.0\n",
      "0.9497282608695652\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  1.4704939  <-> test loss :  1.3465067 |||\n",
      "epoch:  1 ||| train loss :  1.0203622  <-> test loss :  3.2316809 |||\n",
      "save model\n",
      "epoch:  2 ||| train loss :  0.7334112  <-> test loss :  0.6857477 |||\n",
      "epoch:  3 ||| train loss :  0.5502612  <-> test loss :  1.0131292 |||\n",
      "epoch:  4 ||| train loss :  0.4536849  <-> test loss :  1.27246 |||\n",
      "save model\n",
      "epoch:  5 ||| train loss :  0.3581707  <-> test loss :  0.5719081 |||\n",
      "save model\n",
      "epoch:  6 ||| train loss :  0.3069571  <-> test loss :  0.3061906 |||\n",
      "epoch:  7 ||| train loss :  0.2605038  <-> test loss :  0.8685093 |||\n",
      "epoch:  8 ||| train loss :  0.2301576  <-> test loss :  1.3450066 |||\n",
      "epoch:  9 ||| train loss :  0.1880141  <-> test loss :  0.3556675 |||\n",
      "save model\n",
      "epoch:  10 ||| train loss :  0.1733878  <-> test loss :  0.2654279 |||\n",
      "save model\n",
      "epoch:  11 ||| train loss :  0.1130103  <-> test loss :  0.0973528 |||\n",
      "epoch:  12 ||| train loss :  0.1242192  <-> test loss :  0.3096899 |||\n",
      "epoch:  13 ||| train loss :  0.0939401  <-> test loss :  0.7445524 |||\n",
      "epoch:  14 ||| train loss :  0.0705579  <-> test loss :  0.1398185 |||\n",
      "epoch:  15 ||| train loss :  0.0739696  <-> test loss :  0.3797718 |||\n",
      "epoch:  16 ||| train loss :  0.0598408  <-> test loss :  0.1151766 |||\n",
      "epoch:  17 ||| train loss :  0.0648734  <-> test loss :  0.1143074 |||\n",
      "epoch:  18 ||| train loss :  0.0495298  <-> test loss :  0.1700486 |||\n",
      "save model\n",
      "epoch:  19 ||| train loss :  0.0341793  <-> test loss :  0.073139 |||\n",
      "save model\n",
      "epoch:  20 ||| train loss :  0.0268817  <-> test loss :  0.0283971 |||\n",
      "save model\n",
      "epoch:  21 ||| train loss :  0.0177074  <-> test loss :  0.0179805 |||\n",
      "epoch:  22 ||| train loss :  0.0140128  <-> test loss :  0.038851 |||\n",
      "epoch:  23 ||| train loss :  0.0149039  <-> test loss :  0.0316175 |||\n",
      "save model\n",
      "epoch:  24 ||| train loss :  0.0096179  <-> test loss :  0.0110286 |||\n",
      "save model\n",
      "epoch:  25 ||| train loss :  0.0082295  <-> test loss :  0.0042614 |||\n",
      "epoch:  26 ||| train loss :  0.007951  <-> test loss :  0.0092748 |||\n",
      "epoch:  27 ||| train loss :  0.0076494  <-> test loss :  0.0110545 |||\n",
      "epoch:  28 ||| train loss :  0.0085217  <-> test loss :  1.1354852 |||\n",
      "epoch:  29 ||| train loss :  0.0230738  <-> test loss :  0.0812267 |||\n",
      "epoch:  30 ||| train loss :  0.0091558  <-> test loss :  0.0057348 |||\n",
      "epoch:  31 ||| train loss :  0.0067141  <-> test loss :  0.0097975 |||\n",
      "epoch:  32 ||| train loss :  0.0056028  <-> test loss :  0.0106242 |||\n",
      "save model\n",
      "epoch:  33 ||| train loss :  0.0059812  <-> test loss :  0.0028038 |||\n",
      "save model\n",
      "epoch:  34 ||| train loss :  0.003551  <-> test loss :  0.0019329 |||\n",
      "epoch:  35 ||| train loss :  0.0039447  <-> test loss :  0.0036531 |||\n",
      "epoch:  36 ||| train loss :  0.0037049  <-> test loss :  0.003325 |||\n",
      "epoch:  37 ||| train loss :  0.0031067  <-> test loss :  0.0023257 |||\n",
      "epoch:  38 ||| train loss :  0.0033872  <-> test loss :  0.0019763 |||\n",
      "epoch:  39 ||| train loss :  0.0036795  <-> test loss :  0.002749 |||\n",
      "save model\n",
      "epoch:  40 ||| train loss :  0.0027784  <-> test loss :  0.0015376 |||\n",
      "save model\n",
      "epoch:  41 ||| train loss :  0.0028429  <-> test loss :  0.0015087 |||\n",
      "epoch:  42 ||| train loss :  0.0028328  <-> test loss :  0.0019217 |||\n",
      "epoch:  43 ||| train loss :  0.0027678  <-> test loss :  0.0043166 |||\n",
      "save model\n",
      "epoch:  44 ||| train loss :  0.0026748  <-> test loss :  0.0014637 |||\n",
      "epoch:  45 ||| train loss :  0.0024175  <-> test loss :  0.0019294 |||\n",
      "save model\n",
      "epoch:  46 ||| train loss :  0.0020607  <-> test loss :  0.0007935 |||\n",
      "save model\n",
      "epoch:  47 ||| train loss :  0.0024624  <-> test loss :  0.0005552 |||\n",
      "epoch:  48 ||| train loss :  0.0019484  <-> test loss :  0.0007734 |||\n",
      "epoch:  49 ||| train loss :  0.0025718  <-> test loss :  0.0010636 |||\n",
      "epoch:  50 ||| train loss :  0.001982  <-> test loss :  0.0012852 |||\n",
      "epoch:  51 ||| train loss :  0.0015032  <-> test loss :  0.0006036 |||\n",
      "epoch:  52 ||| train loss :  0.0018787  <-> test loss :  0.000779 |||\n",
      "epoch:  53 ||| train loss :  0.0015682  <-> test loss :  0.0008418 |||\n",
      "epoch:  54 ||| train loss :  0.0019492  <-> test loss :  0.0005612 |||\n",
      "save model\n",
      "epoch:  55 ||| train loss :  0.0018529  <-> test loss :  0.0005132 |||\n",
      "epoch:  56 ||| train loss :  0.0015367  <-> test loss :  0.0007037 |||\n",
      "epoch:  57 ||| train loss :  0.0013651  <-> test loss :  0.0007542 |||\n",
      "epoch:  58 ||| train loss :  0.0016955  <-> test loss :  0.0008687 |||\n",
      "epoch:  59 ||| train loss :  0.001485  <-> test loss :  0.0007569 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165636235304.pth\n",
      "0.90625\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "0.90625\n",
      "0.9375\n",
      "0.90625\n",
      "0.96875\n",
      "0.90625\n",
      "0.96875\n",
      "1.0\n",
      "0.96875\n",
      "0.9375\n",
      "0.90625\n",
      "0.9375\n",
      "0.9375\n",
      "0.90625\n",
      "0.9375\n",
      "0.9375\n",
      "0.875\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "0.938858695652174\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,632\n",
      "         MaxPool2d-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "              ReLU-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 32, 62, 62]          25,632\n",
      "         MaxPool2d-6           [-1, 32, 60, 60]               0\n",
      "       BatchNorm2d-7           [-1, 32, 60, 60]              64\n",
      "              ReLU-8           [-1, 32, 60, 60]               0\n",
      "            Conv2d-9           [-1, 64, 60, 60]          51,264\n",
      "        MaxPool2d-10           [-1, 64, 58, 58]               0\n",
      "      BatchNorm2d-11           [-1, 64, 58, 58]             128\n",
      "             ReLU-12           [-1, 64, 58, 58]               0\n",
      "           Conv2d-13           [-1, 32, 58, 58]          51,232\n",
      "        AvgPool2d-14           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-15           [-1, 32, 56, 56]              64\n",
      "             ReLU-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 60, 60]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 60, 60]              64\n",
      "             ReLU-19           [-1, 32, 60, 60]               0\n",
      "           Conv2d-20           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-21           [-1, 32, 58, 58]              64\n",
      "             ReLU-22           [-1, 32, 58, 58]               0\n",
      "           Conv2d-23           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-24           [-1, 32, 56, 56]              64\n",
      "             ReLU-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 32, 58, 58]           9,248\n",
      "      BatchNorm2d-27           [-1, 32, 58, 58]              64\n",
      "             ReLU-28           [-1, 32, 58, 58]               0\n",
      "           Conv2d-29           [-1, 32, 56, 56]           9,248\n",
      "      BatchNorm2d-30           [-1, 32, 56, 56]              64\n",
      "             ReLU-31           [-1, 32, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]          18,464\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "             ReLU-34           [-1, 32, 56, 56]               0\n",
      "           Conv2d-35            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-36            [-1, 1, 56, 56]               2\n",
      "             ReLU-37            [-1, 1, 56, 56]               0\n",
      "           Conv2d-38            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-39            [-1, 1, 56, 56]               2\n",
      "             ReLU-40            [-1, 1, 56, 56]               0\n",
      "           Conv2d-41            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-42            [-1, 1, 56, 56]               2\n",
      "             ReLU-43            [-1, 1, 56, 56]               0\n",
      "           Conv2d-44            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-45            [-1, 1, 56, 56]               2\n",
      "             ReLU-46            [-1, 1, 56, 56]               0\n",
      "           Conv2d-47           [-1, 32, 56, 56]              64\n",
      "             ReLU-48           [-1, 32, 56, 56]               0\n",
      "           Linear-49                   [-1, 32]         100,384\n",
      "           Conv2d-50            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-51            [-1, 1, 56, 56]               2\n",
      "             ReLU-52            [-1, 1, 56, 56]               0\n",
      "           Conv2d-53            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-54            [-1, 1, 56, 56]               2\n",
      "             ReLU-55            [-1, 1, 56, 56]               0\n",
      "           Conv2d-56            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-57            [-1, 1, 56, 56]               2\n",
      "             ReLU-58            [-1, 1, 56, 56]               0\n",
      "           Conv2d-59            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-60            [-1, 1, 56, 56]               2\n",
      "             ReLU-61            [-1, 1, 56, 56]               0\n",
      "           Conv2d-62           [-1, 32, 56, 56]              64\n",
      "             ReLU-63           [-1, 32, 56, 56]               0\n",
      "           Linear-64                   [-1, 32]         100,384\n",
      "           Conv2d-65            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-66            [-1, 1, 56, 56]               2\n",
      "             ReLU-67            [-1, 1, 56, 56]               0\n",
      "           Conv2d-68            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-69            [-1, 1, 56, 56]               2\n",
      "             ReLU-70            [-1, 1, 56, 56]               0\n",
      "           Conv2d-71            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-72            [-1, 1, 56, 56]               2\n",
      "             ReLU-73            [-1, 1, 56, 56]               0\n",
      "           Conv2d-74            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-75            [-1, 1, 56, 56]               2\n",
      "             ReLU-76            [-1, 1, 56, 56]               0\n",
      "           Conv2d-77           [-1, 32, 56, 56]              64\n",
      "             ReLU-78           [-1, 32, 56, 56]               0\n",
      "           Linear-79                   [-1, 32]         100,384\n",
      "           Conv2d-80            [-1, 1, 56, 56]              33\n",
      "      BatchNorm2d-81            [-1, 1, 56, 56]               2\n",
      "             ReLU-82            [-1, 1, 56, 56]               0\n",
      "           Conv2d-83            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-84            [-1, 1, 56, 56]               2\n",
      "             ReLU-85            [-1, 1, 56, 56]               0\n",
      "           Conv2d-86            [-1, 1, 56, 56]             289\n",
      "      BatchNorm2d-87            [-1, 1, 56, 56]               2\n",
      "             ReLU-88            [-1, 1, 56, 56]               0\n",
      "           Conv2d-89            [-1, 1, 56, 56]              10\n",
      "      BatchNorm2d-90            [-1, 1, 56, 56]               2\n",
      "             ReLU-91            [-1, 1, 56, 56]               0\n",
      "           Conv2d-92           [-1, 32, 56, 56]              64\n",
      "             ReLU-93           [-1, 32, 56, 56]               0\n",
      "           Linear-94                   [-1, 32]         100,384\n",
      "AdaptiveAvgPool2d-95             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-96             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-97             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-98             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-99             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-101             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-102             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 32, 1, 1]               0\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 4]           1,284\n",
      "AdaptiveAvgPool2d-106             [-1, 32, 1, 1]               0\n",
      "          Linear-107                    [-1, 4]             132\n",
      "================================================================\n",
      "Total params: 600,892\n",
      "Trainable params: 600,892\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 38.93\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 41.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "epoch:  0 ||| train loss :  1.4596922  <-> test loss :  1.4662007 |||\n",
      "save model\n",
      "epoch:  1 ||| train loss :  1.0467463  <-> test loss :  1.2516867 |||\n",
      "epoch:  2 ||| train loss :  0.7872947  <-> test loss :  4.5939488 |||\n",
      "save model\n",
      "epoch:  3 ||| train loss :  0.6268965  <-> test loss :  0.8292468 |||\n",
      "epoch:  4 ||| train loss :  0.4986021  <-> test loss :  1.1376525 |||\n",
      "epoch:  5 ||| train loss :  0.4197445  <-> test loss :  1.2390158 |||\n",
      "epoch:  6 ||| train loss :  0.3456981  <-> test loss :  2.4896193 |||\n",
      "save model\n",
      "epoch:  7 ||| train loss :  0.2962552  <-> test loss :  0.3842194 |||\n",
      "epoch:  8 ||| train loss :  0.243283  <-> test loss :  0.5052556 |||\n",
      "epoch:  9 ||| train loss :  0.2072103  <-> test loss :  2.0669236 |||\n",
      "epoch:  10 ||| train loss :  0.1688193  <-> test loss :  1.3873457 |||\n",
      "epoch:  11 ||| train loss :  0.1373976  <-> test loss :  0.3980291 |||\n",
      "epoch:  12 ||| train loss :  0.145167  <-> test loss :  0.4942984 |||\n",
      "epoch:  13 ||| train loss :  0.1123659  <-> test loss :  1.1648158 |||\n",
      "save model\n",
      "epoch:  14 ||| train loss :  0.0997197  <-> test loss :  0.2264545 |||\n",
      "epoch:  15 ||| train loss :  0.0686265  <-> test loss :  0.290676 |||\n",
      "save model\n",
      "epoch:  16 ||| train loss :  0.0580287  <-> test loss :  0.2164441 |||\n",
      "epoch:  17 ||| train loss :  0.0465729  <-> test loss :  0.2674334 |||\n",
      "save model\n",
      "epoch:  18 ||| train loss :  0.0325765  <-> test loss :  0.0844538 |||\n",
      "save model\n",
      "epoch:  19 ||| train loss :  0.03242  <-> test loss :  0.0460576 |||\n",
      "save model\n",
      "epoch:  20 ||| train loss :  0.0224224  <-> test loss :  0.0412154 |||\n",
      "epoch:  21 ||| train loss :  0.0179109  <-> test loss :  0.1639208 |||\n",
      "save model\n",
      "epoch:  22 ||| train loss :  0.0256162  <-> test loss :  0.0346289 |||\n",
      "save model\n",
      "epoch:  23 ||| train loss :  0.0167458  <-> test loss :  0.0215924 |||\n",
      "save model\n",
      "epoch:  24 ||| train loss :  0.0109933  <-> test loss :  0.007835 |||\n",
      "epoch:  25 ||| train loss :  0.0173328  <-> test loss :  0.441376 |||\n",
      "epoch:  26 ||| train loss :  0.0187104  <-> test loss :  0.0268655 |||\n",
      "epoch:  27 ||| train loss :  0.015737  <-> test loss :  0.0620426 |||\n",
      "save model\n",
      "epoch:  28 ||| train loss :  0.0080438  <-> test loss :  0.003902 |||\n",
      "epoch:  29 ||| train loss :  0.0070327  <-> test loss :  0.0514267 |||\n",
      "epoch:  30 ||| train loss :  0.0073261  <-> test loss :  0.0043982 |||\n",
      "epoch:  31 ||| train loss :  0.0065091  <-> test loss :  0.008215 |||\n",
      "epoch:  32 ||| train loss :  0.0050061  <-> test loss :  0.0050972 |||\n",
      "epoch:  33 ||| train loss :  0.0076238  <-> test loss :  0.033324 |||\n",
      "epoch:  34 ||| train loss :  0.0059082  <-> test loss :  0.0130952 |||\n",
      "epoch:  35 ||| train loss :  0.0047185  <-> test loss :  0.0050762 |||\n",
      "save model\n",
      "epoch:  36 ||| train loss :  0.0036544  <-> test loss :  0.0021042 |||\n",
      "epoch:  37 ||| train loss :  0.0045274  <-> test loss :  0.0872105 |||\n",
      "epoch:  38 ||| train loss :  0.0061543  <-> test loss :  0.0065279 |||\n",
      "save model\n",
      "epoch:  39 ||| train loss :  0.0046276  <-> test loss :  0.0012545 |||\n",
      "epoch:  40 ||| train loss :  0.0029464  <-> test loss :  0.0030721 |||\n",
      "epoch:  41 ||| train loss :  0.0034554  <-> test loss :  0.0040442 |||\n",
      "save model\n",
      "epoch:  42 ||| train loss :  0.0030566  <-> test loss :  0.000973 |||\n",
      "save model\n",
      "epoch:  43 ||| train loss :  0.0019201  <-> test loss :  0.0009366 |||\n",
      "epoch:  44 ||| train loss :  0.0022308  <-> test loss :  0.000951 |||\n",
      "epoch:  45 ||| train loss :  0.001805  <-> test loss :  0.0010001 |||\n",
      "epoch:  46 ||| train loss :  0.0021395  <-> test loss :  0.0010419 |||\n",
      "epoch:  47 ||| train loss :  0.0018071  <-> test loss :  0.0011645 |||\n",
      "epoch:  48 ||| train loss :  0.0019257  <-> test loss :  0.0011647 |||\n",
      "epoch:  49 ||| train loss :  0.0022091  <-> test loss :  0.0019049 |||\n",
      "save model\n",
      "epoch:  50 ||| train loss :  0.0019197  <-> test loss :  0.0007208 |||\n",
      "epoch:  51 ||| train loss :  0.0015078  <-> test loss :  0.0008994 |||\n",
      "epoch:  52 ||| train loss :  0.0014409  <-> test loss :  0.000722 |||\n",
      "epoch:  53 ||| train loss :  0.0014757  <-> test loss :  0.000823 |||\n",
      "save model\n",
      "epoch:  54 ||| train loss :  0.0015057  <-> test loss :  0.0003812 |||\n",
      "epoch:  55 ||| train loss :  0.0019055  <-> test loss :  0.0009768 |||\n",
      "epoch:  56 ||| train loss :  0.0016068  <-> test loss :  0.00061 |||\n",
      "epoch:  57 ||| train loss :  0.0015267  <-> test loss :  0.0006168 |||\n",
      "epoch:  58 ||| train loss :  0.0013718  <-> test loss :  0.0008676 |||\n",
      "epoch:  59 ||| train loss :  0.0013594  <-> test loss :  0.0004408 |||\n",
      "最后测试 Model_pth/Loss/Ablation_Study_model_cuda_165636282937.pth\n",
      "0.90625\n",
      "0.96875\n",
      "0.9375\n",
      "1.0\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "0.96875\n",
      "0.96875\n",
      "1.0\n",
      "0.9375\n",
      "0.96875\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.90625\n",
      "0.90625\n",
      "0.90625\n",
      "1.0\n",
      "0.9483695652173914\n",
      "[0.9456521739130435, 0.9524456521739131, 0.9497282608695652, 0.938858695652174, 0.9483695652173914]\n",
      "0.9470108695652174\n",
      "0.947\n",
      "----------loss weight==0.2----------\n",
      "----------loss weight==5----------\n",
      "所有损失分别为：[0.9456521739130435, 0.9510869565217391, 0.9429347826086957, 0.9415760869565217, 0.938858695652174]\n",
      "平均损失：     0.944    (0.9440217391304347)   \n",
      " \n",
      "----------loss weight==2----------\n",
      "所有损失分别为：[0.9538043478260869, 0.9497282608695652, 0.9619565217391305, 0.9538043478260869, 0.9402173913043478]\n",
      "平均损失：     0.9519    (0.9519021739130435)   \n",
      " \n",
      "----------loss weight==1.25----------\n",
      "所有损失分别为：[0.96875, 0.967391304347826, 0.9551630434782609, 0.9497282608695652, 0.9551630434782609]\n",
      "平均损失：     0.9592    (0.9592391304347826)   \n",
      " \n",
      "----------loss weight==1----------\n",
      "所有损失分别为：[0.9538043478260869, 0.9660326086956522, 0.9442934782608695, 0.9633152173913043, 0.9592391304347826]\n",
      "平均损失：     0.9573    (0.9573369565217391)   \n",
      " \n",
      "----------loss weight==0.8----------\n",
      "所有损失分别为：[0.9578804347826086, 0.9483695652173914, 0.9565217391304348, 0.9551630434782609, 0.9551630434782609]\n",
      "平均损失：     0.9546    (0.9546195652173912)   \n",
      " \n",
      "----------loss weight==0.5----------\n",
      "所有损失分别为：[0.9619565217391305, 0.9660326086956522, 0.9456521739130435, 0.9565217391304348, 0.9538043478260869]\n",
      "平均损失：     0.9568    (0.9567934782608696)   \n",
      " \n",
      "----------loss weight==0.2----------\n",
      "所有损失分别为：[0.9456521739130435, 0.9524456521739131, 0.9497282608695652, 0.938858695652174, 0.9483695652173914]\n",
      "平均损失：     0.947    (0.9470108695652174)   \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# [0.947    ,0.9527    ,0.9592    ,0.9573    ,0.9546    ,0.9568    ,0.9505       ]\n",
    "Train_grid_table_single(backbone_2loss_cross_self_mask_mask2_Projection, times, grid_table_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
